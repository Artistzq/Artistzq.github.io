<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时</title>
    <link href="/2024/01/26/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_3_CRI/"/>
    <url>/2024/01/26/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_3_CRI/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时"><a href="#kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时" class="headerlink" title="kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时"></a>kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时</h1><h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1 概念"></a>1 概念</h2><p>容器运行时（Container Runtime）是 Kubernetes（k8s）集群中每个节点的核心组件，负责管理容器的整个生命周期，包括从拉取和运行容器镜像等关键任务。</p><div align=center><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240124152148.png"></div><p>在k8s 1.23版本之前，kubelet依赖于dockershim来操作Docker容器运行时。Docker内部集成了containerd并在此基础上构建了更加用户友好的功能层，但这也意味着dockershim需要随着Docker的更新持续维护跟进。</p><p>为了实现Kubernetes与具体容器运行时技术的解耦，自k8s 1.5版本起，社区引入了容器运行时接口（Container Runtime Interface，CRI）标准，它提供了一组标准化接口以供 Kubernetes 平台与各种容器运行时进行交互，并推荐使用原生支持CRI的容器运行时如containerd和CRI-O。这一转变标志着Kubernetes未来将不再直接绑定Docker，而是通过兼容CRI的标准运行时统一管控容器，从而提高了容器运行时的可插拔性以及Kubernetes本身的稳定性和灵活性。kubelet通过gRPC协议结合protobuf序列化方式来实现对CRI接口的远程调用过程。</p><p>尽管如此，这并不影响继续使用通过Docker打包的镜像。实际上，任何遵循OCI（开放容器倡议，Open Container Initiative）规范创建的镜像都可以被兼容。由于Docker打包的镜像同样遵循OCI规范，因此它们仍能在k8s环境中得到有效的管理和运行。</p><p>CRI相关的概念如下图所示。</p><p><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240124144045.png" alt="cri-arch"></p><h3 id="1-1-Docker-和-dockershim"><a href="#1-1-Docker-和-dockershim" class="headerlink" title="1.1 Docker 和 dockershim"></a>1.1 Docker 和 dockershim</h3><p>Docker是一个方便的工具，用于处理镜像打包、发布、容器运行等操作。Docker本身包含了多个组件，如：</p><ul><li>docker-cli</li><li>containerd</li><li>runc</li><li>…</li></ul><p>k8s 中包含了一个组件 dockershim，使其能够支持Docker。Docker 没有实现CRI接口（Docker出现时间更早），因此 k8s 不得不自己维护一个工具，通过此工具操作Docker，这就是dockershim。</p><p>随着容器化成为行业标准，k8s 项目为了增加了对额外运行时的支持，提出了 CRI，更多的运行时实现了此接口。k8s 项目对 dockershim 和docker的依赖使整个项目变得脆弱，且对 dockershim 的单独维护增加了成本，在1.23版本后，k8s 开始弃用 dockershim，取消对Docker的直接支持。</p><p>另外，Docker 的多层封装和调用，导致其在可维护性上略逊一筹，增加了线上问题的定位难度。</p><h3 id="1-2-CRI"><a href="#1-2-CRI" class="headerlink" title="1.2 CRI"></a>1.2 CRI</h3><p>类似 JAVA 的 SPI 机制，k8s 定义了 CRI 后，不需要再像维护 dockershim 那样，为某个容器运行时维护一套适配工具。只需要各个容器运行时的提供方实现了这个接口，k8s 就可以直接调用对应的操作，例如containerd和CRI-O。</p><p>containerd 是一个来自 Docker 的高级容器运行时，并实现了 CRI 规范。它是从 Docker 项目中分离出来的，因此 Docker 自己在内部使用 containerd。containerd 通过其 CRI 插件实现了 k8s 容器运行时接口（CRI），它可以管理容器的整个生命周期，包括从镜像的传输、存储到容器的执行、监控再到网络。</p><p>CRI-O 是另一个实现了容器运行时接口（CRI）的高级容器运行时，是 containerd 的一个替代品。</p><p>使用 containerd 和 CRI-O 的方案比起 docker 简洁很多，因为省去了冗余的多层封装。</p><h3 id="1-3-OCI"><a href="#1-3-OCI" class="headerlink" title="1.3 OCI"></a>1.3 OCI</h3><p>OCI 是指开放容器倡议（Open Container Initiative），旨在制定容器镜像和运行时的行业标准。只要是符合规范的不同运行时，这些运行时可以有不同的底层实现。例如，符合 OCI 的在 linux 上的容器运行时和在 Windows 上的容器运行时。遵循该倡议的运行时通常更底层，和操作系统进行交互，从而创建容器。runC和kata-runtime就是这样的容器运行时。</p><p>runc 是轻量级的通用运行时容器，它遵守 OCI 规范，是实现 OCI 接口的最低级别的组件，它与内核交互创建并运行容器。runc 为容器提供了所有的低级功能，与现有的低级 Linux 功能交互，如命名空间和控制组，它使用这些功能来创建和运行容器进程。</p><h3 id="1-4-dockershim的弃用"><a href="#1-4-dockershim的弃用" class="headerlink" title="1.4 dockershim的弃用"></a>1.4 dockershim的弃用</h3><p>dockershim 的弃用并不意味着 k8s 将不能运行 Docker 格式的容器。containerd 和 CRI-O 都可以运行 Docker 格式的镜像，因为 Docker 格式的镜像也遵循了 OCI 格式，只是它们不再需要使用 docker 命令或 Docker 守护程序。</p><p>从功能性来讲，containerd 和 CRI-O 都符合 CRI 和 OCI 的标准。从稳定性来说，containerd 一直在 docker 里使用，生产环境经验比较充足，因此更推荐选择 containerd 作为 k8s 的容器运行时。</p><h2 id="2-实验"><a href="#2-实验" class="headerlink" title="2 实验"></a>2 实验</h2><p>实验目的：使用 containerd 或 CRI-O 作为容器运行时，并在工作节点中使用 <code>crictl</code> 命令代替 docker 命令。</p><h3 id="2-1-kind-默认使用-containerd"><a href="#2-1-kind-默认使用-containerd" class="headerlink" title="2.1 kind 默认使用 containerd"></a>2.1 kind 默认使用 containerd</h3><p>查阅文档得知，Kind 内部使用 Kubeadm 创建和启动集群节点，并使用 containerd 作为容器运行时，所以弃用 dockershim对 Kind 没有什么影响。另一个部署单机集群的工具 <code>minikube</code> 使用 cri-o 作为默认容器运行时。</p><p>进入Master Node，查看使用的容器运行时：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/cni-test$ docker <span class="hljs-built_in">exec</span> -it cni-test-cluster-control-plane /bin/bash<br>root@cni-test-cluster-control-plane:/<span class="hljs-comment"># kubectl get node cni-test-cluster-worker -o json | jq &#x27;.status.nodeInfo.containerRuntimeVersion&#x27;</span><br><span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br></code></pre></td></tr></table></figure><p>使用 <code>crictl</code> 查看镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@cni-test-cluster-control-plane:/<span class="hljs-comment"># crictl images ps</span><br>IMAGE                                      TAG                  IMAGE ID            SIZE<br>docker.io/flannel/flannel-cni-plugin       v1.2.0               a55d1bad692b7       3.88MB<br>docker.io/flannel/flannel                  v0.24.0              0dc86fe0f22e6       28MB<br>docker.io/kindest/kindnetd                 v20230511-dc714da8   b0b1fa0f58c6e       27.7MB<br>docker.io/kindest/local-path-helper        v20230510-486859a6   be300acfc8622       3.05MB<br>docker.io/kindest/local-path-provisioner   v20230511-dc714da8   ce18e076e9d4b       19.4MB<br>docker.io/library/go-hello-world-image     v0.0.1               586004f581b85       21.3MB<br>registry.k8s.io/coredns/coredns            v1.11.1              cbb01a7bd410d       18.2MB<br>registry.k8s.io/etcd                       3.5.10-0             a0eed15eed449       56.6MB<br>registry.k8s.io/kube-apiserver             v1.29.0              4a9136aaad730       86.1MB<br>registry.k8s.io/kube-controller-manager    v1.29.0              23d55452a141c       80.3MB<br>registry.k8s.io/kube-proxy                 v1.29.0              fa4dee78049db       83.5MB<br>registry.k8s.io/kube-scheduler             v1.29.0              ba16e783eea7b       60.6MB<br>registry.k8s.io/pause                      3.7                  221177c6082a8       311kB<br></code></pre></td></tr></table></figure><h3 id="2-2-将-kind-使用的运行时替换成-CRI-O"><a href="#2-2-将-kind-使用的运行时替换成-CRI-O" class="headerlink" title="2.2 将 kind 使用的运行时替换成 CRI-O"></a>2.2 将 kind 使用的运行时替换成 CRI-O</h3><p>在利用 kind 工具构建集群时，默认会采用 <code>kindest/node</code> 镜像作为节点镜像，该镜像内部预设的容器运行时是 containerd。若要将 containerd 更改为 crio 运行时，有两种可行方案：</p><ol><li><p>通过自定义节点镜像的方式进行切换：首先，在 <code>kindest/node</code> 镜像的基础之上重新构建一个新的节点镜像，并结合使用 <code>kind create cluster</code> 命令以及适配 crio 的定制版 <code>kind-config.yaml</code> 文件来创建新的集群。可以参考： <a href="https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2"><code>https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2</code></a> 和 <a href="https://github.com/warm-metal/kindest-base-crio/tree/main"><code>https://github.com/warm-metal/kindest-base-crio/tree/main</code></a> 。</p><blockquote><p>在实际操作过程中，参考网站提供的镜像版本较为陈旧，和现有的 kind 以及 k8s 有兼容性问题。</p></blockquote></li><li><p>在已用 kind 创建好的 k8s 集群上直接操作：进入 Worker 节点或 Master 节点（它们实质上是由 kind 创建的 Docker 容器），手动安装并配置 crio 运行时，然后对节点进行重新初始化或重新加入集群。</p><blockquote><p>为了深入理解 k8s 集群创建过程中的各个环节，并尽可能保留关键操作步骤，我们选择手动在集群内安装 crio 运行时。后续的文章中，我们将尝试探索如何打包适配 crio 的节点镜像。</p></blockquote></li></ol><p>本篇文章重点关注第2种方法。</p><h4 id="2-2-1-创建新集群"><a href="#2-2-1-创建新集群" class="headerlink" title="2.2.1 创建新集群"></a>2.2.1 创建新集群</h4><p>利用本系列第一篇博客的配置文件 <a href="https://github.com/Artistzq/kubelabs/tree/main/hello"><code>https://github.com/Artistzq/kubelabs/tree/main/hello</code></a> ，重新创建集群 <code>crio-hello-cluster</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash create.sh -n crio-hello-cluster<br></code></pre></td></tr></table></figure><p>进入 Master 节点 <code>crio-hello-cluster-control-plane</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-control-plane /bin/bash<br></code></pre></td></tr></table></figure><p>查看信息</p><ul><li><p>查看 k8s 版本</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@hello-crio-cluster-control-plane:/<span class="hljs-comment"># kubectl version</span><br>Client Version: v1.29.0<br>Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3<br>Server Version: v1.29.0<br></code></pre></td></tr></table></figure></li><li><p>查看 linux 发行版本</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@crio-hello-cluster-control-plane:/<span class="hljs-comment"># cat /etc/issue</span><br>Debian GNU/Linux 11 \n \l<br></code></pre></td></tr></table></figure></li><li><p>查看节点详细信息</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@crio-hello-cluster-control-plane:/<span class="hljs-comment"># kubectl get nodes -owide</span><br>NAME                               STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>crio-hello-cluster-control-plane   Ready    control-plane   34m   v1.29.0   172.18.0.6    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker          Ready    &lt;none&gt;          34m   v1.29.0   172.18.0.7    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker2         Ready    &lt;none&gt;          34m   v1.29.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure></li><li><p>查看容器运行时，都是 <code>containerd://1.7.1</code>。</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@crio-hello-cluster-control-plane:/<span class="hljs-comment"># kubectl get nodes -o json | jq -r &#x27;.items[] | &#123;Name: .metadata.name, ContainerRuntime: .status.nodeInfo.containerRuntimeVersion&#125;&#x27;</span><br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-control-plane&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker2&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure></li></ul><h4 id="2-2-2-为-WorkerNode-安装CRIO"><a href="#2-2-2-为-WorkerNode-安装CRIO" class="headerlink" title="2.2.2 为 WorkerNode 安装CRIO"></a>2.2.2 为 WorkerNode 安装CRIO</h4><p><code>kind-config.yaml</code> 配置文件指定了1个 MasterNode 和2个 WorkerNode，我们进入其中一个WorkerNode，按照官网 <a href="https://cri-o.io/"><code>https://cri-o.io/</code></a> 和 Github 官方仓库 <a href="https://github.com/cri-o/cri-o/blob/main/install.md"><code>https://github.com/cri-o/cri-o/blob/main/install.md</code></a> 上的指导，为其安装 CRIO-O。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-worker /bin/bash<br></code></pre></td></tr></table></figure><blockquote><p>注：以下的安装 CRI-O 的操作都是在此节点容器内进行的！</p></blockquote><ol><li><p>指定 <code>VERSION</code> 和 <code>OS</code> 环境变量。  </p><p> OS指定当前系统的版本，官网给出了对应关系，例如Ubuntu 20.04的主机，需要 <code>export OS=xUbuntu_20.04</code> 。 kind 创建的节点为Debian 11，对应 <code>export OS=Debian_11</code>。</p><p> VERSION指定crio的版本，支持主版本和具体版本的指定，例如 <code>1.24:1.24.1</code> 。这里有坑：**<code>opensuse</code> 上支持的最新的版本是 <code>1.24.6</code>** ，更高的版本如 github 上的最新版 <code>1.29.1</code> ，是无法在 <a href="https://download.opensuse.org/repositories"><code>https://download.opensuse.org/repositories</code></a> 中下载的，而文档没有说明这一点，只说明了支持主版本号和具体版本号。因此安装时，如果指定版本为 <code>1.29.1</code> ，会报错，提示无法找到文件。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> OS=Debian_11<br><span class="hljs-built_in">export</span> VERSION=1.24.6<br><span class="hljs-built_in">export</span> SUBVERSION=$(<span class="hljs-built_in">echo</span> <span class="hljs-variable">$VERSION</span> | awk -F<span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-string">&#x27;&#123;print $1&quot;.&quot;$2&#125;&#x27;</span>) <span class="hljs-comment"># 1.24</span><br></code></pre></td></tr></table></figure></li><li><p>安装相关工具</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install -y gnupg tzdata<br></code></pre></td></tr></table></figure></li><li><p>添加 apt 源，并下载安装 crio</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">CONTAINERS_URL=<span class="hljs-string">&quot;https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span class="hljs-variable">$&#123;OS&#125;</span>/&quot;</span><br><span class="hljs-comment"># crio 镜像地址</span><br>CRIO_URL=<span class="hljs-string">&quot;http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/<span class="hljs-variable">$&#123;SUBVERSION&#125;</span>:/<span class="hljs-variable">$&#123;VERSION&#125;</span>/<span class="hljs-variable">$&#123;OS&#125;</span>/&quot;</span><br><br><span class="hljs-comment"># 把上述 url 添加到 apt 的源</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;deb <span class="hljs-variable">$&#123;CONTAINERS_URL&#125;</span> /&quot;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;deb <span class="hljs-variable">$&#123;CRIO_URL&#125;</span> /&quot;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:<span class="hljs-variable">$&#123;VERSION&#125;</span>.list<br><br><span class="hljs-comment"># 下载密钥，并将其添加到 apt 信任密钥列表</span><br>curl -L <span class="hljs-variable">$&#123;CONTAINERS_URL&#125;</span>Release.key | apt-key add - || <span class="hljs-literal">true</span><br>curl -L <span class="hljs-variable">$&#123;CRIO_URL&#125;</span>Release.key | apt-key add - || <span class="hljs-literal">true</span><br><br><span class="hljs-comment"># 通过 apt 安装 cri-o 和 cri-o-runc</span><br>apt-get update<br>apt-get install -y cri-o cri-o-runc<br></code></pre></td></tr></table></figure><p> 如果下载太慢可以临时添加代理环境变量，例如：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;http:192.168.115.1:7890&quot;</span><br><span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;http:192.168.115.1:7890&quot;</span><br><span class="hljs-built_in">env</span> | grep proxy <span class="hljs-comment"># 检查</span><br></code></pre></td></tr></table></figure><blockquote><p>注意：在后续要取消 <code>proxy</code> 相关环境变量，或加入 <code>no_proxy</code> 的变量，否则导致此工作节点无法加入集群。</p></blockquote><p> 安装结束时会提示 <code>crictl.yaml</code> 已存在，因为该节点之前使用的是 <code>containerd</code> ，已经配置了 <code>crictl</code> 。这里选择覆盖 <code>Y</code> 。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Configuration file <span class="hljs-string">&#x27;/etc/crictl.yaml&#x27;</span><br>==&gt; File on system created by you or by a script.<br>==&gt; File also <span class="hljs-keyword">in</span> package provided by package maintainer.<br>What would you like to <span class="hljs-keyword">do</span> about it ?  Your options are:<br>    Y or I  : install the package maintainer<span class="hljs-string">&#x27;s version</span><br><span class="hljs-string">    N or O  : keep your currently-installed version</span><br><span class="hljs-string">    D     : show the differences between the versions</span><br><span class="hljs-string">    Z     : start a shell to examine the situation</span><br><span class="hljs-string">The default action is to keep your current version.</span><br><span class="hljs-string">*** crictl.yaml (Y/I/N/O/D/Z) [default=N] ? Y</span><br></code></pre></td></tr></table></figure></li><li><p>配置 cri-o</p><p> 安装完后配置 <code>cri-o</code></p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;[crio.runtime]</span><br><span class="hljs-string">cgroup_manager=\&quot;cgroupfs\&quot;</span><br><span class="hljs-string">conmon_cgroup=\&quot;pod\&quot;</span><br><span class="hljs-string">pause_image = \&quot;k8s.gcr.io/pause:3.2\&quot;</span><br><span class="hljs-string">storage_driver = \&quot;vfs\&quot;&quot;</span> &gt; /etc/crio/crio.conf<br><br></code></pre></td></tr></table></figure></li><li><p>设置 crictl</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将 crictl.yaml 中所有出现的 containerd 字符串替换为 crio</span><br>sed -i <span class="hljs-string">&#x27;s/containerd/crio/g&#x27;</span> /etc/crictl.yaml<br></code></pre></td></tr></table></figure></li><li><p>检查 containerd 和 crio</p><p> 检查当前状态</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># containerd 状态</span><br>$ systemctl status containerd | grep Active <span class="hljs-comment"># Runing</span><br> Active: active (running) since Fri 2024-01-26 08:23:49 UTC; 1h 8min ago<br><span class="hljs-comment"># crio 状态</span><br>$ systemctl status crio | grep Active <span class="hljs-comment"># Dead</span><br> Active: inactive (dead)<br></code></pre></td></tr></table></figure><p> 关闭 <code>containerd</code> 服务，启动 <code>crio</code> 服务</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl <span class="hljs-built_in">disable</span> containerd &amp;&amp; systemctl stop containerd<br>Removed /etc/systemd/system/multi-user.target.wants/containerd.service.<br>$ systemctl <span class="hljs-built_in">enable</span> crio &amp;&amp; systemctl start crio<br></code></pre></td></tr></table></figure><p> 再次检查状态</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl status containerd | grep Active <span class="hljs-comment"># Dead</span><br> Active: inactive (dead) since Fri 2024-01-26 09:36:03 UTC; 50s ago<br>$ systemctl status crio | grep Active <span class="hljs-comment"># Runing</span><br> Active: active (running) since Fri 2024-01-26 09:35:08 UTC; 1min 50s ago<br></code></pre></td></tr></table></figure></li></ol><h4 id="2-2-3-使用-Kubeadm-重新加入集群"><a href="#2-2-3-使用-Kubeadm-重新加入集群" class="headerlink" title="2.2.3 使用 Kubeadm 重新加入集群"></a>2.2.3 使用 <code>Kubeadm</code> 重新加入集群</h4><ol><li><p>检查节点状态</p><p> 重新开一个终端，进入 MasterNode，检查集群状态</p><blockquote><p>此命令在宿主机上执行</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-control-plane kubectl get nodes -owide<br>NAME                               STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>crio-hello-cluster-control-plane   Ready      control-plane   77m   v1.29.0   172.18.0.6    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker          NotReady   &lt;none&gt;          76m   v1.29.0   172.18.0.7    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://Unknown<br>crio-hello-cluster-worker2         Ready      &lt;none&gt;          76m   v1.29.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure><p> 观察到前面操作的节点 <code>crio-hello-cluster-worker</code> 已经进入 <code>NotReady</code> 状态，CONTAINER-RUNTIME 为 <code>containerd://Unknown</code> 。</p></li><li><p>使用 <code>kubeadm join</code> 将节点加入集群</p><p> 此时，为了将工作节点重新加入此集群，需要先在MasterNode上生成token：</p><blockquote><p>此命令在MasterNode中执行。记得关闭代理，否则无法加入。可以 <code>unset http_proxy &amp;&amp; unset https_proxy</code>。如果添加环境变量 <code>export no_proxy=&quot;localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,172.16.0.0/12&quot;</code> ，测试即使重启也无效。</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubeadm token create --print-join-command<br>kubeadm <span class="hljs-built_in">join</span> crio-hello-cluster-control-plane:6443 --token xeogi6.plnsolw9cd8457mc --discovery-token-ca-cert-hash sha256:0f84c44d447641f23345cd8269a27645fe149d19028a6afe6a07f0b7aac191c5 <br></code></pre></td></tr></table></figure><p> 但在 WorkerNode上执行返回的命令，会报错，提示预检错误，表明kubelet在执行系统验证时尝试加载名为“configs”的内核模块以解析内核配置，但未能在 <code>/lib/modules/5.15.0-92-generic</code> 目录下找到该模块。</p><blockquote><p>以下命令在WorkerNode <code>crio-hello-cluster-worker</code>上执行</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@hello-crio-worker:/home<span class="hljs-comment"># kubeadm join crio-hello-cluster-control-plane:6443 --token xeogi6.plnsolw9cd8457mc --discovery-token-ca-cert-hash sha256:0f84c44d447641f23345cd8269a27645fe149d19028a6afe6a07f0b7aac191c5 </span><br>[preflight] Running pre-flight checks<br>    [WARNING Swap]: swap is supported <span class="hljs-keyword">for</span> cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default<br>    [WARNING FileExisting-socat]: socat not found <span class="hljs-keyword">in</span> system path<br>[preflight] The system verification failed. Printing the output from the verification:<br>...<br>error execution phase preflight: [preflight] Some fatal errors occurred:<br>    [ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: <span class="hljs-string">&quot;configs&quot;</span>, output: <span class="hljs-string">&quot;modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-92-generic\n&quot;</span>, err: <span class="hljs-built_in">exit</span> status 1<br>[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`<br>To see the stack trace of this error execute with --v=5 or higher<br></code></pre></td></tr></table></figure><p> 暂时有两个解决办法：</p><ul><li>忽略问题。<a href="https://github.com/kubernetes/kubernetes/issues/41025"><code>https://github.com/kubernetes/kubernetes/issues/41025</code></a></li><li>升级内核镜像、配置容器。<a href="https://stackoverflow.com/questions/54128045/errors-while-creating-master-in-cluster-of-kubernetes-in-lxc-container"><code>https://stackoverflow.com/questions/54128045/errors-while-creating-master-in-cluster-of-kubernetes-in-lxc-container</code></a></li></ul><p> 目前采用的是忽略问题，暂时无法判断使用此选项的后续问题。加上忽略错误，节点添加成功！</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubeadm <span class="hljs-built_in">join</span> crio-hello-cluster-control-plane:6443 --token xeogi6.plnsolw9cd8457mc --discovery-token-ca-cert-hash sha256:0f84c44d447641f23345cd8269a27645fe149d19028a6afe6a07f0b7aac191c5 --ignore-preflight-errors=SystemVerification<br><br>[preflight] Running pre-flight checks<br>    [WARNING Swap]: swap is supported <span class="hljs-keyword">for</span> cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default<br>    [WARNING FileExisting-socat]: socat not found <span class="hljs-keyword">in</span> system path<br>[preflight] The system verification failed. Printing the output from the verification:<br>KERNEL_VERSION: 5.15.0-92-generic<br>OS: Linux<br>...<br>    [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: <span class="hljs-string">&quot;configs&quot;</span>, output: <span class="hljs-string">&quot;modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-92-generic\n&quot;</span>, err: <span class="hljs-built_in">exit</span> status 1<br>[preflight] Reading configuration from the cluster...<br>[preflight] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br>[kubelet-start] Writing kubelet configuration to file <span class="hljs-string">&quot;/var/lib/kubelet/config.yaml&quot;</span><br>[kubelet-start] Writing kubelet environment file with flags to file <span class="hljs-string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br>[kubelet-start] Starting the kubelet<br>[kubelet-start] Waiting <span class="hljs-keyword">for</span> the kubelet to perform the TLS Bootstrap...<br><br>This node has joined the cluster:<br>* Certificate signing request was sent to apiserver and a response was received.<br>* The Kubelet was informed of the new secure connection details.<br><br>Run <span class="hljs-string">&#x27;kubectl get nodes&#x27;</span> on the control-plane to see this node <span class="hljs-built_in">join</span> the cluster.<br></code></pre></td></tr></table></figure></li><li><p>检查集群状态</p><p> 从宿主机进入MasterNode</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu$ docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-control-plane /bin/bash<br></code></pre></td></tr></table></figure><blockquote><p>以下命令在 <code>crio-hello-cluster-control-plane</code> 中执行</p></blockquote><p> 检查节点状态，节点 <code>crio-hello-cluster-worker</code> 已处于 <code>Ready</code> 状态，容器运行时已切换为 <code>cri-o://1.24.6</code> 。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes -owide<br>NAME                               STATUS   ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>crio-hello-cluster-control-plane   Ready    control-plane   106m   v1.29.0   172.18.0.6    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker          Ready    &lt;none&gt;          11s    v1.29.0   172.18.0.7    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   cri-o://1.24.6<br>crio-hello-cluster-worker2         Ready    &lt;none&gt;          106m   v1.29.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes -o json | jq -r <span class="hljs-string">&#x27;.items[] | &#123;Name: .metadata.name, ContainerRuntime: .status.nodeInfo.containerRuntimeVersion&#125;&#x27;</span><br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-control-plane&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;cri-o://1.24.6&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker2&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure></li></ol><h3 id="2-3-自己构建-kind-node-镜像"><a href="#2-3-自己构建-kind-node-镜像" class="headerlink" title="2.3 自己构建 kind node 镜像"></a>2.3 自己构建 <code>kind node</code> 镜像</h3><p>后续文章尝试</p><hr><h2 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h2><h3 id="以前的配置无法创建集群"><a href="#以前的配置无法创建集群" class="headerlink" title="以前的配置无法创建集群"></a>以前的配置无法创建集群</h3><p>中途突然无法启动kubelet，任何多节点的集群都无法创建，即使使用前面博客文章的配置文件也无法创建集群，提示kubelet无法启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ bash create.sh -name <span class="hljs-built_in">test</span><br>cluster-name: ame<br>Creating cluster <span class="hljs-string">&quot;ame&quot;</span> ...<br> ✓ Ensuring node image (kindest/node:v1.29.0) 🖼<br> ✓ Preparing nodes 📦 📦 📦  <br> ✓ Writing configuration 📜 <br> ✗ Starting control-plane 🕹️ <br>Deleted nodes: [<span class="hljs-string">&quot;ame-worker&quot;</span> <span class="hljs-string">&quot;ame-worker2&quot;</span> <span class="hljs-string">&quot;ame-control-plane&quot;</span>]<br>ERROR: failed to create cluster: failed to init node with kubeadm: <span class="hljs-built_in">command</span> <span class="hljs-string">&quot;docker exec --privileged ame-control-plane kubeadm init --skip-phases=preflight --config=/kind/kubeadm.conf --skip-token-print --v=6&quot;</span> failed with error: <span class="hljs-built_in">exit</span> status 1<br>Command Output: I0125 11:40:19.477864     172 initconfiguration.go:260] loading configuration from <span class="hljs-string">&quot;/kind/kubeadm.conf&quot;</span><br>···<br>[control-plane] Creating static Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;kube-apiserver&quot;</span><br>I0125 11:40:24.816401     172 local.go:65] [etcd] wrote Static Pod manifest <span class="hljs-keyword">for</span> a <span class="hljs-built_in">local</span> etcd member to <span class="hljs-string">&quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br>···<br>[control-plane] Creating static Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;kube-controller-manager&quot;</span><br>I0125 11:40:24.820970     172 manifests.go:157] [control-plane] wrote static Pod manifest <span class="hljs-keyword">for</span> component <span class="hljs-string">&quot;kube-controller-manager&quot;</span> to <span class="hljs-string">&quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br>I0125 11:40:24.821181     172 manifests.go:102] [control-plane] getting StaticPodSpecs<br>[control-plane] Creating static Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;kube-scheduler&quot;</span><br>I0125 11:40:24.821653     172 manifests.go:128] [control-plane] adding volume <span class="hljs-string">&quot;kubeconfig&quot;</span> <span class="hljs-keyword">for</span> component <span class="hljs-string">&quot;kube-scheduler&quot;</span><br>I0125 11:40:24.822478     172 manifests.go:157] [control-plane] wrote static Pod manifest <span class="hljs-keyword">for</span> component <span class="hljs-string">&quot;kube-scheduler&quot;</span> to <span class="hljs-string">&quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br>I0125 11:40:24.822522     172 kubelet.go:68] Stopping the kubelet<br>[kubelet-start] Writing kubelet environment file with flags to file <span class="hljs-string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br>[kubelet-start] Writing kubelet configuration to file <span class="hljs-string">&quot;/var/lib/kubelet/config.yaml&quot;</span><br>[kubelet-start] Starting the kubelet<br>I0125 11:40:24.982554     172 waitcontrolplane.go:83] [wait-control-plane] Waiting <span class="hljs-keyword">for</span> the API server to be healthy<br>I0125 11:40:24.983290     172 loader.go:395] Config loaded from file:  /etc/kubernetes/admin.conf<br>[wait-control-plane] Waiting <span class="hljs-keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="hljs-string">&quot;/etc/kubernetes/manifests&quot;</span>. This can take up to 4m0s<br>I0125 11:40:24.985303     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?<span class="hljs-built_in">timeout</span>=10s  <span class="hljs-keyword">in</span> 0 milliseconds<br>···<br>[kubelet-check] Initial <span class="hljs-built_in">timeout</span> of 40s passed.<br>I0125 11:41:04.986148     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?<span class="hljs-built_in">timeout</span>=10s  <span class="hljs-keyword">in</span> 0 milliseconds<br>[kubelet-check] It seems like the kubelet isn<span class="hljs-string">&#x27;t running or healthy.</span><br><span class="hljs-string">[kubelet-check] The HTTP call equal to &#x27;</span>curl -sSL http://localhost:10248/healthz<span class="hljs-string">&#x27; failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.</span><br><span class="hljs-string">I0125 11:41:05.487445     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?timeout=10s  in 0 milliseconds</span><br><span class="hljs-string">···</span><br><span class="hljs-string">[kubelet-check] It seems like the kubelet isn&#x27;</span>t running or healthy.<br>[kubelet-check] The HTTP call equal to <span class="hljs-string">&#x27;curl -sSL http://localhost:10248/healthz&#x27;</span> failed with error: Get <span class="hljs-string">&quot;http://localhost:10248/healthz&quot;</span>: dial tcp [::1]:10248: connect: connection refused.<br>I0125 11:41:10.486636     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?<span class="hljs-built_in">timeout</span>=10s  <span class="hljs-keyword">in</span> 0 milliseconds<br>···<br>[kubelet-check] It seems like the kubelet isn<span class="hljs-string">&#x27;t running or healthy.</span><br><span class="hljs-string">[kubelet-check] The HTTP call equal to &#x27;</span>curl -sSL http://localhost:10248/healthz<span class="hljs-string">&#x27; failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.</span><br><span class="hljs-string">I0125 11:41:20.486427     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?timeout=10s  in 0 milliseconds</span><br><span class="hljs-string">···</span><br><span class="hljs-string">couldn&#x27;</span>t initialize a Kubernetes cluster<br>[kubelet-check] It seems like the kubelet isn<span class="hljs-string">&#x27;t running or healthy.</span><br><span class="hljs-string">[kubelet-check] The HTTP call equal to &#x27;</span>curl -sSL http://localhost:10248/healthz<span class="hljs-string">&#x27; failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Unfortunately, an error has occurred:</span><br><span class="hljs-string">timed out waiting for the condition</span><br><span class="hljs-string"></span><br><span class="hljs-string">This error is likely caused by:</span><br><span class="hljs-string">- The kubelet is not running</span><br><span class="hljs-string">- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)</span><br><span class="hljs-string">···</span><br></code></pre></td></tr></table></figure><p>先后检查了节点的 <code>kubelet status</code>、 <code>api-server</code>、 <code>container runtime</code> 等，无问题，最后参考 <a href="https://github.com/kubernetes-sigs/kind/issues/2702"><code>https://github.com/kubernetes-sigs/kind/issues/2702</code></a> ，可能是资源占用太大，因此删除了之前的集群，重新建立，错误解决。</p><h3 id="使用-kind-crio-镜像无法创建集群"><a href="#使用-kind-crio-镜像无法创建集群" class="headerlink" title="使用 kind-crio 镜像无法创建集群"></a>使用 kind-crio 镜像无法创建集群</h3><p>使用 <a href="https://github.com/warm-metal/kindest-base-crio/tree/main"><code>https://github.com/warm-metal/kindest-base-crio/tree/main</code></a> 提供的镜像，依然提示 kubelet 无法连接，错误同上。</p><p>查阅<a href="https://hub.docker.com/r/warmmetal/kindest-node-crio"><code>https://hub.docker.com/r/warmmetal/kindest-node-crio</code></a>，该镜像最近一个月更新过，但即使使用最新的也无法创建集群。 <a href="https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2"><code>https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2</code></a> 提到可能是 kind 和 k8s 更新太快，兼容性被打破，因此本实验抛弃了使用该仓库的镜像 <code>kindest-base-crio</code> 的方案。</p>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>cri</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件</title>
    <link href="/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_2_CNI/"/>
    <url>/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_2_CNI/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件"><a href="#kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件" class="headerlink" title="kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件"></a>kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><code>Container Network Interface（CNI）</code>，即容器网络接口，是一种标准化的API规范，旨在为容器提供一致且灵活的网络配置方案。在Kubernetes环境中，Kubelet通过调用遵循CNI标准的API与各类网络插件进行交互，以实现容器间多样化的网络连接与管理。接口定义的基本操作包括 <code>ADD</code> 和 <code>DEL</code> 等，负责将Pod添加到网络和从网络中删除。</p><p>k8s通过配置文件来设置使用的CNI插件。在创建pod之前，<a id="ref1"> 管理员可以放置好CNI配置文件 </a> ，当kubelet创建pod时，通过读取配置文件，根据其中指定的CNI插件，执行相应的二进制文件，从而配置Pod网络。当kubelet对Pod执行 ADD 操作时，对应的CNI插件将为Pod分配IP地址、设置路由规则以及其他必要的网络配置；当Pod终止或被删除时，kubelet会执行 DEL 操作，这时CNI插件负责清理相关的网络资源，如释放IP地址等。</p><p>不同的CNI插件有不同的实现方式，例如Overlay和Underlay。常见的CNI插件有Calico和Flannel等，它们各自具备不同的功能特点与适用场景，从而满足不同规模、性能及安全需求的容器网络部署要求。</p><ul><li><p><code>Overlay Network</code></p><ul><li><p>是一种虚拟化技术。通过在底层数据包的基础上，添加额外的封装头部信息，借助隧道打通网络连接，可以屏蔽底层网络通信方式的差异，兼容多种异构底层网络。部署方便；扩展性好。</p></li><li><p>例如：Flannel-vxlan 插件</p></li></ul></li><li><p><code>Underlay Network</code></p><ul><li><p>提供了真正的物理层和数据链路层的连接，网络性能直接取决于硬件设备。直接基于底层传输数据，没有额外数据封装，性能高，吞吐量大，延迟低。但在异构网络环境下扩展困难，网络配置复杂。某些基于Underlay的插件无法使用复杂均衡和服务发现。</p></li><li><p>例如：clico-bgp 插件</p></li></ul></li></ul><p>实际选择时，要从多个需求出发考虑，例如安全需求、负载均衡需求、性能需求等。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="1-创建集群，不配置CNI插件"><a href="#1-创建集群，不配置CNI插件" class="headerlink" title="1 创建集群，不配置CNI插件"></a>1 创建集群，不配置CNI插件</h3><p><code>kind</code> 默认使用了 <code>kindnetd</code> 作为网络插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it hello-cluster-worker <span class="hljs-built_in">ls</span> /etc/cni/net.d<br>10-kindnet.conflist<br></code></pre></td></tr></table></figure><p>为了实验，设置创建集群时不配置CNI插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kind create cluster --name cni-test-cluster --config kind-config.yaml<br><span class="hljs-comment"># 加载本地镜像（如果不加载会出现ImagePullBackOff、ErrImagePull等错误）</span><br>$ kind load docker-image go-hello-world-image:v0.0.1 --name cni-test-cluster<br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># kind-config.yaml</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span><br><span class="hljs-attr">networking:</span><br>  <span class="hljs-comment"># the default CNI will not be installed</span><br>  <span class="hljs-attr">disableDefaultCNI:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">nodes:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br></code></pre></td></tr></table></figure><p>查看 <code>/etc/cni/net.d</code> 下目录为空。</p><p>查看 <code>nodes</code>，状态为 not ready。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes<br>NAME                             STATUS     ROLES           AGE    VERSION<br>cni-test-cluster-control-plane   NotReady   control-plane   2d2h   v1.29.0<br>cni-test-cluster-worker          NotReady   &lt;none&gt;          2d2h   v1.29.0<br>cni-test-cluster-worker2         NotReady   &lt;none&gt;          2d2h   v1.29.0<br></code></pre></td></tr></table></figure><p>查看 <code>pods</code> 。<code>core-dns</code> 和 <code>local-path-provisioner</code> 不在运行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods --all-namespaces<br>NAMESPACE            NAME                                                     READY   STATUS    RESTARTS         AGE<br>kube-system          coredns-76f75df574-9swqg                                 0/1     Pending   0                47h<br>kube-system          coredns-76f75df574-j5mgx                                 0/1     Pending   0                47h<br>kube-system          etcd-cni-test-cluster-control-plane                      1/1     Running   0                3m7s<br>kube-system          kube-apiserver-cni-test-cluster-control-plane            1/1     Running   0                3m7s<br>kube-system          kube-controller-manager-cni-test-cluster-control-plane   1/1     Running   7 (3m25s ago)    47h<br>kube-system          kube-proxy-l8cpb                                         1/1     Running   71 (3m25s ago)   47h<br>kube-system          kube-proxy-nwpzn                                         1/1     Running   4 (3m25s ago)    47h<br>kube-system          kube-proxy-zhhfn                                         1/1     Running   71 (3m25s ago)   47h<br>kube-system          kube-scheduler-cni-test-cluster-control-plane            1/1     Running   7 (3m25s ago)    47h<br>local-path-storage   local-path-provisioner-6f8956fb48-f59n6                  0/1     Pending   0                47h<br></code></pre></td></tr></table></figure><p><code>kubectl apply -f cni-deploy.yaml</code> 部署服务后，Pod处于Pending状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl get pods<br>NAME                                   READY   STATUS    RESTARTS   AGE<br>cni-test-deployment-58569cd74c-qrxz5   0/1     Pending   0          142m<br>cni-test-deployment-58569cd74c-v86tf   0/1     Pending   0          142m<br>cni-test-deployment-58569cd74c-wg9rn   0/1     Pending   0          142m<br></code></pre></td></tr></table></figure><h3 id="2-手动添加flannel插件"><a href="#2-手动添加flannel插件" class="headerlink" title="2 手动添加flannel插件"></a>2 手动添加flannel插件</h3><p>Flannel插件旨在为不同节点上的容器重新规划IP地址的使用规则，分配同属一个内网且不重复的IP地址。</p><h4 id="2-1-准备好配置文件"><a href="#2-1-准备好配置文件" class="headerlink" title="2.1 准备好配置文件"></a>2.1 准备好配置文件</h4><p>如<a href="#ref1">上文</a>提到，添加CNI插件需要将配置文件放入节点的&#x2F;etc&#x2F;cni&#x2F;net.d。</p><p>可以自动化地进行。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 下载 kube-flannel.yaml 文件</span><br>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml<br><span class="hljs-comment"># 部署 flannel</span><br>kubectl apply -f kube-flannel.yml<br></code></pre></td></tr></table></figure><p>节点已经处于 <code>Ready</code> 状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes --all-namespaces<br>NAME                             STATUS   ROLES           AGE    VERSION<br>cni-test-cluster-control-plane   Ready    control-plane   2d2h   v1.29.0<br>cni-test-cluster-worker          Ready    &lt;none&gt;          2d2h   v1.29.0<br>cni-test-cluster-worker2         Ready    &lt;none&gt;          2d2h   v1.29.0<br></code></pre></td></tr></table></figure><p>配置文件已自动写入工作节点 <code>/etc/cni/net.d</code> 目录下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it cni-test-cluster-worker <span class="hljs-built_in">ls</span> /etc/cni/net.d<br>10-flannel.conflist<br></code></pre></td></tr></table></figure><p>但pods依然未就绪：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods --all-namespaces<br>NAMESPACE            NAME                                                     READY   STATUS              RESTARTS         AGE<br>default              cni-test-deployment-58569cd74c-qrxz5                     0/1     ContainerCreating   0                165m<br>default              cni-test-deployment-58569cd74c-v86tf                     0/1     ContainerCreating   0                165m<br>default              cni-test-deployment-58569cd74c-wg9rn                     0/1     ContainerCreating   0                165m<br>kube-flannel         kube-flannel-ds-bkl5b                                    1/1     Running             0                4m1s<br>kube-flannel         kube-flannel-ds-qfhwm                                    1/1     Running             0                4m1s<br>kube-flannel         kube-flannel-ds-xwn6m                                    1/1     Running             0                4m1s<br>kube-system          coredns-76f75df574-9swqg                                 0/1     ContainerCreating   0                2d2h<br>kube-system          coredns-76f75df574-j5mgx                                 0/1     ContainerCreating   0                2d2h<br>kube-system          etcd-cni-test-cluster-control-plane                      1/1     Running             0                5m31s<br>kube-system          kube-apiserver-cni-test-cluster-control-plane            1/1     Running             0                5m31s<br>kube-system          kube-controller-manager-cni-test-cluster-control-plane   1/1     Running             8 (6m26s ago)    2d2h<br>kube-system          kube-proxy-l8cpb                                         1/1     Running             72 (6m26s ago)   2d2h<br>kube-system          kube-proxy-nwpzn                                         1/1     Running             5 (6m26s ago)    2d2h<br>kube-system          kube-proxy-zhhfn                                         1/1     Running             72 (6m26s ago)   2d2h<br>kube-system          kube-scheduler-cni-test-cluster-control-plane            1/1     Running             8 (6m26s ago)    2d2h<br>local-path-storage   local-path-provisioner-6f8956fb48-f59n6                  0/1     ContainerCreating   0                2d2h<br><br></code></pre></td></tr></table></figure><p>通过 <code>kubectl describe pod cni-test-deployment-58569cd74c-qrxz5</code> 查看信息，提示在委派 <code>ADD</code> 操作时，无法找到 <code>/opt/cni/bin</code> 目录下的<code>bridge</code>二进制文件插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl describe pod cni-test-deployment-58569cd74c-qrxz5<br>...<br>Conditions:<br>  Type                        Status<br>  PodReadyToStartContainers   False <br>  Initialized                 True <br>  Ready                       False <br>  ContainersReady             False <br>  PodScheduled                True <br>Events:<br>  Type     Reason                  Age                     From               Message<br>  ----     ------                  ----                    ----               -------<br>  Warning  FailedCreatePodSandBox  4m16s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network <span class="hljs-keyword">for</span> sandbox <span class="hljs-string">&quot;0ca3e187ace57dd4af46bb8e4c60b5b5973510f5f4a02e8efe1954d34f1b7101&quot;</span>: plugin <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;flannel&quot;</span> failed (add): failed to delegate add: failed to find plugin <span class="hljs-string">&quot;bridge&quot;</span> <span class="hljs-keyword">in</span> path [/opt/cni/bin]<br></code></pre></td></tr></table></figure><h4 id="2-2-手动安装插件的二进制文件"><a href="#2-2-手动安装插件的二进制文件" class="headerlink" title="2.2 手动安装插件的二进制文件"></a>2.2 手动安装插件的二进制文件</h4><p>在运行 kind 的宿主机上下载文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 下载文件</span><br>$ wget https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz<br></code></pre></td></tr></table></figure><p>复制到多个工作节点（docker容器），并解压。创建脚本自动处理这一过程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ./unzip_to_docker.sh</span><br><br><span class="hljs-comment"># 获取所有名称以 &#x27;cni-test-cluster&#x27; 开头的容器</span><br>container_names=$(docker ps --format <span class="hljs-string">&quot;&#123;&#123;.Names&#125;&#125;&quot;</span> | grep <span class="hljs-string">&#x27;^cni-test-cluster&#x27;</span>)<br><br><span class="hljs-comment"># 定义本地压缩文件路径和目标解压目录（请替换为实际值）</span><br>local_archive=<span class="hljs-string">&quot;cni-plugins-linux-amd64-v1.4.0.tgz&quot;</span><br>target_directory=<span class="hljs-string">&quot;/opt/cni/bin&quot;</span><br><br><span class="hljs-comment"># 遍历每个匹配到的容器</span><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> <span class="hljs-variable">$container_names</span>; <span class="hljs-keyword">do</span><br>    <span class="hljs-comment"># 将本地压缩文件复制到容器内</span><br>    docker <span class="hljs-built_in">cp</span> <span class="hljs-string">&quot;<span class="hljs-variable">$local_archive</span>&quot;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$name</span>:<span class="hljs-variable">$target_directory</span>&quot;</span><br>    <span class="hljs-comment"># 在容器内部执行解压命令</span><br>    docker <span class="hljs-built_in">exec</span> -it <span class="hljs-string">&quot;<span class="hljs-variable">$name</span>&quot;</span> bash -c <span class="hljs-string">&quot;cd &#x27;<span class="hljs-variable">$target_directory</span>&#x27; &amp;&amp; tar -zxvf &#x27;<span class="hljs-variable">$local_archive</span>&#x27;&quot;</span><br>    <span class="hljs-comment"># 展示文件夹下内容</span><br>    docker <span class="hljs-built_in">exec</span> -it <span class="hljs-string">&quot;<span class="hljs-variable">$name</span>&quot;</span> bash -c <span class="hljs-string">&quot;cd &#x27;<span class="hljs-variable">$target_directory</span>&#x27; &amp;&amp; ls&quot;</span><br><span class="hljs-keyword">done</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Finished deploying files to containers.&quot;</span><br></code></pre></td></tr></table></figure><p>在运行 kind 的宿主机上运行此脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ bash unzip_to_docker.sh <br>Successfully copied 46.9MB to cni-test-cluster-control-plane:/opt/cni/bin<br>./<br>./loopback<br>./bandwidth<br>./ptp<br>./vlan<br>./host-device<br>./tuning<br>./vrf<br>./sbr<br>./tap<br>./dhcp<br>./static<br>./firewall<br>./macvlan<br>./dummy<br>./bridge<br>./ipvlan<br>./portmap<br>./host-local<br>bandwidth  cni-plugins-linux-amd64-v1.4.0.tgz  dummy flannel      host-local  loopback  portmap  sbr     tap     vlan<br>bridge   dhcp       firewall  host-device  ipvlan  macvlan   ptp      static  tuning  vrf<br>Successfully copied 46.9MB to cni-test-cluster-worker2:/opt/cni/bin<br>./<br>./loopback<br>./bandwidth<br>./ptp<br>./vlan<br>./host-device<br>./tuning<br>./vrf<br>./sbr<br>./tap<br>./dhcp<br>./static<br>./firewall<br>./macvlan<br>./dummy<br>./bridge<br>./ipvlan<br>./portmap<br>./host-local<br>bandwidth  cni-plugins-linux-amd64-v1.4.0.tgz  dummy flannel      host-local  loopback  portmap  sbr     tap     vlan<br>bridge   dhcp       firewall  host-device  ipvlan  macvlan   ptp      static  tuning  vrf<br>Successfully copied 46.9MB to cni-test-cluster-worker:/opt/cni/bin<br>./<br>./loopback<br>./bandwidth<br>./ptp<br>./vlan<br>./host-device<br>./tuning<br>./vrf<br>./sbr<br>./tap<br>./dhcp<br>./static<br>./firewall<br>./macvlan<br>./dummy<br>./bridge<br>./ipvlan<br>./portmap<br>./host-local<br>bandwidth  cni-plugins-linux-amd64-v1.4.0.tgz  dummy flannel      host-local  loopback  portmap  sbr     tap     vlan<br>bridge   dhcp       firewall  host-device  ipvlan  macvlan   ptp      static  tuning  vrf<br>Finished deploying files to containers.<br></code></pre></td></tr></table></figure><p>进入workNode查看网络，可以看到网卡flannel.1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it cni-test-cluster-worker /bin/bash<br>root@cni-test-cluster-worker:/<span class="hljs-comment"># ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000<br>    <span class="hljs-built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host <br>       valid_lft forever preferred_lft forever<br>2: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default <br>    <span class="hljs-built_in">link</span>/ether a6:b3:45:67:33:b2 brd ff:ff:ff:ff:ff:ff<br>    inet 10.244.1.0/32 scope global flannel.1<br>       valid_lft forever preferred_lft forever<br>10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default <br>    <span class="hljs-built_in">link</span>/ether 02:42:ac:12:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0<br>    inet 172.18.0.4/16 brd 172.18.255.255 scope global eth0<br>       valid_lft forever preferred_lft forever<br>    inet6 fc00:f853:ccd:e793::4/64 scope global nodad <br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::42:acff:fe12:4/64 scope <span class="hljs-built_in">link</span> <br>       valid_lft forever preferred_lft forever<br></code></pre></td></tr></table></figure><p>进入masterNode后，检查子网环境变量，已写入。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@cni-test-cluster-control-plane:/var/run/flannel<span class="hljs-comment"># cat subnet.env </span><br>FLANNEL_NETWORK=10.244.0.0/16<br>FLANNEL_SUBNET=10.244.0.1/24<br>FLANNEL_MTU=1450<br>FLANNEL_IPMASQ=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>检查pods，<code>coredns</code> 和自定义的pod <code>cni-test-deployment</code> 已正常运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods --all-namespaces<br>NAMESPACE            NAME                                                     READY   STATUS    RESTARTS       AGE<br>default              cni-test-deployment-58569cd74c-7hdmp                     1/1     Running   0              9m52s<br>default              cni-test-deployment-58569cd74c-9wcqw                     1/1     Running   0              9m52s<br>default              cni-test-deployment-58569cd74c-bbwhk                     1/1     Running   0              9m52s<br>kube-flannel         kube-flannel-ds-7dfkb                                    1/1     Running   1 (35m ago)    56m<br>kube-flannel         kube-flannel-ds-vfvbz                                    1/1     Running   1 (35m ago)    56m<br>kube-flannel         kube-flannel-ds-zc88f                                    1/1     Running   2 (34m ago)    56m<br>kube-system          coredns-76f75df574-9swqg                                 1/1     Running   0              2d3h<br>kube-system          coredns-76f75df574-j5mgx                                 1/1     Running   0              2d3h<br>kube-system          etcd-cni-test-cluster-control-plane                      1/1     Running   1 (35m ago)    95m<br>kube-system          kube-apiserver-cni-test-cluster-control-plane            1/1     Running   1 (35m ago)    95m<br>kube-system          kube-controller-manager-cni-test-cluster-control-plane   1/1     Running   9 (35m ago)    2d3h<br>kube-system          kube-proxy-l8cpb                                         1/1     Running   73 (35m ago)   2d3h<br>kube-system          kube-proxy-nwpzn                                         1/1     Running   6 (35m ago)    2d3h<br>kube-system          kube-proxy-zhhfn                                         1/1     Running   73 (35m ago)   2d3h<br>kube-system          kube-scheduler-cni-test-cluster-control-plane            1/1     Running   9 (35m ago)    2d3h<br>local-path-storage   local-path-provisioner-6f8956fb48-f59n6                  1/1     Running   0              2d3h<br></code></pre></td></tr></table></figure><p>进入在 <code>worker2</code> 节点上运行的 <code>IP = 10.244.2.7</code> 的pod ，访问在 <code>worker</code> 节点上运行的<code>IP = 10.244.1.8</code> 的pod，可以访问。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods -owide<br>NAME                                   READY   STATUS    RESTARTS   AGE   IP           NODE                       NOMINATED NODE   READINESS GATES<br>cni-test-deployment-58569cd74c-7hdmp   1/1     Running   0          11m   10.244.2.7   cni-test-cluster-worker2   &lt;none&gt;           &lt;none&gt;<br>cni-test-deployment-58569cd74c-9wcqw   1/1     Running   0          11m   10.244.1.8   cni-test-cluster-worker    &lt;none&gt;           &lt;none&gt;<br>cni-test-deployment-58569cd74c-bbwhk   1/1     Running   0          11m   10.244.1.7   cni-test-cluster-worker    &lt;none&gt;           &lt;none&gt;<br><br>$ kubectl <span class="hljs-built_in">exec</span> cni-test-deployment-58569cd74c-7hdmp -it sh<br>/app <span class="hljs-comment"># curl 10.244.1.8:8080/test</span><br>hello world<br>/app <span class="hljs-comment"># exit</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>cni</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【1】使用 kind 搭建本地集群</title>
    <link href="/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_1_%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4/"/>
    <url>/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_1_%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【1】使用-kind-搭建本地集群"><a href="#kubelabs——k8s学习与实验【1】使用-kind-搭建本地集群" class="headerlink" title="kubelabs——k8s学习与实验【1】使用 kind 搭建本地集群"></a>kubelabs——k8s学习与实验【1】使用 kind 搭建本地集群</h1><h2 id="1-概念总结"><a href="#1-概念总结" class="headerlink" title="1 概念总结"></a>1 概念总结</h2><h3 id="1-1-kubernetes（k8s）"><a href="#1-1-kubernetes（k8s）" class="headerlink" title="1.1 kubernetes（k8s）"></a>1.1 kubernetes（k8s）</h3><ul><li>kubernetes（k8s）是用于管理云平台中的多个容器化应用的工具。</li><li>理解：<ul><li>现在的大型程序被分割为多个服务，各自运行在单独的容器里。</li><li>为了提升性能和可用性，多个容器需要分散在多个物理机器上，并拥有备份。</li><li>多个服务和容器需要通信</li><li>k8s用于自动化管理此系统。</li></ul></li></ul><h3 id="1-2-组成"><a href="#1-2-组成" class="headerlink" title="1.2 组成"></a>1.2 组成</h3><h4 id="物理层面"><a href="#物理层面" class="headerlink" title="物理层面"></a>物理层面</h4><ul><li>Master节点<ul><li>物理服务器</li><li>kubectl：操作k8s的命令行工具。添加deployment、service等。</li><li>apiServer：对象的请求和调用操作通过此模块提供的接口进行。</li><li>kube-scheduler：资源调度</li><li>kube-controller-manager：维护集群状态</li><li>etcd分布式存储：保存集群状态</li></ul></li><li>Worker节点<ul><li>物理服务器</li><li>kubelet：创建和管理pod；与master通讯</li><li>kube-proxy：负责不同pod通信。</li><li>资源（Pod等）</li></ul></li></ul><h4 id="逻辑层面"><a href="#逻辑层面" class="headerlink" title="逻辑层面"></a>逻辑层面</h4><ul><li><p>主要资源对象</p><ul><li>Container：容器化应用。</li><li>Pod：k8s创建或销毁的最小单位。一个pod包含一个或多个容器。Pod中容器共享网络、存储和计算资源。</li><li>Service：通过labels绑定pod，提供Cluster IP地址和服务名来访问Pod资源。好处是可以实现多个Pod负载均衡，并固定访问的url，Pod IP或Cluster IP变动时不会产生影响。</li><li>Deployment：管理和控制Pod的数量，确保每时每刻有用户要求数量的 Pod 在工作，某Pod出现问题就重新拉起。</li></ul></li><li><p>为了更好的提供开放、扩展、规范等能力的规范</p><ul><li>CNI：容器网络接口。定义通信规范，屏蔽底层使用不同网络插件的差异。</li><li>CSI：容器存储接口。将任意存储系统规范地暴露给容器化应用程序。</li><li>CRI：容器运行时接口。定义规范，使k8s可以兼容多种容器引擎，如docker、frakti等。</li></ul></li></ul><h2 id="2-实验操作"><a href="#2-实验操作" class="headerlink" title="2 实验操作"></a>2 实验操作</h2><h3 id="2-1-环境准备"><a href="#2-1-环境准备" class="headerlink" title="2.1 环境准备"></a>2.1 环境准备</h3><ul><li>宿主机：Windows下通过vmware运行的Ubuntu。磁盘：30GB，内存：5.2G  </li><li>选择用kind搭建集群</li></ul><h3 id="2-2-自制服务镜像"><a href="#2-2-自制服务镜像" class="headerlink" title="2.2 自制服务镜像"></a>2.2 自制服务镜像</h3><ul><li>go 服务<br>请求ip:8080&#x2F;test返回字符串”hello world”</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br>        <span class="hljs-string">&quot;fmt&quot;</span><br>        <span class="hljs-string">&quot;net/http&quot;</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">helloWorld</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<br>        <span class="hljs-keyword">if</span> r.Method == http.MethodGet &amp;&amp; r.URL.Path == <span class="hljs-string">&quot;/test&quot;</span> &#123;<br>                w.WriteHeader(http.StatusOK)<br>                fmt.Fprintln(w, <span class="hljs-string">&quot;hello world&quot;</span>)<br>                <span class="hljs-keyword">return</span><br>        &#125;<br>        http.NotFound(w, r)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>        http.HandleFunc(<span class="hljs-string">&quot;/test&quot;</span>, helloWorld)<br><br>        <span class="hljs-comment">// 监听并在 8080 端口启动服务</span><br>        fmt.Println(<span class="hljs-string">&quot;Server is listening on port 8080...&quot;</span>)<br>        http.ListenAndServe(<span class="hljs-string">&quot;:8080&quot;</span>, <span class="hljs-literal">nil</span>)<br>&#125;<br><br></code></pre></td></tr></table></figure><ul><li>Dockerfile</li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs Dockerfile"><span class="hljs-keyword">ARG</span> IMAGE_VERSION=v0.<span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 使用官方的Golang基础镜像作为父镜像</span><br><span class="hljs-keyword">FROM</span> golang:latest as builder<br><br><span class="hljs-comment"># 设置工作目录</span><br><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span><br><br><span class="hljs-comment"># 将项目文件复制到容器的工作目录中</span><br><span class="hljs-keyword">COPY</span><span class="language-bash"> . .</span><br><br><span class="hljs-keyword">ENV</span> CGO_ENABLED=<span class="hljs-number">0</span><br><br><span class="hljs-comment"># 构建应用</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> go build -o main .</span><br><br><span class="hljs-comment"># 使用一个新的轻量级基础镜像，例如Alpine</span><br><span class="hljs-keyword">FROM</span> alpine:latest<br><br><span class="hljs-keyword">RUN</span><span class="language-bash"> apk update</span><br><br><span class="hljs-comment"># 安装curl和其他可能需要的依赖</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> apk add --no-cache curl</span><br><br><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span><br><br><span class="hljs-comment"># 复制编译好的二进制文件到新镜像中</span><br><span class="hljs-keyword">COPY</span><span class="language-bash"> --from=builder /app/main /app/</span><br><br><span class="hljs-comment"># 设置工作目录和提供运行时环境</span><br><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span><br><span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">&quot;./main&quot;</span>]</span><br><br><span class="hljs-comment"># 声明运行时容器提供的服务端口</span><br><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8080</span><br><br></code></pre></td></tr></table></figure><ul><li>运行命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker build -t go-hello-world-image:v0.0.1 .<br></code></pre></td></tr></table></figure><h3 id="2-3-kind创建集群"><a href="#2-3-kind创建集群" class="headerlink" title="2.3 kind创建集群"></a>2.3 kind创建集群</h3><ul><li>文件结构</li></ul><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs txt">├── go-hello<br>│   ├── app<br>│   ├── Dockerfile<br>│   ├── go.mod<br>│   └── main.go<br>├── hello-deploy.yaml<br>├── hello-service.yaml<br>├── kind-config.yaml<br>├── nginx-deployment.yaml<br>└── nginx-service.yaml<br></code></pre></td></tr></table></figure><ul><li>集群配置文件：kind-config.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span><br><span class="hljs-attr">nodes:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span> <span class="hljs-comment"># 指定worknode镜像</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br></code></pre></td></tr></table></figure><ul><li>deployment配置文件：hello-deploy.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">go-hello-world-deployment</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">go-hello-world</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">go-hello-world</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">go-hello-world-container</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">go-hello-world-image:v0.0.1</span> <span class="hljs-comment"># 替换为您的镜像名称和标签</span><br>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span> <span class="hljs-comment"># 假设服务监听在容器内部的8080端口</span><br></code></pre></td></tr></table></figure><ul><li>service配置文件：hello-service.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">hello-service</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span> <span class="hljs-comment"># 或者 LoadBalancer 如果在模拟云环境中</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">go-hello-world</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">30007</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span><br>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30008</span> <span class="hljs-comment"># 如果是NodePort类型，设置一个可用端口</span><br></code></pre></td></tr></table></figure><ul><li>搭建集群</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建集群</span><br>kind create cluster --name hello-cluster --config kind-config.yaml<br><span class="hljs-comment"># 加载镜像</span><br>kind load docker-image go-hello-world-image:v0.0.1 --name hello-cluster<br><span class="hljs-comment"># 创建 deployment</span><br>kubectl apply -f hello-deploy.yaml<br><span class="hljs-comment"># 创建 service</span><br>kubectl apply -f hello-service.yaml<br></code></pre></td></tr></table></figure><ul><li>Docker信息概览</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ docker ps<br>CONTAINER ID   IMAGE                         COMMAND                  CREATED       STATUS       PORTS                                         NAMES<br>a20802131005   kindest/node:v1.29.0          <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   4 hours ago   Up 4 hours                                                 hello-cluster-worker<br>710f6226c8ab   kindest/node:v1.29.0          <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   4 hours ago   Up 4 hours   127.0.0.1:35839-&gt;6443/tcp                     hello-cluster-control-plane<br>c21dc643b049   kindest/node:v1.29.0          <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   4 hours ago   Up 4 hours                                                 hello-cluster-worker2<br>bc232db8553e   go-hello-world-image:v0.0.1   <span class="hljs-string">&quot;./main&quot;</span>                 4 hours ago   Up 4 hours   0.0.0.0:10086-&gt;8080/tcp, :::10086-&gt;8080/tcp   hello-local<br></code></pre></td></tr></table></figure><ul><li>nodes信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl get nodes -o wide<br>NAME                          STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>hello-cluster-control-plane   Ready    control-plane   3h41m   v1.29.0   172.18.0.2    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-91-generic   containerd://1.7.1<br>hello-cluster-worker          Ready    &lt;none&gt;          3h41m   v1.29.0   172.18.0.4    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-91-generic   containerd://1.7.1<br>hello-cluster-worker2         Ready    &lt;none&gt;          3h41m   v1.29.0   172.18.0.3    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-91-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure><ul><li>pods信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl get pods -o wide<br>NAME                                         READY   STATUS    RESTARTS   AGE     IP           NODE                    NOMINATED NODE   READINESS GATES<br>go-hello-world-deployment-58569cd74c-85hw5   1/1     Running   0          3h33m   10.244.1.5   hello-cluster-worker    &lt;none&gt;           &lt;none&gt;<br>go-hello-world-deployment-58569cd74c-bd8ht   1/1     Running   0          3h33m   10.244.2.3   hello-cluster-worker2   &lt;none&gt;           &lt;none&gt;<br>go-hello-world-deployment-58569cd74c-qgfm7   1/1     Running   0          3h33m   10.244.1.4   hello-cluster-worker    &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><ul><li>service信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl get service -o wide<br>NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE     SELECTOR<br>hello-service   NodePort    10.96.2.242   &lt;none&gt;        8080:30008/TCP   3h13m   app=go-hello-world<br>kubernetes      ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP          3h42m   &lt;none&gt;<br></code></pre></td></tr></table></figure><h3 id="2-4-结果与问题"><a href="#2-4-结果与问题" class="headerlink" title="2.4 结果与问题"></a>2.4 结果与问题</h3><p>使用Kind创建了集群。自定义的服务监听 <code>port = 8080</code>，service监听 <code>sport = 30007</code>，在workNode上开放 <code>NodePort=30008</code>。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h5 id="1-进入到pod内部"><a href="#1-进入到pod内部" class="headerlink" title="1 进入到pod内部"></a>1 进入到pod内部</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl <span class="hljs-built_in">exec</span> -it go-hello-world-deployment-58569cd74c-85hw5 sh<br>kubectl <span class="hljs-built_in">exec</span> [POD] [COMMAND] is DEPRECATED and will be removed <span class="hljs-keyword">in</span> a future version. Use kubectl <span class="hljs-built_in">exec</span> [POD] -- [COMMAND] instead.<br>/app <span class="hljs-comment"># curl localhost:8080/test</span><br>hello world<br>/app <span class="hljs-comment"># curl 10.244.1.4:8080/test</span><br>hello world<br>/app <span class="hljs-comment"># curl 10.96.2.242:30007/test</span><br>hello world<br>/app <span class="hljs-comment"># curl hello-service:30007/test</span><br>hello world<br></code></pre></td></tr></table></figure><ul><li>可以用 <code>localhost:port</code>访问本pod服务</li><li>可以用 <code>pod ip:port</code>访问对应pod的服务</li><li>创建Service后，可以用 <code>clusterIP:sport</code>可以访问服务</li><li>创建Service后，<strong>无法</strong>通过 <code>hello-service:sport</code>访问服务 <a href="#bug1">[1]</a></li></ul><h5 id="2-进入到workNode内部"><a href="#2-进入到workNode内部" class="headerlink" title="2 进入到workNode内部"></a>2 进入到workNode内部</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it hello-cluster-worker sh<br></code></pre></td></tr></table></figure><ul><li>使用 <code>localhost:NodePort</code>可以访问本workNode上的服务；</li><li>使用 <code>WorkNodeIp:NodePort</code>和 <code>其他WorkNodeIp:NodePort</code><strong>无法</strong>访问服务 <a href="#bug2">[2]</a></li></ul><h4 id="问题与解决"><a href="#问题与解决" class="headerlink" title="问题与解决"></a>问题与解决</h4><p><a id="bug1">[1]</a> 检查kube-dns，无明显错误，重启后解决。</p><p><a id="bug2">[2]</a> 排查过程如下：</p><ul><li><p>检查 <code>ufw</code>防火墙：未开启防火墙</p></li><li><p>检查CNI日志，workNode拥有子网段，可以分配给pod<br>Node hello-cluster-control-plane has CIDR [<code>10.244.0.0/24</code>]<br>Node hello-cluster-worker has CIDR [<code>10.244.1.0/24</code>]<br>Node hello-cluster-worker2 has CIDR [<code>10.244.2.0/24</code>]</p></li><li><p>检查 <code>ping</code>：可以ping通IP</p></li><li><p>通过 <code>curl -v</code> 查看详细信息，发现走了代理 <code>192.168.115.1</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Windows的IP为192.168.115.1；在这个windows里面，我运行了一个ubuntu虚拟机，IP为192.168.115.128；</span><br><span class="hljs-comment"># windows上的7890端口设置了代理服务器，用于访问资源；</span><br><span class="hljs-comment"># 为了让ubuntu虚拟机同样能够访问资源，我配置ubuntu的代理服务器为192.168.115.1:7890；   </span><br><br>curl -v 172.18.0.3:30008<br><br>* Uses proxy <span class="hljs-built_in">env</span> variable no_proxy == <span class="hljs-string">&#x27;172.18.0.0/16,fc00:f853:ccd:e793::/64,localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24,::1,10.96.0.0/16,10.244.0.0/16,hello-cluster-control-plane,hello-cluster-worker,hello-cluster-worker2,.svc,.svc.cluster,.svc.cluster.local&#x27;</span><br>* Uses proxy <span class="hljs-built_in">env</span> variable http_proxy == <span class="hljs-string">&#x27;http://192.168.115.1:7890/&#x27;</span><br>*   Trying 192.168.115.1:7890...<br>* Connected to 192.168.115.1 (192.168.115.1) port 7890 (<span class="hljs-comment">#0)</span><br>&gt; GET http://172.18.0.3:30008/ HTTP/1.1<br>&gt; Host: 172.18.0.3:30008<br>&gt; User-Agent: curl/7.74.0<br>&gt; Accept: */*<br>&gt; Proxy-Connection: Keep-Alive<br>&gt; <br>* Mark bundle as not supporting multiuse<br>&lt; HTTP/1.1 502 Bad Gateway<br>&lt; Connection: keep-alive<br>&lt; Keep-Alive: <span class="hljs-built_in">timeout</span>=4<br>&lt; Proxy-Connection: keep-alive<br>&lt; Content-Length: 0<br>&lt; <br></code></pre></td></tr></table></figure></li><li><p><code>curl --noproxy</code>：强制使用curl不走代理，访问成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># curl -v --noproxy &#x27;*&#x27; 172.18.0.3:30008/test</span><br>hello world<br></code></pre></td></tr></table></figure></li><li><p>检查workNode上的环境变量：确实存在no_proxy环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># env | grep no_proxy</span><br>no_proxy=172.18.0.0/16,fc00:f853:ccd:e793::/64,localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24,::1,10.96.0.0/16,10.244.0.0/16,hello-cluster-control-plane,hello-cluster-worker,hello-cluster-worker2,.svc,.svc.cluster,.svc.cluster.local<span class="hljs-string">&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p>使用其他工具 <code>wget</code>访问接口：也走了代理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># wget -O - http://172.18.0.3:30008/test</span><br></code></pre></td></tr></table></figure></li><li><p>现在问题已经在于：在kind创建的这个workNode中，path包含了proxy和no_proxy，但no_proxy没有起作用，在curl和wget都出现这个现象。</p></li><li><p>回到vmware ubuntu界面，把整个虚拟机的代理关闭。</p></li><li><p>重新创建集群，使用<code>workNode:NodePort</code><strong>可以</strong>访问。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>kind</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
