<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【5】容器存储接口CSI（下）：NFS CSI 插件实验</title>
    <link href="/2024/02/10/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_5_CSI/"/>
    <url>/2024/02/10/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_5_CSI/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【5】容器存储接口CSI（下）：NFS-CSI-插件实验"><a href="#kubelabs——k8s学习与实验【5】容器存储接口CSI（下）：NFS-CSI-插件实验" class="headerlink" title="kubelabs——k8s学习与实验【5】容器存储接口CSI（下）：NFS CSI 插件实验"></a>kubelabs——k8s学习与实验【5】容器存储接口CSI（下）：NFS CSI 插件实验</h1><h2 id="1-相关概念简介"><a href="#1-相关概念简介" class="headerlink" title="1 相关概念简介"></a>1 相关概念简介</h2><p>上一篇文章中介绍了 Kubernetes 存储相关的概念，并进行了实验。本篇文章更详细地介绍其中的 PV 和 PVC 的持久化方式，并引入 CSI（Container Storage Interface）的概念。</p><h3 id="1-1-StorageClass"><a href="#1-1-StorageClass" class="headerlink" title="1.1 StorageClass"></a>1.1 StorageClass</h3><p>上一篇文章提到，PVC 用来描述 pod 希望使用的 PV 的属性，比如大小，读写权限等等。用户只需要关注 PVC ，VolumeController 的 PersistVolumeController 会根据 PVC 去寻找匹配的 PV 进行绑定。PV 资源的创建是我们手动创建的本地卷。</p><p>在实际生产中，手动创建并提供 PV 限制了部署的灵活性。StorageClass是为了解决 PV 的供应（Provisioning）问题，例如，当集群管理员创建的 PV 中，任何一个都无法满足 PVC 的需求，Kubernetes 系统就会动态创建 PV 。这一动态供应（Dynamic Provisioning） 过程由 Storage Class负责，如下图所示。</p><p><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240209143746.png" alt="storage-class"></p><h3 id="1-2-CSI"><a href="#1-2-CSI" class="headerlink" title="1.2 CSI"></a>1.2 CSI</h3><p>CSI 是一个用于为 Kubernetes 集群提供持久化存储的标准接口。在 CSI 之前，扩展存储插件功能的方式相对复杂且不统一。CSI 推出之后，类似 CNI、CRI，该接口提供规范，第三方存储供应商（如云提供商和存储厂商）按照此规范设计插件，Kubernetes 可以使用该插件创建和管理持久化存储，从而支持更多的存储解决方案，如块存储、文件存储或对象存储。CSI 插件通过 grpc 与 Kubernetes 的系统组件进行通信。</p><blockquote><p>kubelet不直接调用CSI接口来管理PV的整个生命周期。Kubernetes中的控制器组件（如CSI Provisioner(external-provisioner)和CSI Attacher）才是直接与CSI驱动程序交互，负责动态地根据PVC的需求进行PV的供应（Provisioning）、绑定（Binding）及挂载到节点上。</p></blockquote><p>CSI 主要由两个部分组成：在控制节点的 CSI Provisioner 和 工作节点的 CSI Plugin。</p><p>CSI Provisioner 运行在 Kubernetes 集群的控制平面节点上，通常是在集群的管理服务器上。其主要功能是处理存储卷的高级生命周期管理和自动化操作。如创建、删除 （PV）或动态供应 PVC 时所需的存储资源。CSI 控制器插件通过监听 Kubernetes API 对象的变化并与之交互来实现上述功能。它们使用 gRPC 接口与 Kubernetes 控制器管理器进行通信。</p><p>CSI Plugin 部署在每个 Kubernetes 工作节点上，它们负责执行与存储卷挂载到节点本地以及卸载存储卷相关的低级操作。<br>包括：</p><ul><li>将远程或网络存储资源在节点上实际挂载为文件系统或者作为原始块设备。</li><li>在 Pod 创建或调度到该节点时，根据 PVC 请求将合适的 PV 挂载到Pod容器内。</li><li>当 Pod 销毁或者从节点上移除时，执行对应的 detach 操作，从节点上卸载存储卷。</li></ul><p>同样地，CSI 节点插件也是通过 gRPC 接口与 Kubernetes 节点上的 kubelet 进程进行通信，kubelet 根据 Pod 的定义信息请求 CSI 插件完成具体的存储操作。</p><p>常见的 CSI 插件（CSI Driver）包括：</p><ul><li><strong>AWS EBS CSI Driver</strong><br>该 CSI 驱动允许 Kubernetes 用户在 AWS 上动态创建和管理基于块存储的 PersistentVolumes。</li><li><strong>Google Cloud Persistent Disk (GCP PD) CSI Driver</strong><br>Google Cloud Platform 提供的 CSI 驱动。</li><li><strong>Azure Disk CSI Driver</strong><br>Microsoft Azure 的 CSI 驱动程序，让 Kubernetes 能够与 Azure 磁盘服务进行交互，提供块存储给集群内的 Pod 使用。</li><li><a href="https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver"><strong>Alibaba Cloud Kubernetes CSI Plugin（ACK）</strong></a><br>阿里云提供了自家云平台上的多种存储产品（如块存储EBS、文件存储NAS等）对应的 CSI 驱动，便于 Kubernetes 用户无缝地使用阿里云存储资源。</li><li><strong>NFS CSI Driver</strong><br>针对网络文件系统 (NFS) 的 CSI 驱动插件，可以自动配置 NFS 卷并将其挂载到 Kubernetes Pods 中</li></ul><h2 id="2-实验"><a href="#2-实验" class="headerlink" title="2 实验"></a>2 实验</h2><p>在本地的 kubernetes 集群上使用阿里云的 CSI 插件使用有一系列的前置工作，如：</p><ul><li>接入VNode</li><li>集群各主机通过VPN接入阿里云ECI</li><li>…</li></ul><blockquote><p>官方文档：<a href="https://help.aliyun.com/zh/eci/user-guide/overview-5?spm=a2c4g.11186623.0.0.3e6c72a5IjaDwp"><code>https://help.aliyun.com/zh/eci/user-guide/overview-5?spm=a2c4g.11186623.0.0.3e6c72a5IjaDwp</code></a></p></blockquote><p>我们本地是由 kind 搭建的集群，集群节点是 docker 容器，在第一篇博客中就曾因为宿主机配置了代理产生了较复杂的问题；如果将 kind 集群接入ECI，相当于本地网络还要经过一层docker网络，比较复杂。我们的学习博客目标在于通过实验初步地了解相关组件原理和效果，暂不需要使用复杂度较高的适用于生产环境的组件，因此这里暂不使用阿里云 CSI，转而使用普通的NFS CSI Driver进行试验。</p><blockquote><p>后续博客中会尝试阿里云 CSI 插件。</p></blockquote><p>我们继续沿用上一篇博客创建的 kind 集群 <code>storage-cluster</code>。</p><h3 id="2-1-安装-NFS-Server"><a href="#2-1-安装-NFS-Server" class="headerlink" title="2.1 安装 NFS Server"></a>2.1 安装 NFS Server</h3><p>新建一个docker容器，并将其加入到集群的docker容器的网络中，以便直接通信。</p><p>查看集群的网络：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker inspect --format=<span class="hljs-string">&#x27;&#123;&#123;json .NetworkSettings.Networks&#125;&#125;&#x27;</span> storage-cluster-worker<br>&#123;<span class="hljs-string">&quot;kind&quot;</span>:&#123;<span class="hljs-string">&quot;IPAMConfig&quot;</span>:null,<span class="hljs-string">&quot;Links&quot;</span>:null,<span class="hljs-string">&quot;Aliases&quot;</span>:[<span class="hljs-string">&quot;1cfe2d76e6f7&quot;</span>,<span class="hljs-string">&quot;storage-cluster-worker&quot;</span>],<span class="hljs-string">&quot;NetworkID&quot;</span>:<span class="hljs-string">&quot;53a809b86d8fe0fef419b8a1a05feaf3be95a8681f6d2787ecd536a0b4d575ae&quot;</span>,<span class="hljs-string">&quot;EndpointID&quot;</span>:<span class="hljs-string">&quot;4414964e344455622be8137404c0e78d64f4685129303101277091ec38e431a3&quot;</span>,<span class="hljs-string">&quot;Gateway&quot;</span>:<span class="hljs-string">&quot;172.18.0.1&quot;</span>,<span class="hljs-string">&quot;IPAddress&quot;</span>:<span class="hljs-string">&quot;172.18.0.4&quot;</span>,<span class="hljs-string">&quot;IPPrefixLen&quot;</span>:16,<span class="hljs-string">&quot;IPv6Gateway&quot;</span>:<span class="hljs-string">&quot;fc00:f853:ccd:e793::1&quot;</span>,<span class="hljs-string">&quot;GlobalIPv6Address&quot;</span>:<span class="hljs-string">&quot;fc00:f853:ccd:e793::4&quot;</span>,<span class="hljs-string">&quot;GlobalIPv6PrefixLen&quot;</span>:64,<span class="hljs-string">&quot;MacAddress&quot;</span>:<span class="hljs-string">&quot;02:42:ac:12:00:04&quot;</span>,<span class="hljs-string">&quot;DriverOpts&quot;</span>:null&#125;&#125;<br></code></pre></td></tr></table></figure><p>我们直接部署容器化的 NFS 服务器，使用 10M+ 下载量的镜像 <a href="https://hub.docker.com/r/itsthenetwork/nfs-server-alpine"><code>itsthenetwork/nfs-server-alpine</code></a> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull itsthenetwork/nfs-server-alpine<br></code></pre></td></tr></table></figure><p>创建容器 <code>nfs-server</code> ，将宿主机上的 <code>/home/nfsshare</code> 挂载到 NFS 服务器中的 <code>/nfsshare</code>目录。</p><blockquote><p>宿主机对应的是20049端口，因为宿主机使通过VMWARE虚拟的Ubuntu，本身2049已经被占用了。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker volume create --name nfs-volume<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run \<br>--name nfs-server \<br>--privileged \<br>--restart=unless-stopped \<br>-p 2049:2049 \<br>-v nfs-volume:/nfsshare \<br>-e SHARED_DIRECTORY=/nfsshare \<br>--network kind \<br>itsthenetwork/nfs-server-alpine<br></code></pre></td></tr></table></figure><p>查看现在的容器，有一个nfs-server容器，另外三个容器是kind集群创建的三个节点，这4个容器在同一个网络中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker ps<br>CONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS         PORTS                                         NAMES<br>9b95c2f7a0eb   itsthenetwork/nfs-server-alpine   <span class="hljs-string">&quot;/usr/bin/nfsd.sh&quot;</span>       13 minutes ago   Up 3 minutes   0.0.0.0:20049-&gt;2049/tcp, :::20049-&gt;2049/tcp   nfs-server<br>1cfe2d76e6f7   kindest/node:v1.29.0              <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   21 hours ago     Up 3 minutes                                                 storage-cluster-worker<br>34309581dffe   kindest/node:v1.29.0              <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   21 hours ago     Up 3 minutes   127.0.0.1:42707-&gt;6443/tcp                     storage-cluster-control-plane<br>f998b1033432   kindest/node:v1.29.0              <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   21 hours ago     Up 3 minutes                                                 storage-cluster-worker2<br><br></code></pre></td></tr></table></figure><p>查看 <code>kind</code> 网络下的容器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker network inspect kind | jq <span class="hljs-string">&#x27;.[0].Containers | to_entries[] | .value&#x27;</span><br>&#123;<br>  <span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;storage-cluster-worker2&quot;</span>,<br>  <span class="hljs-string">&quot;EndpointID&quot;</span>: <span class="hljs-string">&quot;407df86e981930be8277f91f855b9a30e8db9dd26d230dbd00f68f13ff2ae2c2&quot;</span>,<br>  <span class="hljs-string">&quot;MacAddress&quot;</span>: <span class="hljs-string">&quot;02:42:ac:12:00:05&quot;</span>,<br>  <span class="hljs-string">&quot;IPv4Address&quot;</span>: <span class="hljs-string">&quot;172.18.0.5/16&quot;</span>,<br>  <span class="hljs-string">&quot;IPv6Address&quot;</span>: <span class="hljs-string">&quot;fc00:f853:ccd:e793::5/64&quot;</span><br>&#125;<br>&#123;<br>  <span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;nfs-server&quot;</span>,<br>  <span class="hljs-string">&quot;EndpointID&quot;</span>: <span class="hljs-string">&quot;c64dbe8c9d36c2f75784ccfe51aa854a75536402cf89cd58042959403623f95c&quot;</span>,<br>  <span class="hljs-string">&quot;MacAddress&quot;</span>: <span class="hljs-string">&quot;02:42:ac:12:00:02&quot;</span>,<br>  <span class="hljs-string">&quot;IPv4Address&quot;</span>: <span class="hljs-string">&quot;172.18.0.2/16&quot;</span>,<br>  <span class="hljs-string">&quot;IPv6Address&quot;</span>: <span class="hljs-string">&quot;fc00:f853:ccd:e793::2/64&quot;</span><br>&#125;<br>&#123;<br>  <span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;storage-cluster-control-plane&quot;</span>,<br>  <span class="hljs-string">&quot;EndpointID&quot;</span>: <span class="hljs-string">&quot;3dd90fe415d89a31ee480970884695768d2bde757d60ddb8b397e220b301f36b&quot;</span>,<br>  <span class="hljs-string">&quot;MacAddress&quot;</span>: <span class="hljs-string">&quot;02:42:ac:12:00:04&quot;</span>,<br>  <span class="hljs-string">&quot;IPv4Address&quot;</span>: <span class="hljs-string">&quot;172.18.0.4/16&quot;</span>,<br>  <span class="hljs-string">&quot;IPv6Address&quot;</span>: <span class="hljs-string">&quot;fc00:f853:ccd:e793::4/64&quot;</span><br>&#125;<br>&#123;<br>  <span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;storage-cluster-worker&quot;</span>,<br>  <span class="hljs-string">&quot;EndpointID&quot;</span>: <span class="hljs-string">&quot;28d4a47609b70b051e003ef3791c97e09641bae0270b28602a3df49cd3ba725a&quot;</span>,<br>  <span class="hljs-string">&quot;MacAddress&quot;</span>: <span class="hljs-string">&quot;02:42:ac:12:00:03&quot;</span>,<br>  <span class="hljs-string">&quot;IPv4Address&quot;</span>: <span class="hljs-string">&quot;172.18.0.3/16&quot;</span>,<br>  <span class="hljs-string">&quot;IPv6Address&quot;</span>: <span class="hljs-string">&quot;fc00:f853:ccd:e793::3/64&quot;</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>进入 NFS 服务器 <code>nfs-server</code>，在 <code>/nfsshare</code> 目录下写入文件 <code>NFS.txt</code> 备用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it nfs-server bash -c <span class="hljs-string">&#x27;echo &quot;This is NFS&quot; &gt; /nfsshare/NFS.txt&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-2-检查-NFS-服务"><a href="#2-2-检查-NFS-服务" class="headerlink" title="2.2 检查 NFS 服务"></a>2.2 检查 NFS 服务</h3><p>拉取镜像 <a href="https://hub.docker.com/r/d3fk/nfs-client"><code>d3fk/nfs-client</code></a> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull d3fk/nfs-client<br></code></pre></td></tr></table></figure><p>运行容器 <code>nfs-client</code> ，将容器加入 <code>kind</code> 网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run -itd --name nfs-client --privileged=<span class="hljs-literal">true</span> --net=kind d3fk/nfs-client<br></code></pre></td></tr></table></figure><p>进入 <code>nfs-client</code> ，挂载 <code>nfs-server</code> 上的 <code>/nfsshare</code> 到 <code>nfs-client</code> 上的 <code>~/temp</code>。</p><blockquote><p>注意：确保 <code>nfs-client</code> 上存在 <code>~/temp</code> 目录。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it nfs-client sh<br>mount -t nfs nfs-server:/ ~/temp<br></code></pre></td></tr></table></figure><blockquote><p>注意：<a id="anchor1"><a href="#bug1">[1]</a><a> 如果使用命令 <code>mount -t nfs nfs-server:/nfsshare ~/temp</code> 则会提示<strong>目录不存在</strong>。因为这个 <code>nfs-server</code> 镜像设定了 <code>fsid=0</code>，默认挂载的是 <code>nfs-server</code> 中的共享根目录。如果在 <code>nfs-server</code> 上存在 <code>/nfsshare/a-dir</code> 目录，则可以通过命令 <code>mount -t nfs nfs-server:/a-dir ~/temp</code> 将该目录挂载到 <code>nfs-client</code> 的 <code>~/temp</code> 上。</p></blockquote><p>检查挂载 nfs 是否成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it nfs-client sh<br>~ <span class="hljs-comment"># cat ~/temp/NFS.txt </span><br>This is NFS<br></code></pre></td></tr></table></figure><p>NFS 服务已经可用。</p><h3 id="2-3-在-k8s-集群安装-NFS-CSI-插件"><a href="#2-3-在-k8s-集群安装-NFS-CSI-插件" class="headerlink" title="2.3 在 k8s 集群安装 NFS CSI 插件"></a>2.3 在 k8s 集群安装 NFS CSI 插件</h3><blockquote><p>kind 节点容器 <code>kindest/node</code> 默认已安装 <code>nfs-common</code> 客户端。</p></blockquote><p>参照文档 <a href="https://github.com/kubernetes-csi/csi-driver-nfs/blob/master/docs/install-csi-driver-v4.6.0.md"><code>https://github.com/kubernetes-csi/csi-driver-nfs/blob/master/docs/install-csi-driver-v4.6.0.md</code></a>。</p><p>进入 Master 节点 <code>storage-cluster-control-plane</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it storage-cluster-control-plane /bin/bash<br></code></pre></td></tr></table></figure><p>安装CSI插件</p><blockquote><p>如果有网络障碍，先在宿主机 <code>git clone https://github.com/kubernetes-csi/csi-driver-nfs.git</code> 下载文件，<code>docker cp csi-driver-nfs/ storage-cluster-control-plane:/root/csi-driver-nfs</code> 复制到容器内，<code>cd csi-driver-nfs</code>、<br><code>./deploy/install-driver.sh master local</code>本地安装。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ curl -skSL https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/v4.6.0/deploy/install-driver.sh | bash -s v4.6.0 --<br>use <span class="hljs-built_in">local</span> deploy<br>Installing NFS CSI driver, version: master ...<br>serviceaccount/csi-nfs-controller-sa created<br>serviceaccount/csi-nfs-node-sa created<br>clusterrole.rbac.authorization.k8s.io/nfs-external-provisioner-role created<br>clusterrolebinding.rbac.authorization.k8s.io/nfs-csi-provisioner-binding created<br>csidriver.storage.k8s.io/nfs.csi.k8s.io created<br>deployment.apps/csi-nfs-controller created<br>daemonset.apps/csi-nfs-node created<br>NFS CSI driver installed successfully.<br></code></pre></td></tr></table></figure><p>检查 csi 插件状态，卡在了镜像拉取环节。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:~/csi-driver-nfs<span class="hljs-comment"># kubectl -n kube-system get pod -o wide -l app=csi-nfs-node</span><br>NAME                 READY   STATUS         RESTARTS   AGE    IP           NODE                            NOMINATED NODE   READINESS GATES<br>csi-nfs-node-4hj66   0/3     ErrImagePull   0          3m5s   172.18.0.4   storage-cluster-worker          &lt;none&gt;           &lt;none&gt;<br>csi-nfs-node-5qj48   0/3     ErrImagePull   0          3m5s   172.18.0.3   storage-cluster-control-plane   &lt;none&gt;           &lt;none&gt;<br>csi-nfs-node-sz24f   0/3     ErrImagePull   0          3m5s   172.18.0.2   storage-cluster-worker2         &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><p>检查日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:~/csi-driver-nfs<span class="hljs-comment"># kubectl describe pod csi-nfs-node-4hj66 -n kube-system</span><br>Name:                 csi-nfs-node-4hj66<br>...<br>  Warning  Failed     85s (x2 over 3m34s)   kubelet            Error: ErrImagePull<br>  Normal   Pulling    85s (x2 over 3m34s)   kubelet            Pulling image <span class="hljs-string">&quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0&quot;</span><br>  Warning  Failed     42s (x2 over 2m51s)   kubelet            Failed to pull image <span class="hljs-string">&quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0&quot;</span>: failed to pull and unpack image <span class="hljs-string">&quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0&quot;</span>: failed to resolve reference <span class="hljs-string">&quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0&quot;</span>: failed to <span class="hljs-keyword">do</span> request: Head <span class="hljs-string">&quot;https://us-west2-docker.pkg.dev/v2/k8s-artifacts-prod/images/sig-storage/csi-node-driver-registrar/manifests/v2.10.0&quot;</span>: dial tcp 108.177.125.82:443: connect: connection refused<br>  Warning  Failed     42s (x2 over 2m51s)   kubelet            Error: ErrImagePull<br>  Normal   Pulling    42s (x2 over 2m51s)   kubelet            Pulling image <span class="hljs-string">&quot;gcr.io/k8s-staging-sig-storage/nfsplugin:canary&quot;</span><br></code></pre></td></tr></table></figure><p>该安装脚本的核心是使用 <code>kubectl</code> 部署了四个 <code>yaml</code> 文件网络连通性存在障碍，按顺序依次是：</p><ul><li><code>deploy/rbac-csi-nfs.yaml</code></li><li><code>deploy/csi-nfs-driverinfo.yaml</code></li><li><code>deploy/csi-nfs-controller.yaml</code></li><li><code>deploy/csi-nfs-node.yaml</code></li></ul><p>这些文件中指定的镜像包括</p><ul><li><code>registry.k8s.io/sig-storage/csi-provisioner:v4.0.0</code></li><li><code>registry.k8s.io/sig-storage/csi-snapshotter:v6.3.3</code></li><li><code>registry.k8s.io/sig-storage/livenessprobe:v2.12.0</code></li><li><code>gcr.io/k8s-staging-sig-storage/nfsplugin:canary</code></li><li><code>registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0</code></li></ul><p>在 kind 集群中拉去这些镜像时，网络存在连通障碍，因此我们拉取在宿主机中从阿里云镜像中心拉取镜像，并用 <code>docker tag</code> 修改对应的tag，再通过 <code>kind load</code> 加载到集群中。</p><blockquote><p>也可以通过代理进行 docker pull，再使用 <code>kind load</code>，<strong>此处使用代理</strong>。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull registry.k8s.io/sig-storage/csi-provisioner:v4.0.0<br>docker pull registry.k8s.io/sig-storage/csi-snapshotter:v6.3.3<br>docker pull registry.k8s.io/sig-storage/livenessprobe:v2.12.0<br>docker pull gcr.io/k8s-staging-sig-storage/nfsplugin:canary<br>docker pull registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0<br><br>kind load docker-image registry.k8s.io/sig-storage/csi-provisioner:v4.0.0            --name storage-cluster       <br>kind load docker-image registry.k8s.io/sig-storage/csi-snapshotter:v6.3.3            --name storage-cluster       <br>kind load docker-image registry.k8s.io/sig-storage/livenessprobe:v2.12.0             --name storage-cluster       <br>kind load docker-image gcr.io/k8s-staging-sig-storage/nfsplugin:canary               --name storage-cluster   <br>kind load docker-image registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.0 --name storage-cluster                   <br></code></pre></td></tr></table></figure><p>加载完成后，重新部署 CSI 插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@yzq-virtual-machine:~$ docker <span class="hljs-built_in">exec</span> -it storage-cluster-control-plane /bin/bash<br>root@storage-cluster-control-plane:/<span class="hljs-comment"># cd ~/csi-driver-nfs/</span><br>root@storage-cluster-control-plane:~/csi-driver-nfs<span class="hljs-comment"># ./deploy/install-driver.sh master local</span><br></code></pre></td></tr></table></figure><p>再次检查 CSI 插件状态，已经部署成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:~/csi-driver-nfs<span class="hljs-comment"># kubectl -n kube-system get pod -o wide -l app=csi-nfs-controller</span><br>NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE                            NOMINATED NODE   READINESS GATES<br>csi-nfs-controller-65df4b8c9-5d7lx   4/4     Running   0          44m   172.18.0.2   storage-cluster-control-plane   &lt;none&gt;           &lt;none&gt;<br>root@storage-cluster-control-plane:~/csi-driver-nfs<span class="hljs-comment"># kubectl -n kube-system get pod -o wide -l app=csi-nfs-node</span><br>NAME                 READY   STATUS    RESTARTS   AGE   IP           NODE                            NOMINATED NODE   READINESS GATES<br>csi-nfs-node-4hj66   3/3     Running   0          8h    172.18.0.4   storage-cluster-worker          &lt;none&gt;           &lt;none&gt;<br>csi-nfs-node-5qj48   3/3     Running   0          8h    172.18.0.2   storage-cluster-control-plane   &lt;none&gt;           &lt;none&gt;<br>csi-nfs-node-sz24f   3/3     Running   0          8h    172.18.0.3   storage-cluster-worker2         &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><h3 id="2-4-创建-NFS-类型的-PV"><a href="#2-4-创建-NFS-类型的-PV" class="headerlink" title="2.4 创建 NFS 类型的 PV"></a>2.4 创建 NFS 类型的 PV</h3><p>先创建 Storage Class，它会利用NFS CSI插件动态供应PV。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># nfs-sc.yaml</span><br><span class="hljs-attr">allowVolumeExpansion:</span> <span class="hljs-literal">true</span> <span class="hljs-comment"># 允许扩张</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nfs-sc</span><br><span class="hljs-attr">provisioner:</span> <span class="hljs-string">nfs.csi.k8s.io</span> <span class="hljs-comment"># 指定 CSI 驱动程序</span><br><span class="hljs-attr">parameters:</span><br>  <span class="hljs-attr">server:</span> <span class="hljs-string">nfs-server</span> <span class="hljs-comment"># NFS 服务器 IP 地址 （Docker 网络）</span><br>  <span class="hljs-attr">share:</span> <span class="hljs-string">/</span> <span class="hljs-comment"># 在NFS服务器上共享的路径</span><br><span class="hljs-attr">reclaimPolicy:</span> <span class="hljs-string">Retain</span> <span class="hljs-comment"># 可根据需要设置为Delete或其他策略</span><br></code></pre></td></tr></table></figure><p>保存为 <code>nfs-csi-sc.yaml</code> 并应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f nfs-sc.yaml<br></code></pre></td></tr></table></figure><p>再创建 PVC，指定 Storage Class，请求 10MB 空间。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># nfs-pvc</span><br>apiVersion: v1<br>kind: PersistentVolumeClaim<br>metadata:<br>  name: nfs-pvc<br>spec:<br>  accessModes:<br>    - ReadWriteMany <br>  storageClassName: nfs-sc <span class="hljs-comment"># 使用刚才创建的 Storage Class 名</span><br>  resources:<br>    requests:<br>      storage: 10Mi <span class="hljs-comment"># </span><br></code></pre></td></tr></table></figure><p>保存为 <code>nfs-pvc.yaml</code> 并应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f nfs-pvc.yaml<br></code></pre></td></tr></table></figure><p>检查 PVC 状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:~/csi-exp<span class="hljs-comment"># kubectl get pvc</span><br>NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE<br>nfs-pvc   Pending                                      nfs-sc         &lt;<span class="hljs-built_in">unset</span>&gt;                 2m14s<br>root@storage-cluster-control-plane:~/csi-exp<span class="hljs-comment"># kubectl describe pvc nfs-pvc</span><br>Name:          nfs-pvc<br>Namespace:     default<br>StorageClass:  nfs-sc<br>Status:        Pending<br>Volume:        <br>Labels:        &lt;none&gt;<br>Annotations:   volume.beta.kubernetes.io/storage-provisioner: nfs.csi.k8s.io<br>               volume.kubernetes.io/storage-provisioner: nfs.csi.k8s.io<br>Finalizers:    [kubernetes.io/pvc-protection]<br>Capacity:      <br>Access Modes:  <br>VolumeMode:    Filesystem<br>Used By:       &lt;none&gt;<br>Events:<br>  Type     Reason                Age                    From                                                                               Message<br>  ----     ------                ----                   ----                                                                               -------<br>  Warning  ProvisioningFailed    2m37s (x2 over 2m40s)  nfs.csi.k8s.io_storage-cluster-control-plane_4472bfa5-01fc-4ccd-a3a6-ddf147075886  failed to provision volume with StorageClass <span class="hljs-string">&quot;nfs-sc&quot;</span>: rpc error: code = InvalidArgument desc = invalid parameter <span class="hljs-string">&quot;path&quot;</span> <span class="hljs-keyword">in</span> storage class<br>  Normal   Provisioning          97s (x7 over 2m40s)    nfs.csi.k8s.io_storage-cluster-control-plane_4472bfa5-01fc-4ccd-a3a6-ddf147075886  External provisioner is provisioning volume <span class="hljs-keyword">for</span> claim <span class="hljs-string">&quot;default/nfs-pvc&quot;</span><br>  Warning  ProvisioningFailed    97s (x5 over 2m39s)    nfs.csi.k8s.io_storage-cluster-control-plane_4472bfa5-01fc-4ccd-a3a6-ddf147075886  failed to provision volume with StorageClass <span class="hljs-string">&quot;nfs-sc&quot;</span>: rpc error: code = InvalidArgument desc = invalid parameter <span class="hljs-string">&quot;fsType&quot;</span> <span class="hljs-keyword">in</span> storage class<br>  Warning  ProvisioningFailed    33s                    persistentvolume-controller                                                        storageclass.storage.k8s.io <span class="hljs-string">&quot;nfs-sc&quot;</span> not found<br>  Normal   ExternalProvisioning  3s (x11 over 2m40s)    persistentvolume-controller                                                        Waiting <span class="hljs-keyword">for</span> a volume to be created either by the external provisioner <span class="hljs-string">&#x27;nfs.csi.k8s.io&#x27;</span> or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.<br><br></code></pre></td></tr></table></figure><p>再创建一个 Pod ，该 Pod 使用创建的 PVC <code>nfs-pvc</code>。该 Pod 持续写入日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: nfs-pod<br>spec:<br>  containers:<br>  - name: go-container<br>    image: go-hello-world-image:v0.0.1<br>    volumeMounts:<br>    - mountPath: /mnt/nfs <span class="hljs-comment"># 挂载点</span><br>      name: nfs-volume<br>    <span class="hljs-built_in">command</span>: [<span class="hljs-string">&quot;/bin/sh&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]<br>    args:<br>    - <span class="hljs-keyword">while</span> <span class="hljs-literal">true</span>; <span class="hljs-keyword">do</span> <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(date +&#x27;%H:%M:%S&#x27;)</span> Hello World&quot;</span> &gt;&gt; /mnt/nfs/output.txt; <span class="hljs-built_in">sleep</span> 5; <span class="hljs-keyword">done</span> <span class="hljs-comment"># 每5秒向//mnt/nfs/output.txt中追加当前时间和消息</span><br>  volumes:<br>  - name: nfs-volume<br>    persistentVolumeClaim:<br>      claimName: nfs-pvc <span class="hljs-comment"># 绑定 PVC</span><br></code></pre></td></tr></table></figure><p>命名为 <code>nfs-pod.yaml</code> 并应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f nfs-pod.yaml<br></code></pre></td></tr></table></figure><p>检查 pod 状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl get pod<br>NAME      READY   STATUS    RESTARTS   AGE<br>nfs-pod   1/1     Running   0          26s<br></code></pre></td></tr></table></figure><h3 id="2-5-检查-NFS-PV-的挂载"><a href="#2-5-检查-NFS-PV-的挂载" class="headerlink" title="2.5 检查 NFS PV 的挂载"></a>2.5 检查 NFS PV 的挂载</h3><p>进入Pod <code>nfs-pod</code>，检查目录 <code>/mnt/nfs/</code> 目录下的 <code>output.txt</code> 文件，文件无误。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:~/csi-exp<span class="hljs-comment"># kubectl exec -it nfs-pod -- sh -c &quot;tail /mnt/nfs/output.txt&quot;</span><br>11:57:12 Hello World<br>11:57:17 Hello World<br>11:57:22 Hello World<br>11:57:27 Hello World<br>11:57:32 Hello World<br>11:57:37 Hello World<br>11:57:42 Hello World<br>11:57:47 Hello World<br>11:57:52 Hello World<br>11:57:57 Hello World<br></code></pre></td></tr></table></figure><p>进入 NFS 服务器 <code>nfs-server</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it nfs-server bash<br></code></pre></td></tr></table></figure><p>检查共享文件夹 <code>/nfsshare</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@1538c90c8c07:/nfsshare <span class="hljs-comment"># cd /nfsshare &amp;&amp; ls</span><br>root@1538c90c8c07:/nfsshare <span class="hljs-comment"># cd /nfsshare &amp;&amp; ls -l</span><br>total 12<br>-rw-r--r--    1 root     root            12 Feb  9 15:21 NFS.txt<br>drwxr-xr-x    2 root     root          4096 Feb 10 11:53 pvc-b320c46c-582e-4580-94b7-7eedb1003140<br></code></pre></td></tr></table></figure><p>存在之前与之的NFS.txt，以及 kubernetes 集群创建的文件夹 <code> pvc-b320c46c-582e-4580-94b7-7eedb1003140</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@1538c90c8c07:/nfsshare <span class="hljs-comment"># cat pvc-b320c46c-582e-4580-94b7-7eedb1003140/output.txt | grep 11:57:57</span><br>11:57:57 Hello World<br></code></pre></td></tr></table></figure><p>进入此目录，检查其内容，是 <code>nfs-pod</code> Pod 中服务写入的文件。</p><p>NFS CSI 插件安装完成。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://baijiahao.baidu.com/s?id=1716423711735232537&wfr=spider&for=pc"><code>https://baijiahao.baidu.com/s?id=1716423711735232537&amp;wfr=spider&amp;for=pc</code></a><br>[2] <a href="https://developer.aliyun.com/article/727327"><code>https://developer.aliyun.com/article/727327</code></a><br>[3] <a href="https://github.com/kubernetes-csi/csi-driver-nfs/blob/master/docs/install-csi-driver-v4.6.0.md"><code>https://github.com/kubernetes-csi/csi-driver-nfs/blob/master/docs/install-csi-driver-v4.6.0.md</code></a><br>[4] <a href="https://blog.csdn.net/fengcai_ke/article/details/129151551"><code>https://blog.csdn.net/fengcai_ke/article/details/129151551</code></a></p><h2 id="BUG-记录"><a href="#BUG-记录" class="headerlink" title="BUG 记录"></a>BUG 记录</h2><p><a id="bug1"><a href="#anchor1">[1]</a></a> 挂载 nfs 时，导出目录 <code>nfsshare</code> 出现错误。</p>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>csi</tag>
      
      <tag>nfs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【4】容器存储接口CSI（上）：Kubernetes存储</title>
    <link href="/2024/02/04/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_4_k8s%E5%AD%98%E5%82%A8/"/>
    <url>/2024/02/04/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_4_k8s%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【4】容器存储接口CSI（上）：Kubernetes存储"><a href="#kubelabs——k8s学习与实验【4】容器存储接口CSI（上）：Kubernetes存储" class="headerlink" title="kubelabs——k8s学习与实验【4】容器存储接口CSI（上）：Kubernetes存储"></a>kubelabs——k8s学习与实验【4】容器存储接口CSI（上）：Kubernetes存储</h1><h2 id="1-相关概念简介"><a href="#1-相关概念简介" class="headerlink" title="1 相关概念简介"></a>1 相关概念简介</h2><p>k8s集群存储的概念建立在容器存储的基础之上，因此这里讨论两个关键的领域：Docker存储和k8s存储。</p><h3 id="1-1-Docker存储"><a href="#1-1-Docker存储" class="headerlink" title="1.1 Docker存储"></a>1.1 Docker存储</h3><blockquote><p>容器存储：镜像层（只读）+ 可写层（可写）+ 外部存储（挂载或卷）</p></blockquote><h4 id="1-1-1-镜像"><a href="#1-1-1-镜像" class="headerlink" title="1.1.1 镜像"></a>1.1.1 镜像</h4><p>Docker镜像是一个静态的、只读的模板，它包含了运行某个应用所需要的所有内容，包括代码、运行时环境、依赖库、配置文件等。镜像按照层次结构存储，每一层都是对上一层的修改，有利于资源复用。镜像是不可变的，一旦创建后就不能更改。如果需要更改，则基于原有的镜像创建新的镜像层。</p><h4 id="1-1-2-非持久化数据（Storage-Driven）"><a href="#1-1-2-非持久化数据（Storage-Driven）" class="headerlink" title="1.1.2 非持久化数据（Storage Driven）"></a>1.1.2 非持久化数据（Storage Driven）</h4><p>通过分层，多个容器可以共享一个镜像资源（一个镜像创建多个容器），甚至更细粒度地共享镜像层（多个镜像在同一个镜像的基础上构建）。这些层是只读的，以保证镜像不会在创建容器后就被改变。</p><p>容器继承了镜像的内容并在其基础上添加了一个可写层，允许容器在运行过程中进行数据读写操作，当容器停止运行时，这些改变不会影响到原始的镜像。对于读，遵循上层数据覆盖下载数据的原则；对于写，遵循写时复制的规则。</p><p>当一个容器启动时，它不会直接复制整个底层镜像的所有数据到新的存储空间中。相反，它创建了一个新的可写层，并将所有读操作指向父镜像层。只有当容器内进程试图修改已存在于只读镜像层中的文件或目录时，Docker 才会执行写时复制操作，即将该文件从只读层复制到当前容器自己的可写层中，然后在这个可写的副本上进行修改。在写时复制策略下，多个容器可以共享同一份基础镜像的只读部分，直到它们各自需要修改不同的文件为止。</p><p>容器可以被创建、启动、停止、删除等，具有短暂和轻量的特点，可以快速地创建和销毁。当容器被销毁后，镜像的存储不会受到影响，但是可写层的数据会丢失，无法持久化。Docker 通过 Storage Driven 来管理以上非持久化存储。</p><blockquote><p>另外，docker 提供了tmpfs mount，将文件存储在主机内存中，当宿主机关闭时，数据丢失。如果容器生成的是非持久状态数据，建议考虑使用tmpfs挂载，这样既避免了永久存储数据，又能通过不向容器可写层写入数据的方式提升容器性能。</p></blockquote><h4 id="1-1-3-持久化数据"><a href="#1-1-3-持久化数据" class="headerlink" title="1.1.3 持久化数据"></a>1.1.3 持久化数据</h4><blockquote><p>文档见 <a href="https://docs.docker.com/storage/"><code>Docker Docs / Docker Engine / storage</code></a></p></blockquote><p>默认情况下，在容器内创建的所有文件都存储在可写层。这意味着：</p><ul><li>当该容器不再存在时，数据不会保留；且如果另一个进程需要数据，则很难将数据从容器中取出。</li><li>容器的可写层与主机上运行容器的位置紧密耦合，无法轻松地将数据移动到其他位置。</li><li>写入容器的可写层需要存储驱动程序来管理文件系统，降低了性能。</li></ul><p>为了使写入的内容不会丢失，Docker使用了多种方法，包括bind-mount和volume，将数据存储在宿主机上。允许数据持久化并跨越容器生命周期 。</p><ul><li><p><strong>bind mount</strong></p><p>  绑定挂载是 Docker 早期引入的功能，其功能相对数据卷而言较为有限。在使用绑定挂载时，会将宿主机上的<strong>某个具体文件或目录</strong>通过<strong>绝对路径</strong>映射到容器内部。<br>  对于要挂载的文件或目录，无需提前确保它存在于Docker宿主机上；若宿主机不存在此目录，会在实际需要时动态创建。</p><p>  尽管绑定挂载在性能方面表现出色，但它的正常运行依赖于宿主机文件系统中特定目录结构的存在，且无法直接利用 <code>Docker CLI</code> 命令对绑定挂载进行管理操作。</p></li><li><p><strong>volume</strong></p><p>  与 bind mount 不同的是，数据卷的运作方式是在宿主机的Docker存储目录下新建一个子目录，并由Docker系统来管理该目录内的内容。相比绑定挂载具有多项优势：</p><ol><li>更易于备份或迁移。</li><li>可以通过Docker CLI命令或Docker API来管理数据卷。</li><li>数据卷不仅适用于Linux容器，也支持Windows容器。</li><li>数据卷能够更安全地在多个容器之间共享。</li></ol></li></ul><p><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240204161318.png" alt="docker-storage"></p><h3 id="1-2-Kubernetes存储"><a href="#1-2-Kubernetes存储" class="headerlink" title="1.2 Kubernetes存储"></a>1.2 Kubernetes存储</h3><blockquote><p>文档见 <a href="https://kubernetes.io/zh-cn/docs/concepts/storage/"><code>https://kubernetes.io/zh-cn/docs/concepts/storage/</code></a></p></blockquote><h4 id="1-2-1-临时卷（Ephemeral-Volume）"><a href="#1-2-1-临时卷（Ephemeral-Volume）" class="headerlink" title="1.2.1 临时卷（Ephemeral Volume）"></a>1.2.1 临时卷（Ephemeral Volume）</h4><p>临时卷是一个 Pod 级别的概念。</p><p>有些应用需要额外的存储空间，但不关心数据在重启后是否保存。为此场景，k8s设计了临时卷的概念，其生命周期与Pod相同，与 Pod 一起创建和删除。临时卷包括：</p><ul><li><p>emptyDir</p><p>  该卷是在Pod创建的空目录，用于在容器之间共享文件。</p></li><li><p>configMap、downloadAPI、secret</p><p>  configMap 卷提供了向 Pod 注入配置数据的方法。 ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用，然后被 Pod 中运行的容器化应用使用。<br>  downwardAPI 卷用于为应用提供 downward API 数据。 在这类卷中，所公开的数据以纯文本格式的只读文件形式存在。<br>  secret 卷用来给 Pod 传递敏感信息，例如密码。</p></li><li><p>通用临时卷<br>  类似于 emptyDir 卷，为每个 Pod 提供临时数据存放目录，在最初制备完毕时一般为空。</p></li></ul><blockquote><p>一个Pod内的容器都可以共用，它们可以指定各自的 mount 路径。</p></blockquote><h4 id="1-2-2-持久卷（Persistent-Volumes）"><a href="#1-2-2-持久卷（Persistent-Volumes）" class="headerlink" title="1.2.2 持久卷（Persistent Volumes）"></a>1.2.2 持久卷（Persistent Volumes）</h4><p>持久卷是一个集群级别的概念，常见的包括 hostPath、PV等。PV是类似Pod的抽象对象。</p><ul><li><p><strong>hostPath</strong></p><p>  hostPath 卷是讲节点主机上的目录直接挂载到Pod中，从而使Pod中的数据在Pod销毁后依然能保存。</p><blockquote><p>极不推荐使用 hostPath ，会带来安全风险。如果要使用，也应该定义一个 <code>local</code> PV，用来代替 hostPath。</p></blockquote></li><li><p><strong>PV（PersistentVolume）</strong></p><p>  PV可以抽象地理解为一个存储资源，描述的是集群中可供使用的持久化存储资源。</p><p>  Kubernetes 内置或通过 CSI 支持多种类型的 Volume 插件，例如对于本地磁盘、网络存储（NFS存储, iSCSI, Ceph RBD）、云服务商提供的块存储（如 AWS EBS, GCP Persistent Disk 或 Azure Disk）等。</p><p>  当创建一个 PV 并指定其 storageClassName 和相关属性时，实际上是告诉 Kubernetes 使用特定的 Volume 插件来连接到实际的存储资源。</p></li><li><p><strong>PVC（PersistentVolumeClaim）</strong></p><p>  开发人员或运维人员定义并创建PVC，它代表了对存储资源的需求，包括所需容量、访问模式（如 ReadWriteOnce, ReadOnlyMany 或 ReadWriteMany）和 StorageClass 等属性</p><p>  PV控制器会自动查找满足PVC的PV，将PV和PVC绑定。当Pod停止并重新部署时，数据持久化在独立于Pod的PV中，并通过PVC再次挂载PV。</p><blockquote><p>一般来说，PV由运维工程师（存储工程师）来进行维护，而PVC则是由开发人员自己申请使用即可。当一个容器需要进行数据卷挂载，只需要写一个PVC来绑定PV就可以，k8s自身会查找符合条件的PV。</p></blockquote></li><li><p><strong>CSI（Container Storage Interface）</strong></p><p>  CSI是由Kubernetes、Docker等社区联合制定的行业标准接口规范，旨在将任何存储系统暴露给容器化应用程序。存储服务提供方需要实现该接口规范的一些方法，包括：创建存储卷、删除存储卷、挂载卷、卸载卷、创建卷快照、删除卷快照等方法。任何实现了CSI的存储插件，都可以在k8s中使用。<strong>通过CSI可以创建可以被k8s使用的PV</strong>。</p></li></ul><h2 id="2-实验"><a href="#2-实验" class="headerlink" title="2 实验"></a>2 实验</h2><h3 id="2-1-Docker存储"><a href="#2-1-Docker存储" class="headerlink" title="2.1 Docker存储"></a>2.1 Docker存储</h3><p>docker的非持久化存储不做赘述，在此处针对docker的两种持久化存储进行实验。</p><h4 id="bind-mount"><a href="#bind-mount" class="headerlink" title="bind mount"></a>bind mount</h4><p>在主机创建目录，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ~/tempdir/dockerdir<br></code></pre></td></tr></table></figure><p>启动容器，并将目录挂载到容器内部的 <code>~/downloads</code> 目录下，这里依然使用之前博客的镜像。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run -itd --name storage-test -v ~/tempdir/dockerdir:/root/downloads go-hello-world-image:v0.0.1<br></code></pre></td></tr></table></figure><p>进入容器，检查 <code>~/downloads</code> 目录，并写入文件 <code>hello.txt</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it storage-test sh<br>/app <span class="hljs-comment"># echo &quot;Hello&quot; &gt; ~/downloads/hello.txt</span><br>/app <span class="hljs-comment"># ls ~/downloads/</span><br>hello.txt<br></code></pre></td></tr></table></figure><p>检查宿主机，文件已存在。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ <span class="hljs-built_in">ls</span> tempdir/dockerdir/<br>hello.txt<br></code></pre></td></tr></table></figure><p>删除容器，并检查宿主机 <code>~/tempdir/dockerdir/</code> 目录下文件是否存在。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker stop storage-test &amp;&amp; docker <span class="hljs-built_in">rm</span> storage-test &amp;&amp; <span class="hljs-built_in">ls</span> ~/tempdir/dockerdir/ &amp;&amp; <span class="hljs-built_in">cat</span> ~/tempdir/dockerdir/hello.txt <br>storage-test<br>storage-test<br>hello.txt<br>Hello<br></code></pre></td></tr></table></figure><p>文件和文件内容均无误。</p><h4 id="volume"><a href="#volume" class="headerlink" title="volume"></a>volume</h4><p>创建名为 <code>temp-volume</code> 的volume，检查volume。除了刚才创建的volume外，还有之前任务留下的volume。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker volume create temp-volume<br>temp-volume<br>yzq@ubuntu:~$ docker volume <span class="hljs-built_in">ls</span><br>DRIVER    VOLUME NAME<br><span class="hljs-built_in">local</span>     36ea4e358c355f1a1154767e98bd1e5bdb8448d2c0077b82ed37908aea586e96<br><span class="hljs-built_in">local</span>     2683dee230e5c309a797b89d092c0072ef62244ad313ba91b0d28b4d5496dcb4<br><span class="hljs-built_in">local</span>     636005babd42afa31a1e8dd3e04dd5060b2eb330906db2da7b2d46c9aed47fcf<br><span class="hljs-built_in">local</span>     bc4ff633f0f6ba98f704229dc5ab70f5a31488163d188dcc8228fd3d4fbbf59c<br><span class="hljs-built_in">local</span>     c28dbe3855e69424a686db9da8a0c23912ece48500e0d488d1bf2311cfa00db1<br><span class="hljs-built_in">local</span>     d5a5bb54399aad85e8eddf512e737921d3fcae4d1299e55535cfd970998fec57<br><span class="hljs-built_in">local</span>     dbe071cbcad47a9eec711b2387e3927b1ed209c9f7ff03ac8ae26dc2fb760d8a<br><span class="hljs-built_in">local</span>     e4b0d76afe344d2e0cd4c015527aa276a1002bd753e5a9587e19a851f420f840<br><span class="hljs-built_in">local</span>     fe295be57089470edea5a63adf2b6fe2c06745b168e4285f9d7625e3a6c0d1e6<br><span class="hljs-built_in">local</span>     minikube<br><span class="hljs-built_in">local</span>     temp-volume<br></code></pre></td></tr></table></figure><p>启动容器时挂载该volume至容器内的 <code>~/downloads</code> 路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker run -itd --name storage-test2 -v temp-volume:/root/downloads go-hello-world-image:v0.0.1<br>yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it storage-test2 sh<br>/app <span class="hljs-comment"># cd /root/downloads/</span><br>~/downloads <span class="hljs-comment"># echo &quot;Hello&quot; &gt; ~/downloads/volume.txt</span><br>~/downloads <span class="hljs-comment"># cat volume.txt </span><br>Hello<br></code></pre></td></tr></table></figure><p>删除容器，并重新创建容器，但将volume挂载在 <code>~/files</code> 目录下。随后检查该目录下的文件 <code>volume.txt</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it storage-test3 sh<br>/app <span class="hljs-comment"># cat ~/files/volume.txt </span><br>Hello<br></code></pre></td></tr></table></figure><h3 id="2-2-Kubernetes存储"><a href="#2-2-Kubernetes存储" class="headerlink" title="2.2 Kubernetes存储"></a>2.2 Kubernetes存储</h3><h4 id="2-2-1-emptyDir"><a href="#2-2-1-emptyDir" class="headerlink" title="2.2.1 emptyDir"></a>2.2.1 emptyDir</h4><p>利用本系列第一篇博客的配置文件 <a href="https://github.com/Artistzq/kubelabs/tree/main/hello"><code>https://github.com/Artistzq/kubelabs/tree/main/hello</code></a> ，创建集群 <code>storage-cluster</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash create.sh -n storage-cluster<br></code></pre></td></tr></table></figure><p>并进入 <code>control plane</code> ，创建一个使用emptyDir的Pod，该Pod中的服务每5秒写入当前时间。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it storage-cluster-control-plane<br></code></pre></td></tr></table></figure><p>该 pod 的 yaml 配置文件如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-with-emptydir</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">experiment-emptydir</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">myapp-container</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">go-hello-world-image:v0.0.1</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/data</span>     <span class="hljs-comment"># 容器内部将emptyDir挂载到/data路径</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">temporary-storage</span><br>    <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/bin/sh&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]<br>    <span class="hljs-attr">args:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">while</span> <span class="hljs-literal">true</span><span class="hljs-string">;</span> <span class="hljs-string">do</span> <span class="hljs-string">echo</span> <span class="hljs-string">&quot;$(date) Hello World&quot;</span> <span class="hljs-string">&gt;&gt;</span> <span class="hljs-string">/data/output.txt;</span> <span class="hljs-string">sleep</span> <span class="hljs-number">5</span><span class="hljs-string">;</span> <span class="hljs-string">done</span> <span class="hljs-comment"># 这条命令会让容器每5秒向/data/output.txt中追加当前时间和消息</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">temporary-storage</span><br>    <span class="hljs-attr">emptyDir:</span> &#123;&#125; <span class="hljs-comment"># 使用emptyDir卷类型</span><br></code></pre></td></tr></table></figure><p>手动部署该Pod。（未通过Deployment）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl apply -f experiment-emptydir.yaml<br></code></pre></td></tr></table></figure><p>进入pod，检查文件 <code>/data/output.txt</code> ，持续写入日志。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/store-test$ kubectl <span class="hljs-built_in">exec</span> -it pod-with-emptydir -- <span class="hljs-built_in">tail</span> /data/output.txt<br>04:09:22 UTC 2024 Hello World<br>04:09:27 UTC 2024 Hello World<br>04:09:32 UTC 2024 Hello World<br>04:09:37 UTC 2024 Hello World<br>04:09:42 UTC 2024 Hello World<br>04:09:47 UTC 2024 Hello World<br>04:09:52 UTC 2024 Hello World<br>04:09:57 UTC 2024 Hello World<br>04:10:02 UTC 2024 Hello World<br>04:10:07 UTC 2024 Hello World<br></code></pre></td></tr></table></figure><p>删除此pod，并用配置文件 <code>experiment-emptydir.yaml</code> 重新部署 pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl delete pod pod-with-emptydir &amp;&amp; kubectl apply -f experiment-emptydir.yaml<br></code></pre></td></tr></table></figure><p>检查日志是否存在之前的记录。之前的记录已丢失，验证了 emptyDir 的临时性。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl <span class="hljs-built_in">exec</span> -it pod-with-emptydir -- <span class="hljs-built_in">tail</span> /data/output.txt | grep 04:10:07<br><span class="hljs-comment"># 输出空，没有时间为 04:10:07的记录</span><br></code></pre></td></tr></table></figure><blockquote><p>注：hostpath 卷的方式与docker bind mount类似，直接将宿主机上的绝对路径挂载到pod中，以达到持久化目的。此处不进行实验。</p></blockquote><h4 id="2-2-2-PV和PVC"><a href="#2-2-2-PV和PVC" class="headerlink" title="2.2.2 PV和PVC"></a>2.2.2 PV和PVC</h4><p>首先需要搭建一个PV资源（存储资源，包括本地、NFS等）。在初步实验中，我们搭建一个基于本地存储的PV存储。</p><blockquote><p>这里只进行最基本的 host path PV，<strong>后续实验将会在CSI的讨论中展开</strong>。</p></blockquote><p>进入工作节点 <code>storage-cluster-worker</code> ，创建一个基于hostpath的PV，该方法是使用宿主机上的一块资源创建PV。</p><blockquote><p>生产中不要使用这种方法，其一，数据应该使用外挂盘，否则宿主机空间占满会影响服务的运行；其二，该方法创建的PV与宿主机Node绑定了，需要为Pod制定节点亲和性（nodeAffinity），不够方便。对于生产环境，请考虑使用网络存储解决方案来提供跨节点的数据持久化和高可用性。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it storage-cluster-worker /bin/bash<br></code></pre></td></tr></table></figure><p>在 <code>storage-cluster-worker</code> 内创建目录 <code>/data</code>，基于此创建存储容量为10MB的PV，并设定延迟绑定。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> /data/hostpath-pv<br></code></pre></td></tr></table></figure><blockquote><p>通过设定storageClassName实现PV和PVC的延迟绑定，提高灵活性。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># PV: pv-hostpath.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolume</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">hostpath-pv</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">capacity:</span><br>    <span class="hljs-attr">storage:</span> <span class="hljs-string">10Mi</span> <span class="hljs-comment"># 设置存储容量为10MB</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span><br>  <span class="hljs-attr">persistentVolumeReclaimPolicy:</span> <span class="hljs-string">Retain</span> <span class="hljs-comment"># 根据实际情况选择合适的策略</span><br>  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">manual</span> <br>  <span class="hljs-attr">hostPath:</span><br>    <span class="hljs-attr">path:</span> <span class="hljs-string">/data/hostpath-pv</span><br></code></pre></td></tr></table></figure><p>进入 <code>control plane (MasterNode)</code> 并应用 <code>pv-hostpath.yaml</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it torage-cluster-control-plane /bin/bash <br>root@storage-cluster-control-plane:~/storage-test<span class="hljs-comment"># kubectl apply -f pv-hostpath.yaml</span><br>root@storage-cluster-control-plane:~/storage-test<span class="hljs-comment"># kubectl get pv</span><br>NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE<br>hostpath-pv   10Mi       RWO            Retain           Available           manual         &lt;<span class="hljs-built_in">unset</span>&gt; <br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># PVC: pvc-hostpath.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">hostpath-pvc</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">accessModes:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span><br>  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">manual</span> <span class="hljs-comment"># 与上面PV的storageClassName一致</span><br>  <span class="hljs-attr">resources:</span><br>    <span class="hljs-attr">requests:</span><br>      <span class="hljs-attr">storage:</span> <span class="hljs-string">10Mi</span> <span class="hljs-comment"># 与PV中定义的存储容量一致</span><br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Pod: pod-with-affinity.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-with-hostpath-affinity</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">affinity:</span><br>    <span class="hljs-attr">nodeAffinity:</span> <span class="hljs-comment"># 节点亲和性，</span><br>      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span> <span class="hljs-comment"># 强制性亲和性规则，在调度时必须满足，但不影响运行时</span><br>        <span class="hljs-attr">nodeSelectorTerms:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubernetes.io/hostname</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>            <span class="hljs-attr">values:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-string">storage-cluster-worker</span> <span class="hljs-comment"># 节点名称</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">myapp-container</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">go-hello-world-image:v0.0.1</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/go-logs</span> <span class="hljs-comment"># 把 PV 挂载 到 Pod 内的目录 /go-logs</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">hostpath-volume</span><br>    <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/bin/sh&quot;</span>, <span class="hljs-string">&quot;-c&quot;</span>]<br>    <span class="hljs-attr">args:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">while</span> <span class="hljs-literal">true</span><span class="hljs-string">;</span> <span class="hljs-string">do</span> <span class="hljs-string">echo</span> <span class="hljs-string">&quot;$(date +&#x27;%H:%M:%S&#x27;) Hello World&quot;</span> <span class="hljs-string">&gt;&gt;</span> <span class="hljs-string">/go-logs/output.txt;</span> <span class="hljs-string">sleep</span> <span class="hljs-number">5</span><span class="hljs-string">;</span> <span class="hljs-string">done</span> <span class="hljs-comment"># 每5秒向/go-logs/output.txt中追加当前时间和消息</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hostpath-volume</span><br>    <span class="hljs-attr">persistentVolumeClaim:</span><br>      <span class="hljs-attr">claimName:</span> <span class="hljs-string">hostpath-pvc</span> <span class="hljs-comment"># 使用PVC申请存储资源</span><br></code></pre></td></tr></table></figure><p>再依次应用 <code>pvc-hostpath.yaml</code> 和 <code>pod-with-affinity.yaml</code>。如果先应用 pod ，会导致 pod 处于 pending 状态，因为请求不到存储资源。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:~/storage-test<span class="hljs-comment"># kubectl apply -f pvc-hostpath.yaml </span><br>persistentvolumeclaim/hostpath-pvc created<br>root@storage-cluster-control-plane:~/storage-test<span class="hljs-comment"># kubectl apply -f pod-with-affinity.yaml </span><br>pod/pod-with-hostpath-affinity created<br></code></pre></td></tr></table></figure><p>查看各资源和服务情况，pod运行在 <code>storage-cluster-worker</code> 工作节点上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:/<span class="hljs-comment"># kubectl get pv</span><br>NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE<br>hostpath-pv   10Mi       RWO            Retain           Bound    default/hostpath-pvc   manual         &lt;<span class="hljs-built_in">unset</span>&gt;                          45m<br>root@storage-cluster-control-plane:/<span class="hljs-comment"># kubectl get pvc</span><br>NAME           STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE<br>hostpath-pvc   Bound    hostpath-pv   10Mi       RWO            manual         &lt;<span class="hljs-built_in">unset</span>&gt;                 3m38s<br>root@storage-cluster-control-plane:/<span class="hljs-comment"># kubectl get pods -owide</span><br>NAME                         READY   STATUS    RESTARTS   AGE     IP           NODE                     NOMINATED NODE   READINESS GATES<br>pod-with-hostpath-affinity   1/1     Running   0          3m48s   10.244.2.2   storage-cluster-worker   &lt;none&gt;           &lt;none&gt;<br><br></code></pre></td></tr></table></figure><p>接下来<strong>检查卷的情况</strong>。首先进入 pod <code>pod-with-hostpath-affinity</code>中，该 pod 中服务每5秒写入一条日志到 <code>/go-logs/output.txt</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@storage-cluster-control-plane:/<span class="hljs-comment"># kubectl exec -it pod-with-hostpath-affinity /bin/sh</span><br>/app <span class="hljs-comment"># ls</span><br>/app <span class="hljs-built_in">tail</span> /go-logs/output.txt<br>15:10:18 Hello World<br>...<br>15:11:03 Hello World<br><br></code></pre></td></tr></table></figure><p>同时，进入工作节点 <code>storage-cluster-worker</code> ，这是指定节点亲和性后调度器为pod <code>pod-with-hostpath-affinity</code> 选择的工作节点。检查目录 <code>/data/hostpath-pv/</code> 下的 <code>output.txt</code> 文件，是否与上述 pod 内检查的相同。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 退回到运行 kind 的宿主机后</span><br>$ docker <span class="hljs-built_in">exec</span> -it storage-cluster-worker /bin/bash<br>$ <span class="hljs-built_in">cd</span> /data/hostpath-pv<br>$ <span class="hljs-built_in">tail</span> output.txt <br>15:10:18 Hello World<br>...<br>15:11:03 Hello World<br></code></pre></td></tr></table></figure><p><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240208231426.png" alt="result-pv"></p><p>接下来检查持久性，回到 <code>control-plane</code> ，删除 pod <code>pod-with-hostpath-affinity</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 退回到运行 kind 的宿主机后</span><br>docker <span class="hljs-built_in">exec</span> -it storage-cluster-control-plane /bin/bash<br>root@storage-cluster-control-plane:~<span class="hljs-comment"># kubectl delete pod pod-with-hostpath-affinity</span><br></code></pre></td></tr></table></figure><p>检查工作节点 <code>storage-cluster-worker</code> 下的目录 <code>/data/hostpath-pv</code> ，依然存在，文件 <code>output.txt</code> 内容也无误。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 退回到运行 kind 的宿主机后</span><br>$ docker <span class="hljs-built_in">exec</span> -it storage-cluster-worker /bin/bash<br>$ <span class="hljs-built_in">cd</span> /data/hostpath-pv<br>$ <span class="hljs-built_in">cat</span> output.txt | grep 15:11:03<br>15:11:03 Hello World <span class="hljs-comment"># 之前的记录依然存在</span><br></code></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://blog.csdn.net/weixin_43970890/article/details/102566854"><code>https://blog.csdn.net/weixin_43970890/article/details/102566854</code></a><br>[2] <a href="https://cloud.tencent.com/developer/article/2234476"><code>https://cloud.tencent.com/developer/article/2234476</code></a></p>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>csi</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时</title>
    <link href="/2024/01/26/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_3_CRI/"/>
    <url>/2024/01/26/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_3_CRI/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时"><a href="#kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时" class="headerlink" title="kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时"></a>kubelabs——k8s学习与实验【3】容器运行时接口（CRI）以及切换容器运行时</h1><h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1 概念"></a>1 概念</h2><p>容器运行时（Container Runtime）是 Kubernetes（k8s）集群中每个节点的核心组件，负责管理容器的整个生命周期，包括从拉取和运行容器镜像等关键任务。</p><div align=center><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240124152148.png"></div><p>在k8s 1.23版本之前，kubelet依赖于dockershim来操作Docker容器运行时。Docker内部集成了containerd并在此基础上构建了更加用户友好的功能层，但这也意味着dockershim需要随着Docker的更新持续维护跟进。</p><p>为了实现Kubernetes与具体容器运行时技术的解耦，自k8s 1.5版本起，社区引入了容器运行时接口（Container Runtime Interface，CRI）标准，它提供了一组标准化接口以供 Kubernetes 平台与各种容器运行时进行交互，并推荐使用原生支持CRI的容器运行时如containerd和CRI-O。这一转变标志着Kubernetes未来将不再直接绑定Docker，而是通过兼容CRI的标准运行时统一管控容器，从而提高了容器运行时的可插拔性以及Kubernetes本身的稳定性和灵活性。kubelet通过gRPC协议结合protobuf序列化方式来实现对CRI接口的远程调用过程。</p><p>尽管如此，这并不影响继续使用通过Docker打包的镜像。实际上，任何遵循OCI（开放容器倡议，Open Container Initiative）规范创建的镜像都可以被兼容。由于Docker打包的镜像同样遵循OCI规范，因此它们仍能在k8s环境中得到有效的管理和运行。</p><p>CRI相关的概念如下图所示。</p><p><img src="https://92697-imgs.oss-cn-hangzhou.aliyuncs.com/blogs/20240124144045.png" alt="cri-arch"></p><h3 id="1-1-Docker-和-dockershim"><a href="#1-1-Docker-和-dockershim" class="headerlink" title="1.1 Docker 和 dockershim"></a>1.1 Docker 和 dockershim</h3><p>Docker是一个方便的工具，用于处理镜像打包、发布、容器运行等操作。Docker本身包含了多个组件，如：</p><ul><li>docker-cli</li><li>containerd</li><li>runc</li><li>…</li></ul><p>k8s 中包含了一个组件 dockershim，使其能够支持Docker。Docker 没有实现CRI接口（Docker出现时间更早），因此 k8s 不得不自己维护一个工具，通过此工具操作Docker，这就是dockershim。</p><p>随着容器化成为行业标准，k8s 项目为了增加了对额外运行时的支持，提出了 CRI，更多的运行时实现了此接口。k8s 项目对 dockershim 和docker的依赖使整个项目变得脆弱，且对 dockershim 的单独维护增加了成本，在1.23版本后，k8s 开始弃用 dockershim，取消对Docker的直接支持。</p><p>另外，Docker 的多层封装和调用，导致其在可维护性上略逊一筹，增加了线上问题的定位难度。</p><h3 id="1-2-CRI"><a href="#1-2-CRI" class="headerlink" title="1.2 CRI"></a>1.2 CRI</h3><p>类似 JAVA 的 SPI 机制，k8s 定义了 CRI 后，不需要再像维护 dockershim 那样，为某个容器运行时维护一套适配工具。只需要各个容器运行时的提供方实现了这个接口，k8s 就可以直接调用对应的操作，例如containerd和CRI-O。</p><p>containerd 是一个来自 Docker 的高级容器运行时，并实现了 CRI 规范。它是从 Docker 项目中分离出来的，因此 Docker 自己在内部使用 containerd。containerd 通过其 CRI 插件实现了 k8s 容器运行时接口（CRI），它可以管理容器的整个生命周期，包括从镜像的传输、存储到容器的执行、监控再到网络。</p><p>CRI-O 是另一个实现了容器运行时接口（CRI）的高级容器运行时，是 containerd 的一个替代品。</p><p>使用 containerd 和 CRI-O 的方案比起 docker 简洁很多，因为省去了冗余的多层封装。</p><h3 id="1-3-OCI"><a href="#1-3-OCI" class="headerlink" title="1.3 OCI"></a>1.3 OCI</h3><p>OCI 是指开放容器倡议（Open Container Initiative），旨在制定容器镜像和运行时的行业标准。只要是符合规范的不同运行时，这些运行时可以有不同的底层实现。例如，符合 OCI 的在 linux 上的容器运行时和在 Windows 上的容器运行时。遵循该倡议的运行时通常更底层，和操作系统进行交互，从而创建容器。runC和kata-runtime就是这样的容器运行时。</p><p>runc 是轻量级的通用运行时容器，它遵守 OCI 规范，是实现 OCI 接口的最低级别的组件，它与内核交互创建并运行容器。runc 为容器提供了所有的低级功能，与现有的低级 Linux 功能交互，如命名空间和控制组，它使用这些功能来创建和运行容器进程。</p><h3 id="1-4-dockershim的弃用"><a href="#1-4-dockershim的弃用" class="headerlink" title="1.4 dockershim的弃用"></a>1.4 dockershim的弃用</h3><p>dockershim 的弃用并不意味着 k8s 将不能运行 Docker 格式的容器。containerd 和 CRI-O 都可以运行 Docker 格式的镜像，因为 Docker 格式的镜像也遵循了 OCI 格式，只是它们不再需要使用 docker 命令或 Docker 守护程序。</p><p>从功能性来讲，containerd 和 CRI-O 都符合 CRI 和 OCI 的标准。从稳定性来说，containerd 一直在 docker 里使用，生产环境经验比较充足，因此更推荐选择 containerd 作为 k8s 的容器运行时。</p><h2 id="2-实验"><a href="#2-实验" class="headerlink" title="2 实验"></a>2 实验</h2><p>实验目的：使用 containerd 或 CRI-O 作为容器运行时，并在工作节点中使用 <code>crictl</code> 命令代替 docker 命令。</p><h3 id="2-1-kind-默认使用-containerd"><a href="#2-1-kind-默认使用-containerd" class="headerlink" title="2.1 kind 默认使用 containerd"></a>2.1 kind 默认使用 containerd</h3><p>查阅文档得知，Kind 内部使用 Kubeadm 创建和启动集群节点，并使用 containerd 作为容器运行时，所以弃用 dockershim对 Kind 没有什么影响。另一个部署单机集群的工具 <code>minikube</code> 使用 cri-o 作为默认容器运行时。</p><p>进入Master Node，查看使用的容器运行时：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/cni-test$ docker <span class="hljs-built_in">exec</span> -it cni-test-cluster-control-plane /bin/bash<br>root@cni-test-cluster-control-plane:/<span class="hljs-comment"># kubectl get node cni-test-cluster-worker -o json | jq &#x27;.status.nodeInfo.containerRuntimeVersion&#x27;</span><br><span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br></code></pre></td></tr></table></figure><p>使用 <code>crictl</code> 查看镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@cni-test-cluster-control-plane:/<span class="hljs-comment"># crictl images ps</span><br>IMAGE                                      TAG                  IMAGE ID            SIZE<br>docker.io/flannel/flannel-cni-plugin       v1.2.0               a55d1bad692b7       3.88MB<br>docker.io/flannel/flannel                  v0.24.0              0dc86fe0f22e6       28MB<br>docker.io/kindest/kindnetd                 v20230511-dc714da8   b0b1fa0f58c6e       27.7MB<br>docker.io/kindest/local-path-helper        v20230510-486859a6   be300acfc8622       3.05MB<br>docker.io/kindest/local-path-provisioner   v20230511-dc714da8   ce18e076e9d4b       19.4MB<br>docker.io/library/go-hello-world-image     v0.0.1               586004f581b85       21.3MB<br>registry.k8s.io/coredns/coredns            v1.11.1              cbb01a7bd410d       18.2MB<br>registry.k8s.io/etcd                       3.5.10-0             a0eed15eed449       56.6MB<br>registry.k8s.io/kube-apiserver             v1.29.0              4a9136aaad730       86.1MB<br>registry.k8s.io/kube-controller-manager    v1.29.0              23d55452a141c       80.3MB<br>registry.k8s.io/kube-proxy                 v1.29.0              fa4dee78049db       83.5MB<br>registry.k8s.io/kube-scheduler             v1.29.0              ba16e783eea7b       60.6MB<br>registry.k8s.io/pause                      3.7                  221177c6082a8       311kB<br></code></pre></td></tr></table></figure><h3 id="2-2-将-kind-使用的运行时替换成-CRI-O"><a href="#2-2-将-kind-使用的运行时替换成-CRI-O" class="headerlink" title="2.2 将 kind 使用的运行时替换成 CRI-O"></a>2.2 将 kind 使用的运行时替换成 CRI-O</h3><p>在利用 kind 工具构建集群时，默认会采用 <code>kindest/node</code> 镜像作为节点镜像，该镜像内部预设的容器运行时是 containerd。若要将 containerd 更改为 crio 运行时，有两种可行方案：</p><ol><li><p>通过自定义节点镜像的方式进行切换：首先，在 <code>kindest/node</code> 镜像的基础之上重新构建一个新的节点镜像，并结合使用 <code>kind create cluster</code> 命令以及适配 crio 的定制版 <code>kind-config.yaml</code> 文件来创建新的集群。可以参考： <a href="https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2"><code>https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2</code></a> 和 <a href="https://github.com/warm-metal/kindest-base-crio/tree/main"><code>https://github.com/warm-metal/kindest-base-crio/tree/main</code></a> 。</p><blockquote><p>在实际操作过程中，参考网站提供的镜像版本较为陈旧，和现有的 kind 以及 k8s 有兼容性问题。</p></blockquote></li><li><p>在已用 kind 创建好的 k8s 集群上直接操作：进入 Worker 节点或 Master 节点（它们实质上是由 kind 创建的 Docker 容器），手动安装并配置 crio 运行时，然后对节点进行重新初始化或重新加入集群。</p><blockquote><p>为了深入理解 k8s 集群创建过程中的各个环节，并尽可能保留关键操作步骤，我们选择手动在集群内安装 crio 运行时。后续的文章中，我们将尝试探索如何打包适配 crio 的节点镜像。</p></blockquote></li></ol><p>本篇文章重点关注第2种方法。</p><h4 id="2-2-1-创建新集群"><a href="#2-2-1-创建新集群" class="headerlink" title="2.2.1 创建新集群"></a>2.2.1 创建新集群</h4><p>利用本系列第一篇博客的配置文件 <a href="https://github.com/Artistzq/kubelabs/tree/main/hello"><code>https://github.com/Artistzq/kubelabs/tree/main/hello</code></a> ，重新创建集群 <code>crio-hello-cluster</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash create.sh -n crio-hello-cluster<br></code></pre></td></tr></table></figure><p>进入 Master 节点 <code>crio-hello-cluster-control-plane</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-control-plane /bin/bash<br></code></pre></td></tr></table></figure><p>查看信息</p><ul><li><p>查看 k8s 版本</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@hello-crio-cluster-control-plane:/<span class="hljs-comment"># kubectl version</span><br>Client Version: v1.29.0<br>Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3<br>Server Version: v1.29.0<br></code></pre></td></tr></table></figure></li><li><p>查看 linux 发行版本</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@crio-hello-cluster-control-plane:/<span class="hljs-comment"># cat /etc/issue</span><br>Debian GNU/Linux 11 \n \l<br></code></pre></td></tr></table></figure></li><li><p>查看节点详细信息</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@crio-hello-cluster-control-plane:/<span class="hljs-comment"># kubectl get nodes -owide</span><br>NAME                               STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>crio-hello-cluster-control-plane   Ready    control-plane   34m   v1.29.0   172.18.0.6    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker          Ready    &lt;none&gt;          34m   v1.29.0   172.18.0.7    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker2         Ready    &lt;none&gt;          34m   v1.29.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure></li><li><p>查看容器运行时，都是 <code>containerd://1.7.1</code>。</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@crio-hello-cluster-control-plane:/<span class="hljs-comment"># kubectl get nodes -o json | jq -r &#x27;.items[] | &#123;Name: .metadata.name, ContainerRuntime: .status.nodeInfo.containerRuntimeVersion&#125;&#x27;</span><br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-control-plane&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker2&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure></li></ul><h4 id="2-2-2-为-WorkerNode-安装CRIO"><a href="#2-2-2-为-WorkerNode-安装CRIO" class="headerlink" title="2.2.2 为 WorkerNode 安装CRIO"></a>2.2.2 为 WorkerNode 安装CRIO</h4><p><code>kind-config.yaml</code> 配置文件指定了1个 MasterNode 和2个 WorkerNode，我们进入其中一个WorkerNode，按照官网 <a href="https://cri-o.io/"><code>https://cri-o.io/</code></a> 和 Github 官方仓库 <a href="https://github.com/cri-o/cri-o/blob/main/install.md"><code>https://github.com/cri-o/cri-o/blob/main/install.md</code></a> 上的指导，为其安装 CRIO-O。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-worker /bin/bash<br></code></pre></td></tr></table></figure><blockquote><p>注：以下的安装 CRI-O 的操作都是在此节点容器内进行的！</p></blockquote><ol><li><p>指定 <code>VERSION</code> 和 <code>OS</code> 环境变量。  </p><p> OS指定当前系统的版本，官网给出了对应关系，例如Ubuntu 20.04的主机，需要 <code>export OS=xUbuntu_20.04</code> 。 kind 创建的节点为Debian 11，对应 <code>export OS=Debian_11</code>。</p><p> VERSION指定crio的版本，支持主版本和具体版本的指定，例如 <code>1.24:1.24.1</code> 。这里有坑：**<code>opensuse</code> 上支持的最新的版本是 <code>1.24.6</code>** ，更高的版本如 github 上的最新版 <code>1.29.1</code> ，是无法在 <a href="https://download.opensuse.org/repositories"><code>https://download.opensuse.org/repositories</code></a> 中下载的，而文档没有说明这一点，只说明了支持主版本号和具体版本号。因此安装时，如果指定版本为 <code>1.29.1</code> ，会报错，提示无法找到文件。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> OS=Debian_11<br><span class="hljs-built_in">export</span> VERSION=1.24.6<br><span class="hljs-built_in">export</span> SUBVERSION=$(<span class="hljs-built_in">echo</span> <span class="hljs-variable">$VERSION</span> | awk -F<span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-string">&#x27;&#123;print $1&quot;.&quot;$2&#125;&#x27;</span>) <span class="hljs-comment"># 1.24</span><br></code></pre></td></tr></table></figure></li><li><p>安装相关工具</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">apt-get install -y gnupg tzdata<br></code></pre></td></tr></table></figure></li><li><p>添加 apt 源，并下载安装 crio</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">CONTAINERS_URL=<span class="hljs-string">&quot;https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span class="hljs-variable">$&#123;OS&#125;</span>/&quot;</span><br><span class="hljs-comment"># crio 镜像地址</span><br>CRIO_URL=<span class="hljs-string">&quot;http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/<span class="hljs-variable">$&#123;SUBVERSION&#125;</span>:/<span class="hljs-variable">$&#123;VERSION&#125;</span>/<span class="hljs-variable">$&#123;OS&#125;</span>/&quot;</span><br><br><span class="hljs-comment"># 把上述 url 添加到 apt 的源</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;deb <span class="hljs-variable">$&#123;CONTAINERS_URL&#125;</span> /&quot;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;deb <span class="hljs-variable">$&#123;CRIO_URL&#125;</span> /&quot;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:<span class="hljs-variable">$&#123;VERSION&#125;</span>.list<br><br><span class="hljs-comment"># 下载密钥，并将其添加到 apt 信任密钥列表</span><br>curl -L <span class="hljs-variable">$&#123;CONTAINERS_URL&#125;</span>Release.key | apt-key add - || <span class="hljs-literal">true</span><br>curl -L <span class="hljs-variable">$&#123;CRIO_URL&#125;</span>Release.key | apt-key add - || <span class="hljs-literal">true</span><br><br><span class="hljs-comment"># 通过 apt 安装 cri-o 和 cri-o-runc</span><br>apt-get update<br>apt-get install -y cri-o cri-o-runc<br></code></pre></td></tr></table></figure><p> 如果下载太慢可以临时添加代理环境变量，例如：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;http:192.168.115.1:7890&quot;</span><br><span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;http:192.168.115.1:7890&quot;</span><br><span class="hljs-built_in">env</span> | grep proxy <span class="hljs-comment"># 检查</span><br></code></pre></td></tr></table></figure><blockquote><p>注意：在后续要取消 <code>proxy</code> 相关环境变量，或加入 <code>no_proxy</code> 的变量，否则导致此工作节点无法加入集群。</p></blockquote><p> 安装结束时会提示 <code>crictl.yaml</code> 已存在，因为该节点之前使用的是 <code>containerd</code> ，已经配置了 <code>crictl</code> 。这里选择覆盖 <code>Y</code> 。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Configuration file <span class="hljs-string">&#x27;/etc/crictl.yaml&#x27;</span><br>==&gt; File on system created by you or by a script.<br>==&gt; File also <span class="hljs-keyword">in</span> package provided by package maintainer.<br>What would you like to <span class="hljs-keyword">do</span> about it ?  Your options are:<br>    Y or I  : install the package maintainer<span class="hljs-string">&#x27;s version</span><br><span class="hljs-string">    N or O  : keep your currently-installed version</span><br><span class="hljs-string">    D     : show the differences between the versions</span><br><span class="hljs-string">    Z     : start a shell to examine the situation</span><br><span class="hljs-string">The default action is to keep your current version.</span><br><span class="hljs-string">*** crictl.yaml (Y/I/N/O/D/Z) [default=N] ? Y</span><br></code></pre></td></tr></table></figure></li><li><p>配置 cri-o</p><p> 安装完后配置 <code>cri-o</code></p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;[crio.runtime]</span><br><span class="hljs-string">cgroup_manager=\&quot;cgroupfs\&quot;</span><br><span class="hljs-string">conmon_cgroup=\&quot;pod\&quot;</span><br><span class="hljs-string">pause_image = \&quot;k8s.gcr.io/pause:3.2\&quot;</span><br><span class="hljs-string">storage_driver = \&quot;vfs\&quot;&quot;</span> &gt; /etc/crio/crio.conf<br><br></code></pre></td></tr></table></figure></li><li><p>设置 crictl</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 将 crictl.yaml 中所有出现的 containerd 字符串替换为 crio</span><br>sed -i <span class="hljs-string">&#x27;s/containerd/crio/g&#x27;</span> /etc/crictl.yaml<br></code></pre></td></tr></table></figure></li><li><p>检查 containerd 和 crio</p><p> 检查当前状态</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># containerd 状态</span><br>$ systemctl status containerd | grep Active <span class="hljs-comment"># Runing</span><br> Active: active (running) since Fri 2024-01-26 08:23:49 UTC; 1h 8min ago<br><span class="hljs-comment"># crio 状态</span><br>$ systemctl status crio | grep Active <span class="hljs-comment"># Dead</span><br> Active: inactive (dead)<br></code></pre></td></tr></table></figure><p> 关闭 <code>containerd</code> 服务，启动 <code>crio</code> 服务</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl <span class="hljs-built_in">disable</span> containerd &amp;&amp; systemctl stop containerd<br>Removed /etc/systemd/system/multi-user.target.wants/containerd.service.<br>$ systemctl <span class="hljs-built_in">enable</span> crio &amp;&amp; systemctl start crio<br></code></pre></td></tr></table></figure><p> 再次检查状态</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ systemctl status containerd | grep Active <span class="hljs-comment"># Dead</span><br> Active: inactive (dead) since Fri 2024-01-26 09:36:03 UTC; 50s ago<br>$ systemctl status crio | grep Active <span class="hljs-comment"># Runing</span><br> Active: active (running) since Fri 2024-01-26 09:35:08 UTC; 1min 50s ago<br></code></pre></td></tr></table></figure></li></ol><h4 id="2-2-3-使用-Kubeadm-重新加入集群"><a href="#2-2-3-使用-Kubeadm-重新加入集群" class="headerlink" title="2.2.3 使用 Kubeadm 重新加入集群"></a>2.2.3 使用 <code>Kubeadm</code> 重新加入集群</h4><ol><li><p>检查节点状态</p><p> 重新开一个终端，进入 MasterNode，检查集群状态</p><blockquote><p>此命令在宿主机上执行</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-control-plane kubectl get nodes -owide<br>NAME                               STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>crio-hello-cluster-control-plane   Ready      control-plane   77m   v1.29.0   172.18.0.6    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker          NotReady   &lt;none&gt;          76m   v1.29.0   172.18.0.7    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://Unknown<br>crio-hello-cluster-worker2         Ready      &lt;none&gt;          76m   v1.29.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure><p> 观察到前面操作的节点 <code>crio-hello-cluster-worker</code> 已经进入 <code>NotReady</code> 状态，CONTAINER-RUNTIME 为 <code>containerd://Unknown</code> 。</p></li><li><p>使用 <code>kubeadm join</code> 将节点加入集群</p><p> 此时，为了将工作节点重新加入此集群，需要先在MasterNode上生成token：</p><blockquote><p>此命令在MasterNode中执行。记得关闭代理，否则无法加入。可以 <code>unset http_proxy &amp;&amp; unset https_proxy</code>。如果添加环境变量 <code>export no_proxy=&quot;localhost,127.0.0.1,10.0.0.0/8,192.168.0.0/16,172.16.0.0/12&quot;</code> ，测试即使重启也无效。</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubeadm token create --print-join-command<br>kubeadm <span class="hljs-built_in">join</span> crio-hello-cluster-control-plane:6443 --token xeogi6.plnsolw9cd8457mc --discovery-token-ca-cert-hash sha256:0f84c44d447641f23345cd8269a27645fe149d19028a6afe6a07f0b7aac191c5 <br></code></pre></td></tr></table></figure><p> 但在 WorkerNode上执行返回的命令，会报错，提示预检错误，表明kubelet在执行系统验证时尝试加载名为“configs”的内核模块以解析内核配置，但未能在 <code>/lib/modules/5.15.0-92-generic</code> 目录下找到该模块。</p><blockquote><p>以下命令在WorkerNode <code>crio-hello-cluster-worker</code>上执行</p></blockquote> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@hello-crio-worker:/home<span class="hljs-comment"># kubeadm join crio-hello-cluster-control-plane:6443 --token xeogi6.plnsolw9cd8457mc --discovery-token-ca-cert-hash sha256:0f84c44d447641f23345cd8269a27645fe149d19028a6afe6a07f0b7aac191c5 </span><br>[preflight] Running pre-flight checks<br>    [WARNING Swap]: swap is supported <span class="hljs-keyword">for</span> cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default<br>    [WARNING FileExisting-socat]: socat not found <span class="hljs-keyword">in</span> system path<br>[preflight] The system verification failed. Printing the output from the verification:<br>...<br>error execution phase preflight: [preflight] Some fatal errors occurred:<br>    [ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: <span class="hljs-string">&quot;configs&quot;</span>, output: <span class="hljs-string">&quot;modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-92-generic\n&quot;</span>, err: <span class="hljs-built_in">exit</span> status 1<br>[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`<br>To see the stack trace of this error execute with --v=5 or higher<br></code></pre></td></tr></table></figure><p> 暂时有两个解决办法：</p><ul><li>忽略问题。<a href="https://github.com/kubernetes/kubernetes/issues/41025"><code>https://github.com/kubernetes/kubernetes/issues/41025</code></a></li><li>升级内核镜像、配置容器。<a href="https://stackoverflow.com/questions/54128045/errors-while-creating-master-in-cluster-of-kubernetes-in-lxc-container"><code>https://stackoverflow.com/questions/54128045/errors-while-creating-master-in-cluster-of-kubernetes-in-lxc-container</code></a></li></ul><p> 目前采用的是忽略问题，暂时无法判断使用此选项的后续问题。加上忽略错误，节点添加成功！</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubeadm <span class="hljs-built_in">join</span> crio-hello-cluster-control-plane:6443 --token xeogi6.plnsolw9cd8457mc --discovery-token-ca-cert-hash sha256:0f84c44d447641f23345cd8269a27645fe149d19028a6afe6a07f0b7aac191c5 --ignore-preflight-errors=SystemVerification<br><br>[preflight] Running pre-flight checks<br>    [WARNING Swap]: swap is supported <span class="hljs-keyword">for</span> cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default<br>    [WARNING FileExisting-socat]: socat not found <span class="hljs-keyword">in</span> system path<br>[preflight] The system verification failed. Printing the output from the verification:<br>KERNEL_VERSION: 5.15.0-92-generic<br>OS: Linux<br>...<br>    [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: <span class="hljs-string">&quot;configs&quot;</span>, output: <span class="hljs-string">&quot;modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-92-generic\n&quot;</span>, err: <span class="hljs-built_in">exit</span> status 1<br>[preflight] Reading configuration from the cluster...<br>[preflight] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span><br>[kubelet-start] Writing kubelet configuration to file <span class="hljs-string">&quot;/var/lib/kubelet/config.yaml&quot;</span><br>[kubelet-start] Writing kubelet environment file with flags to file <span class="hljs-string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br>[kubelet-start] Starting the kubelet<br>[kubelet-start] Waiting <span class="hljs-keyword">for</span> the kubelet to perform the TLS Bootstrap...<br><br>This node has joined the cluster:<br>* Certificate signing request was sent to apiserver and a response was received.<br>* The Kubelet was informed of the new secure connection details.<br><br>Run <span class="hljs-string">&#x27;kubectl get nodes&#x27;</span> on the control-plane to see this node <span class="hljs-built_in">join</span> the cluster.<br></code></pre></td></tr></table></figure></li><li><p>检查集群状态</p><p> 从宿主机进入MasterNode</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu$ docker <span class="hljs-built_in">exec</span> -it crio-hello-cluster-control-plane /bin/bash<br></code></pre></td></tr></table></figure><blockquote><p>以下命令在 <code>crio-hello-cluster-control-plane</code> 中执行</p></blockquote><p> 检查节点状态，节点 <code>crio-hello-cluster-worker</code> 已处于 <code>Ready</code> 状态，容器运行时已切换为 <code>cri-o://1.24.6</code> 。</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes -owide<br>NAME                               STATUS   ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>crio-hello-cluster-control-plane   Ready    control-plane   106m   v1.29.0   172.18.0.6    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br>crio-hello-cluster-worker          Ready    &lt;none&gt;          11s    v1.29.0   172.18.0.7    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   cri-o://1.24.6<br>crio-hello-cluster-worker2         Ready    &lt;none&gt;          106m   v1.29.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-92-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes -o json | jq -r <span class="hljs-string">&#x27;.items[] | &#123;Name: .metadata.name, ContainerRuntime: .status.nodeInfo.containerRuntimeVersion&#125;&#x27;</span><br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-control-plane&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;cri-o://1.24.6&quot;</span><br>&#125;<br>&#123;<br><span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;crio-hello-cluster-worker2&quot;</span>,<br><span class="hljs-string">&quot;ContainerRuntime&quot;</span>: <span class="hljs-string">&quot;containerd://1.7.1&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure></li></ol><h3 id="2-3-自己构建-kind-node-镜像"><a href="#2-3-自己构建-kind-node-镜像" class="headerlink" title="2.3 自己构建 kind node 镜像"></a>2.3 自己构建 <code>kind node</code> 镜像</h3><p>后续文章尝试</p><hr><h2 id="BUG"><a href="#BUG" class="headerlink" title="BUG"></a>BUG</h2><h3 id="以前的配置无法创建集群"><a href="#以前的配置无法创建集群" class="headerlink" title="以前的配置无法创建集群"></a>以前的配置无法创建集群</h3><p>中途突然无法启动kubelet，任何多节点的集群都无法创建，即使使用前面博客文章的配置文件也无法创建集群，提示kubelet无法启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ bash create.sh -name <span class="hljs-built_in">test</span><br>cluster-name: ame<br>Creating cluster <span class="hljs-string">&quot;ame&quot;</span> ...<br> ✓ Ensuring node image (kindest/node:v1.29.0) 🖼<br> ✓ Preparing nodes 📦 📦 📦  <br> ✓ Writing configuration 📜 <br> ✗ Starting control-plane 🕹️ <br>Deleted nodes: [<span class="hljs-string">&quot;ame-worker&quot;</span> <span class="hljs-string">&quot;ame-worker2&quot;</span> <span class="hljs-string">&quot;ame-control-plane&quot;</span>]<br>ERROR: failed to create cluster: failed to init node with kubeadm: <span class="hljs-built_in">command</span> <span class="hljs-string">&quot;docker exec --privileged ame-control-plane kubeadm init --skip-phases=preflight --config=/kind/kubeadm.conf --skip-token-print --v=6&quot;</span> failed with error: <span class="hljs-built_in">exit</span> status 1<br>Command Output: I0125 11:40:19.477864     172 initconfiguration.go:260] loading configuration from <span class="hljs-string">&quot;/kind/kubeadm.conf&quot;</span><br>···<br>[control-plane] Creating static Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;kube-apiserver&quot;</span><br>I0125 11:40:24.816401     172 local.go:65] [etcd] wrote Static Pod manifest <span class="hljs-keyword">for</span> a <span class="hljs-built_in">local</span> etcd member to <span class="hljs-string">&quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br>···<br>[control-plane] Creating static Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;kube-controller-manager&quot;</span><br>I0125 11:40:24.820970     172 manifests.go:157] [control-plane] wrote static Pod manifest <span class="hljs-keyword">for</span> component <span class="hljs-string">&quot;kube-controller-manager&quot;</span> to <span class="hljs-string">&quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br>I0125 11:40:24.821181     172 manifests.go:102] [control-plane] getting StaticPodSpecs<br>[control-plane] Creating static Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;kube-scheduler&quot;</span><br>I0125 11:40:24.821653     172 manifests.go:128] [control-plane] adding volume <span class="hljs-string">&quot;kubeconfig&quot;</span> <span class="hljs-keyword">for</span> component <span class="hljs-string">&quot;kube-scheduler&quot;</span><br>I0125 11:40:24.822478     172 manifests.go:157] [control-plane] wrote static Pod manifest <span class="hljs-keyword">for</span> component <span class="hljs-string">&quot;kube-scheduler&quot;</span> to <span class="hljs-string">&quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br>I0125 11:40:24.822522     172 kubelet.go:68] Stopping the kubelet<br>[kubelet-start] Writing kubelet environment file with flags to file <span class="hljs-string">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br>[kubelet-start] Writing kubelet configuration to file <span class="hljs-string">&quot;/var/lib/kubelet/config.yaml&quot;</span><br>[kubelet-start] Starting the kubelet<br>I0125 11:40:24.982554     172 waitcontrolplane.go:83] [wait-control-plane] Waiting <span class="hljs-keyword">for</span> the API server to be healthy<br>I0125 11:40:24.983290     172 loader.go:395] Config loaded from file:  /etc/kubernetes/admin.conf<br>[wait-control-plane] Waiting <span class="hljs-keyword">for</span> the kubelet to boot up the control plane as static Pods from directory <span class="hljs-string">&quot;/etc/kubernetes/manifests&quot;</span>. This can take up to 4m0s<br>I0125 11:40:24.985303     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?<span class="hljs-built_in">timeout</span>=10s  <span class="hljs-keyword">in</span> 0 milliseconds<br>···<br>[kubelet-check] Initial <span class="hljs-built_in">timeout</span> of 40s passed.<br>I0125 11:41:04.986148     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?<span class="hljs-built_in">timeout</span>=10s  <span class="hljs-keyword">in</span> 0 milliseconds<br>[kubelet-check] It seems like the kubelet isn<span class="hljs-string">&#x27;t running or healthy.</span><br><span class="hljs-string">[kubelet-check] The HTTP call equal to &#x27;</span>curl -sSL http://localhost:10248/healthz<span class="hljs-string">&#x27; failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.</span><br><span class="hljs-string">I0125 11:41:05.487445     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?timeout=10s  in 0 milliseconds</span><br><span class="hljs-string">···</span><br><span class="hljs-string">[kubelet-check] It seems like the kubelet isn&#x27;</span>t running or healthy.<br>[kubelet-check] The HTTP call equal to <span class="hljs-string">&#x27;curl -sSL http://localhost:10248/healthz&#x27;</span> failed with error: Get <span class="hljs-string">&quot;http://localhost:10248/healthz&quot;</span>: dial tcp [::1]:10248: connect: connection refused.<br>I0125 11:41:10.486636     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?<span class="hljs-built_in">timeout</span>=10s  <span class="hljs-keyword">in</span> 0 milliseconds<br>···<br>[kubelet-check] It seems like the kubelet isn<span class="hljs-string">&#x27;t running or healthy.</span><br><span class="hljs-string">[kubelet-check] The HTTP call equal to &#x27;</span>curl -sSL http://localhost:10248/healthz<span class="hljs-string">&#x27; failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.</span><br><span class="hljs-string">I0125 11:41:20.486427     172 round_trippers.go:553] GET https://ame-control-plane:6443/healthz?timeout=10s  in 0 milliseconds</span><br><span class="hljs-string">···</span><br><span class="hljs-string">couldn&#x27;</span>t initialize a Kubernetes cluster<br>[kubelet-check] It seems like the kubelet isn<span class="hljs-string">&#x27;t running or healthy.</span><br><span class="hljs-string">[kubelet-check] The HTTP call equal to &#x27;</span>curl -sSL http://localhost:10248/healthz<span class="hljs-string">&#x27; failed with error: Get &quot;http://localhost:10248/healthz&quot;: dial tcp [::1]:10248: connect: connection refused.</span><br><span class="hljs-string"></span><br><span class="hljs-string">Unfortunately, an error has occurred:</span><br><span class="hljs-string">timed out waiting for the condition</span><br><span class="hljs-string"></span><br><span class="hljs-string">This error is likely caused by:</span><br><span class="hljs-string">- The kubelet is not running</span><br><span class="hljs-string">- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)</span><br><span class="hljs-string">···</span><br></code></pre></td></tr></table></figure><p>先后检查了节点的 <code>kubelet status</code>、 <code>api-server</code>、 <code>container runtime</code> 等，无问题，最后参考 <a href="https://github.com/kubernetes-sigs/kind/issues/2702"><code>https://github.com/kubernetes-sigs/kind/issues/2702</code></a> ，可能是资源占用太大，因此删除了之前的集群，重新建立，错误解决。</p><h3 id="使用-kind-crio-镜像无法创建集群"><a href="#使用-kind-crio-镜像无法创建集群" class="headerlink" title="使用 kind-crio 镜像无法创建集群"></a>使用 kind-crio 镜像无法创建集群</h3><p>使用 <a href="https://github.com/warm-metal/kindest-base-crio/tree/main"><code>https://github.com/warm-metal/kindest-base-crio/tree/main</code></a> 提供的镜像，依然提示 kubelet 无法连接，错误同上。</p><p>查阅<a href="https://hub.docker.com/r/warmmetal/kindest-node-crio"><code>https://hub.docker.com/r/warmmetal/kindest-node-crio</code></a>，该镜像最近一个月更新过，但即使使用最新的也无法创建集群。 <a href="https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2"><code>https://gist.github.com/aojea/bd1fb766302779b77b8f68fa0a81c0f2</code></a> 提到可能是 kind 和 k8s 更新太快，兼容性被打破，因此本实验抛弃了使用该仓库的镜像 <code>kindest-base-crio</code> 的方案。</p>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>cri</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件</title>
    <link href="/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_2_CNI/"/>
    <url>/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_2_CNI/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件"><a href="#kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件" class="headerlink" title="kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件"></a>kubelabs——k8s学习与实验【2】容器网络接口（CNI）以及切换CNI插件</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><code>Container Network Interface（CNI）</code>，即容器网络接口，是一种标准化的API规范，旨在为容器提供一致且灵活的网络配置方案。在Kubernetes环境中，Kubelet通过调用遵循CNI标准的API与各类网络插件进行交互，以实现容器间多样化的网络连接与管理。接口定义的基本操作包括 <code>ADD</code> 和 <code>DEL</code> 等，负责将Pod添加到网络和从网络中删除。</p><p>k8s通过配置文件来设置使用的CNI插件。在创建pod之前，<a id="ref1"> 管理员可以放置好CNI配置文件 </a> ，当kubelet创建pod时，通过读取配置文件，根据其中指定的CNI插件，执行相应的二进制文件，从而配置Pod网络。当kubelet对Pod执行 ADD 操作时，对应的CNI插件将为Pod分配IP地址、设置路由规则以及其他必要的网络配置；当Pod终止或被删除时，kubelet会执行 DEL 操作，这时CNI插件负责清理相关的网络资源，如释放IP地址等。</p><p>不同的CNI插件有不同的实现方式，例如Overlay和Underlay。常见的CNI插件有Calico和Flannel等，它们各自具备不同的功能特点与适用场景，从而满足不同规模、性能及安全需求的容器网络部署要求。</p><ul><li><p><code>Overlay Network</code></p><ul><li><p>是一种虚拟化技术。通过在底层数据包的基础上，添加额外的封装头部信息，借助隧道打通网络连接，可以屏蔽底层网络通信方式的差异，兼容多种异构底层网络。部署方便；扩展性好。</p></li><li><p>例如：Flannel-vxlan 插件</p></li></ul></li><li><p><code>Underlay Network</code></p><ul><li><p>提供了真正的物理层和数据链路层的连接，网络性能直接取决于硬件设备。直接基于底层传输数据，没有额外数据封装，性能高，吞吐量大，延迟低。但在异构网络环境下扩展困难，网络配置复杂。某些基于Underlay的插件无法使用复杂均衡和服务发现。</p></li><li><p>例如：clico-bgp 插件</p></li></ul></li></ul><p>实际选择时，要从多个需求出发考虑，例如安全需求、负载均衡需求、性能需求等。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="1-创建集群，不配置CNI插件"><a href="#1-创建集群，不配置CNI插件" class="headerlink" title="1 创建集群，不配置CNI插件"></a>1 创建集群，不配置CNI插件</h3><p><code>kind</code> 默认使用了 <code>kindnetd</code> 作为网络插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~$ docker <span class="hljs-built_in">exec</span> -it hello-cluster-worker <span class="hljs-built_in">ls</span> /etc/cni/net.d<br>10-kindnet.conflist<br></code></pre></td></tr></table></figure><p>为了实验，设置创建集群时不配置CNI插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kind create cluster --name cni-test-cluster --config kind-config.yaml<br><span class="hljs-comment"># 加载本地镜像（如果不加载会出现ImagePullBackOff、ErrImagePull等错误）</span><br>$ kind load docker-image go-hello-world-image:v0.0.1 --name cni-test-cluster<br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># kind-config.yaml</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span><br><span class="hljs-attr">networking:</span><br>  <span class="hljs-comment"># the default CNI will not be installed</span><br>  <span class="hljs-attr">disableDefaultCNI:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">nodes:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br></code></pre></td></tr></table></figure><p>查看 <code>/etc/cni/net.d</code> 下目录为空。</p><p>查看 <code>nodes</code>，状态为 not ready。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes<br>NAME                             STATUS     ROLES           AGE    VERSION<br>cni-test-cluster-control-plane   NotReady   control-plane   2d2h   v1.29.0<br>cni-test-cluster-worker          NotReady   &lt;none&gt;          2d2h   v1.29.0<br>cni-test-cluster-worker2         NotReady   &lt;none&gt;          2d2h   v1.29.0<br></code></pre></td></tr></table></figure><p>查看 <code>pods</code> 。<code>core-dns</code> 和 <code>local-path-provisioner</code> 不在运行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods --all-namespaces<br>NAMESPACE            NAME                                                     READY   STATUS    RESTARTS         AGE<br>kube-system          coredns-76f75df574-9swqg                                 0/1     Pending   0                47h<br>kube-system          coredns-76f75df574-j5mgx                                 0/1     Pending   0                47h<br>kube-system          etcd-cni-test-cluster-control-plane                      1/1     Running   0                3m7s<br>kube-system          kube-apiserver-cni-test-cluster-control-plane            1/1     Running   0                3m7s<br>kube-system          kube-controller-manager-cni-test-cluster-control-plane   1/1     Running   7 (3m25s ago)    47h<br>kube-system          kube-proxy-l8cpb                                         1/1     Running   71 (3m25s ago)   47h<br>kube-system          kube-proxy-nwpzn                                         1/1     Running   4 (3m25s ago)    47h<br>kube-system          kube-proxy-zhhfn                                         1/1     Running   71 (3m25s ago)   47h<br>kube-system          kube-scheduler-cni-test-cluster-control-plane            1/1     Running   7 (3m25s ago)    47h<br>local-path-storage   local-path-provisioner-6f8956fb48-f59n6                  0/1     Pending   0                47h<br></code></pre></td></tr></table></figure><p><code>kubectl apply -f cni-deploy.yaml</code> 部署服务后，Pod处于Pending状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">kubectl get pods<br>NAME                                   READY   STATUS    RESTARTS   AGE<br>cni-test-deployment-58569cd74c-qrxz5   0/1     Pending   0          142m<br>cni-test-deployment-58569cd74c-v86tf   0/1     Pending   0          142m<br>cni-test-deployment-58569cd74c-wg9rn   0/1     Pending   0          142m<br></code></pre></td></tr></table></figure><h3 id="2-手动添加flannel插件"><a href="#2-手动添加flannel插件" class="headerlink" title="2 手动添加flannel插件"></a>2 手动添加flannel插件</h3><p>Flannel插件旨在为不同节点上的容器重新规划IP地址的使用规则，分配同属一个内网且不重复的IP地址。</p><h4 id="2-1-准备好配置文件"><a href="#2-1-准备好配置文件" class="headerlink" title="2.1 准备好配置文件"></a>2.1 准备好配置文件</h4><p>如<a href="#ref1">上文</a>提到，添加CNI插件需要将配置文件放入节点的&#x2F;etc&#x2F;cni&#x2F;net.d。</p><p>可以自动化地进行。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 下载 kube-flannel.yaml 文件</span><br>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml<br><span class="hljs-comment"># 部署 flannel</span><br>kubectl apply -f kube-flannel.yml<br></code></pre></td></tr></table></figure><p>节点已经处于 <code>Ready</code> 状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get nodes --all-namespaces<br>NAME                             STATUS   ROLES           AGE    VERSION<br>cni-test-cluster-control-plane   Ready    control-plane   2d2h   v1.29.0<br>cni-test-cluster-worker          Ready    &lt;none&gt;          2d2h   v1.29.0<br>cni-test-cluster-worker2         Ready    &lt;none&gt;          2d2h   v1.29.0<br></code></pre></td></tr></table></figure><p>配置文件已自动写入工作节点 <code>/etc/cni/net.d</code> 目录下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it cni-test-cluster-worker <span class="hljs-built_in">ls</span> /etc/cni/net.d<br>10-flannel.conflist<br></code></pre></td></tr></table></figure><p>但pods依然未就绪：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods --all-namespaces<br>NAMESPACE            NAME                                                     READY   STATUS              RESTARTS         AGE<br>default              cni-test-deployment-58569cd74c-qrxz5                     0/1     ContainerCreating   0                165m<br>default              cni-test-deployment-58569cd74c-v86tf                     0/1     ContainerCreating   0                165m<br>default              cni-test-deployment-58569cd74c-wg9rn                     0/1     ContainerCreating   0                165m<br>kube-flannel         kube-flannel-ds-bkl5b                                    1/1     Running             0                4m1s<br>kube-flannel         kube-flannel-ds-qfhwm                                    1/1     Running             0                4m1s<br>kube-flannel         kube-flannel-ds-xwn6m                                    1/1     Running             0                4m1s<br>kube-system          coredns-76f75df574-9swqg                                 0/1     ContainerCreating   0                2d2h<br>kube-system          coredns-76f75df574-j5mgx                                 0/1     ContainerCreating   0                2d2h<br>kube-system          etcd-cni-test-cluster-control-plane                      1/1     Running             0                5m31s<br>kube-system          kube-apiserver-cni-test-cluster-control-plane            1/1     Running             0                5m31s<br>kube-system          kube-controller-manager-cni-test-cluster-control-plane   1/1     Running             8 (6m26s ago)    2d2h<br>kube-system          kube-proxy-l8cpb                                         1/1     Running             72 (6m26s ago)   2d2h<br>kube-system          kube-proxy-nwpzn                                         1/1     Running             5 (6m26s ago)    2d2h<br>kube-system          kube-proxy-zhhfn                                         1/1     Running             72 (6m26s ago)   2d2h<br>kube-system          kube-scheduler-cni-test-cluster-control-plane            1/1     Running             8 (6m26s ago)    2d2h<br>local-path-storage   local-path-provisioner-6f8956fb48-f59n6                  0/1     ContainerCreating   0                2d2h<br><br></code></pre></td></tr></table></figure><p>通过 <code>kubectl describe pod cni-test-deployment-58569cd74c-qrxz5</code> 查看信息，提示在委派 <code>ADD</code> 操作时，无法找到 <code>/opt/cni/bin</code> 目录下的<code>bridge</code>二进制文件插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl describe pod cni-test-deployment-58569cd74c-qrxz5<br>...<br>Conditions:<br>  Type                        Status<br>  PodReadyToStartContainers   False <br>  Initialized                 True <br>  Ready                       False <br>  ContainersReady             False <br>  PodScheduled                True <br>Events:<br>  Type     Reason                  Age                     From               Message<br>  ----     ------                  ----                    ----               -------<br>  Warning  FailedCreatePodSandBox  4m16s                   kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network <span class="hljs-keyword">for</span> sandbox <span class="hljs-string">&quot;0ca3e187ace57dd4af46bb8e4c60b5b5973510f5f4a02e8efe1954d34f1b7101&quot;</span>: plugin <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;flannel&quot;</span> failed (add): failed to delegate add: failed to find plugin <span class="hljs-string">&quot;bridge&quot;</span> <span class="hljs-keyword">in</span> path [/opt/cni/bin]<br></code></pre></td></tr></table></figure><h4 id="2-2-手动安装插件的二进制文件"><a href="#2-2-手动安装插件的二进制文件" class="headerlink" title="2.2 手动安装插件的二进制文件"></a>2.2 手动安装插件的二进制文件</h4><p>在运行 kind 的宿主机上下载文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 下载文件</span><br>$ wget https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz<br></code></pre></td></tr></table></figure><p>复制到多个工作节点（docker容器），并解压。创建脚本自动处理这一过程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ./unzip_to_docker.sh</span><br><br><span class="hljs-comment"># 获取所有名称以 &#x27;cni-test-cluster&#x27; 开头的容器</span><br>container_names=$(docker ps --format <span class="hljs-string">&quot;&#123;&#123;.Names&#125;&#125;&quot;</span> | grep <span class="hljs-string">&#x27;^cni-test-cluster&#x27;</span>)<br><br><span class="hljs-comment"># 定义本地压缩文件路径和目标解压目录（请替换为实际值）</span><br>local_archive=<span class="hljs-string">&quot;cni-plugins-linux-amd64-v1.4.0.tgz&quot;</span><br>target_directory=<span class="hljs-string">&quot;/opt/cni/bin&quot;</span><br><br><span class="hljs-comment"># 遍历每个匹配到的容器</span><br><span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> <span class="hljs-variable">$container_names</span>; <span class="hljs-keyword">do</span><br>    <span class="hljs-comment"># 将本地压缩文件复制到容器内</span><br>    docker <span class="hljs-built_in">cp</span> <span class="hljs-string">&quot;<span class="hljs-variable">$local_archive</span>&quot;</span> <span class="hljs-string">&quot;<span class="hljs-variable">$name</span>:<span class="hljs-variable">$target_directory</span>&quot;</span><br>    <span class="hljs-comment"># 在容器内部执行解压命令</span><br>    docker <span class="hljs-built_in">exec</span> -it <span class="hljs-string">&quot;<span class="hljs-variable">$name</span>&quot;</span> bash -c <span class="hljs-string">&quot;cd &#x27;<span class="hljs-variable">$target_directory</span>&#x27; &amp;&amp; tar -zxvf &#x27;<span class="hljs-variable">$local_archive</span>&#x27;&quot;</span><br>    <span class="hljs-comment"># 展示文件夹下内容</span><br>    docker <span class="hljs-built_in">exec</span> -it <span class="hljs-string">&quot;<span class="hljs-variable">$name</span>&quot;</span> bash -c <span class="hljs-string">&quot;cd &#x27;<span class="hljs-variable">$target_directory</span>&#x27; &amp;&amp; ls&quot;</span><br><span class="hljs-keyword">done</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Finished deploying files to containers.&quot;</span><br></code></pre></td></tr></table></figure><p>在运行 kind 的宿主机上运行此脚本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ bash unzip_to_docker.sh <br>Successfully copied 46.9MB to cni-test-cluster-control-plane:/opt/cni/bin<br>./<br>./loopback<br>./bandwidth<br>./ptp<br>./vlan<br>./host-device<br>./tuning<br>./vrf<br>./sbr<br>./tap<br>./dhcp<br>./static<br>./firewall<br>./macvlan<br>./dummy<br>./bridge<br>./ipvlan<br>./portmap<br>./host-local<br>bandwidth  cni-plugins-linux-amd64-v1.4.0.tgz  dummy flannel      host-local  loopback  portmap  sbr     tap     vlan<br>bridge   dhcp       firewall  host-device  ipvlan  macvlan   ptp      static  tuning  vrf<br>Successfully copied 46.9MB to cni-test-cluster-worker2:/opt/cni/bin<br>./<br>./loopback<br>./bandwidth<br>./ptp<br>./vlan<br>./host-device<br>./tuning<br>./vrf<br>./sbr<br>./tap<br>./dhcp<br>./static<br>./firewall<br>./macvlan<br>./dummy<br>./bridge<br>./ipvlan<br>./portmap<br>./host-local<br>bandwidth  cni-plugins-linux-amd64-v1.4.0.tgz  dummy flannel      host-local  loopback  portmap  sbr     tap     vlan<br>bridge   dhcp       firewall  host-device  ipvlan  macvlan   ptp      static  tuning  vrf<br>Successfully copied 46.9MB to cni-test-cluster-worker:/opt/cni/bin<br>./<br>./loopback<br>./bandwidth<br>./ptp<br>./vlan<br>./host-device<br>./tuning<br>./vrf<br>./sbr<br>./tap<br>./dhcp<br>./static<br>./firewall<br>./macvlan<br>./dummy<br>./bridge<br>./ipvlan<br>./portmap<br>./host-local<br>bandwidth  cni-plugins-linux-amd64-v1.4.0.tgz  dummy flannel      host-local  loopback  portmap  sbr     tap     vlan<br>bridge   dhcp       firewall  host-device  ipvlan  macvlan   ptp      static  tuning  vrf<br>Finished deploying files to containers.<br></code></pre></td></tr></table></figure><p>进入workNode查看网络，可以看到网卡flannel.1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ docker <span class="hljs-built_in">exec</span> -it cni-test-cluster-worker /bin/bash<br>root@cni-test-cluster-worker:/<span class="hljs-comment"># ip a</span><br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000<br>    <span class="hljs-built_in">link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host <br>       valid_lft forever preferred_lft forever<br>2: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default <br>    <span class="hljs-built_in">link</span>/ether a6:b3:45:67:33:b2 brd ff:ff:ff:ff:ff:ff<br>    inet 10.244.1.0/32 scope global flannel.1<br>       valid_lft forever preferred_lft forever<br>10: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default <br>    <span class="hljs-built_in">link</span>/ether 02:42:ac:12:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0<br>    inet 172.18.0.4/16 brd 172.18.255.255 scope global eth0<br>       valid_lft forever preferred_lft forever<br>    inet6 fc00:f853:ccd:e793::4/64 scope global nodad <br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::42:acff:fe12:4/64 scope <span class="hljs-built_in">link</span> <br>       valid_lft forever preferred_lft forever<br></code></pre></td></tr></table></figure><p>进入masterNode后，检查子网环境变量，已写入。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@cni-test-cluster-control-plane:/var/run/flannel<span class="hljs-comment"># cat subnet.env </span><br>FLANNEL_NETWORK=10.244.0.0/16<br>FLANNEL_SUBNET=10.244.0.1/24<br>FLANNEL_MTU=1450<br>FLANNEL_IPMASQ=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>检查pods，<code>coredns</code> 和自定义的pod <code>cni-test-deployment</code> 已正常运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods --all-namespaces<br>NAMESPACE            NAME                                                     READY   STATUS    RESTARTS       AGE<br>default              cni-test-deployment-58569cd74c-7hdmp                     1/1     Running   0              9m52s<br>default              cni-test-deployment-58569cd74c-9wcqw                     1/1     Running   0              9m52s<br>default              cni-test-deployment-58569cd74c-bbwhk                     1/1     Running   0              9m52s<br>kube-flannel         kube-flannel-ds-7dfkb                                    1/1     Running   1 (35m ago)    56m<br>kube-flannel         kube-flannel-ds-vfvbz                                    1/1     Running   1 (35m ago)    56m<br>kube-flannel         kube-flannel-ds-zc88f                                    1/1     Running   2 (34m ago)    56m<br>kube-system          coredns-76f75df574-9swqg                                 1/1     Running   0              2d3h<br>kube-system          coredns-76f75df574-j5mgx                                 1/1     Running   0              2d3h<br>kube-system          etcd-cni-test-cluster-control-plane                      1/1     Running   1 (35m ago)    95m<br>kube-system          kube-apiserver-cni-test-cluster-control-plane            1/1     Running   1 (35m ago)    95m<br>kube-system          kube-controller-manager-cni-test-cluster-control-plane   1/1     Running   9 (35m ago)    2d3h<br>kube-system          kube-proxy-l8cpb                                         1/1     Running   73 (35m ago)   2d3h<br>kube-system          kube-proxy-nwpzn                                         1/1     Running   6 (35m ago)    2d3h<br>kube-system          kube-proxy-zhhfn                                         1/1     Running   73 (35m ago)   2d3h<br>kube-system          kube-scheduler-cni-test-cluster-control-plane            1/1     Running   9 (35m ago)    2d3h<br>local-path-storage   local-path-provisioner-6f8956fb48-f59n6                  1/1     Running   0              2d3h<br></code></pre></td></tr></table></figure><p>进入在 <code>worker2</code> 节点上运行的 <code>IP = 10.244.2.7</code> 的pod ，访问在 <code>worker</code> 节点上运行的<code>IP = 10.244.1.8</code> 的pod，可以访问。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ kubectl get pods -owide<br>NAME                                   READY   STATUS    RESTARTS   AGE   IP           NODE                       NOMINATED NODE   READINESS GATES<br>cni-test-deployment-58569cd74c-7hdmp   1/1     Running   0          11m   10.244.2.7   cni-test-cluster-worker2   &lt;none&gt;           &lt;none&gt;<br>cni-test-deployment-58569cd74c-9wcqw   1/1     Running   0          11m   10.244.1.8   cni-test-cluster-worker    &lt;none&gt;           &lt;none&gt;<br>cni-test-deployment-58569cd74c-bbwhk   1/1     Running   0          11m   10.244.1.7   cni-test-cluster-worker    &lt;none&gt;           &lt;none&gt;<br><br>$ kubectl <span class="hljs-built_in">exec</span> cni-test-deployment-58569cd74c-7hdmp -it sh<br>/app <span class="hljs-comment"># curl 10.244.1.8:8080/test</span><br>hello world<br>/app <span class="hljs-comment"># exit</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>cni</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubelabs——k8s学习与实验【1】使用 kind 搭建本地集群</title>
    <link href="/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_1_%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4/"/>
    <url>/2024/01/14/k8s%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93_1_%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="kubelabs——k8s学习与实验【1】使用-kind-搭建本地集群"><a href="#kubelabs——k8s学习与实验【1】使用-kind-搭建本地集群" class="headerlink" title="kubelabs——k8s学习与实验【1】使用 kind 搭建本地集群"></a>kubelabs——k8s学习与实验【1】使用 kind 搭建本地集群</h1><h2 id="1-概念总结"><a href="#1-概念总结" class="headerlink" title="1 概念总结"></a>1 概念总结</h2><h3 id="1-1-kubernetes（k8s）"><a href="#1-1-kubernetes（k8s）" class="headerlink" title="1.1 kubernetes（k8s）"></a>1.1 kubernetes（k8s）</h3><ul><li>kubernetes（k8s）是用于管理云平台中的多个容器化应用的工具。</li><li>理解：<ul><li>现在的大型程序被分割为多个服务，各自运行在单独的容器里。</li><li>为了提升性能和可用性，多个容器需要分散在多个物理机器上，并拥有备份。</li><li>多个服务和容器需要通信</li><li>k8s用于自动化管理此系统。</li></ul></li></ul><h3 id="1-2-组成"><a href="#1-2-组成" class="headerlink" title="1.2 组成"></a>1.2 组成</h3><h4 id="物理层面"><a href="#物理层面" class="headerlink" title="物理层面"></a>物理层面</h4><ul><li>Master节点<ul><li>物理服务器</li><li>kubectl：操作k8s的命令行工具。添加deployment、service等。</li><li>apiServer：对象的请求和调用操作通过此模块提供的接口进行。</li><li>kube-scheduler：资源调度</li><li>kube-controller-manager：维护集群状态</li><li>etcd分布式存储：保存集群状态</li></ul></li><li>Worker节点<ul><li>物理服务器</li><li>kubelet：创建和管理pod；与master通讯</li><li>kube-proxy：负责不同pod通信。</li><li>资源（Pod等）</li></ul></li></ul><h4 id="逻辑层面"><a href="#逻辑层面" class="headerlink" title="逻辑层面"></a>逻辑层面</h4><ul><li><p>主要资源对象</p><ul><li>Container：容器化应用。</li><li>Pod：k8s创建或销毁的最小单位。一个pod包含一个或多个容器。Pod中容器共享网络、存储和计算资源。</li><li>Service：通过labels绑定pod，提供Cluster IP地址和服务名来访问Pod资源。好处是可以实现多个Pod负载均衡，并固定访问的url，Pod IP或Cluster IP变动时不会产生影响。</li><li>Deployment：管理和控制Pod的数量，确保每时每刻有用户要求数量的 Pod 在工作，某Pod出现问题就重新拉起。</li></ul></li><li><p>为了更好的提供开放、扩展、规范等能力的规范</p><ul><li>CNI：容器网络接口。定义通信规范，屏蔽底层使用不同网络插件的差异。</li><li>CSI：容器存储接口。将任意存储系统规范地暴露给容器化应用程序。</li><li>CRI：容器运行时接口。定义规范，使k8s可以兼容多种容器引擎，如docker、frakti等。</li></ul></li></ul><h2 id="2-实验操作"><a href="#2-实验操作" class="headerlink" title="2 实验操作"></a>2 实验操作</h2><h3 id="2-1-环境准备"><a href="#2-1-环境准备" class="headerlink" title="2.1 环境准备"></a>2.1 环境准备</h3><ul><li>宿主机：Windows下通过vmware运行的Ubuntu。磁盘：30GB，内存：5.2G  </li><li>选择用kind搭建集群</li></ul><h3 id="2-2-自制服务镜像"><a href="#2-2-自制服务镜像" class="headerlink" title="2.2 自制服务镜像"></a>2.2 自制服务镜像</h3><ul><li>go 服务<br>请求ip:8080&#x2F;test返回字符串”hello world”</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br>        <span class="hljs-string">&quot;fmt&quot;</span><br>        <span class="hljs-string">&quot;net/http&quot;</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">helloWorld</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<br>        <span class="hljs-keyword">if</span> r.Method == http.MethodGet &amp;&amp; r.URL.Path == <span class="hljs-string">&quot;/test&quot;</span> &#123;<br>                w.WriteHeader(http.StatusOK)<br>                fmt.Fprintln(w, <span class="hljs-string">&quot;hello world&quot;</span>)<br>                <span class="hljs-keyword">return</span><br>        &#125;<br>        http.NotFound(w, r)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>        http.HandleFunc(<span class="hljs-string">&quot;/test&quot;</span>, helloWorld)<br><br>        <span class="hljs-comment">// 监听并在 8080 端口启动服务</span><br>        fmt.Println(<span class="hljs-string">&quot;Server is listening on port 8080...&quot;</span>)<br>        http.ListenAndServe(<span class="hljs-string">&quot;:8080&quot;</span>, <span class="hljs-literal">nil</span>)<br>&#125;<br><br></code></pre></td></tr></table></figure><ul><li>Dockerfile</li></ul><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs Dockerfile"><span class="hljs-keyword">ARG</span> IMAGE_VERSION=v0.<span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 使用官方的Golang基础镜像作为父镜像</span><br><span class="hljs-keyword">FROM</span> golang:latest as builder<br><br><span class="hljs-comment"># 设置工作目录</span><br><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span><br><br><span class="hljs-comment"># 将项目文件复制到容器的工作目录中</span><br><span class="hljs-keyword">COPY</span><span class="language-bash"> . .</span><br><br><span class="hljs-keyword">ENV</span> CGO_ENABLED=<span class="hljs-number">0</span><br><br><span class="hljs-comment"># 构建应用</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> go build -o main .</span><br><br><span class="hljs-comment"># 使用一个新的轻量级基础镜像，例如Alpine</span><br><span class="hljs-keyword">FROM</span> alpine:latest<br><br><span class="hljs-keyword">RUN</span><span class="language-bash"> apk update</span><br><br><span class="hljs-comment"># 安装curl和其他可能需要的依赖</span><br><span class="hljs-keyword">RUN</span><span class="language-bash"> apk add --no-cache curl</span><br><br><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span><br><br><span class="hljs-comment"># 复制编译好的二进制文件到新镜像中</span><br><span class="hljs-keyword">COPY</span><span class="language-bash"> --from=builder /app/main /app/</span><br><br><span class="hljs-comment"># 设置工作目录和提供运行时环境</span><br><span class="hljs-keyword">WORKDIR</span><span class="language-bash"> /app</span><br><span class="hljs-keyword">CMD</span><span class="language-bash"> [<span class="hljs-string">&quot;./main&quot;</span>]</span><br><br><span class="hljs-comment"># 声明运行时容器提供的服务端口</span><br><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8080</span><br><br></code></pre></td></tr></table></figure><ul><li>运行命令</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker build -t go-hello-world-image:v0.0.1 .<br></code></pre></td></tr></table></figure><h3 id="2-3-kind创建集群"><a href="#2-3-kind创建集群" class="headerlink" title="2.3 kind创建集群"></a>2.3 kind创建集群</h3><ul><li>文件结构</li></ul><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs txt">├── go-hello<br>│   ├── app<br>│   ├── Dockerfile<br>│   ├── go.mod<br>│   └── main.go<br>├── hello-deploy.yaml<br>├── hello-service.yaml<br>├── kind-config.yaml<br>├── nginx-deployment.yaml<br>└── nginx-service.yaml<br></code></pre></td></tr></table></figure><ul><li>集群配置文件：kind-config.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kind.x-k8s.io/v1alpha4</span><br><span class="hljs-attr">nodes:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">control-plane</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span> <span class="hljs-comment"># 指定worknode镜像</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span><br>  <span class="hljs-attr">image:</span> <span class="hljs-string">kindest/node:v1.29.0</span><br></code></pre></td></tr></table></figure><ul><li>deployment配置文件：hello-deploy.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">go-hello-world-deployment</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">go-hello-world</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">go-hello-world</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">go-hello-world-container</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">go-hello-world-image:v0.0.1</span> <span class="hljs-comment"># 替换为您的镜像名称和标签</span><br>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span> <span class="hljs-comment"># 假设服务监听在容器内部的8080端口</span><br></code></pre></td></tr></table></figure><ul><li>service配置文件：hello-service.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">hello-service</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span> <span class="hljs-comment"># 或者 LoadBalancer 如果在模拟云环境中</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">go-hello-world</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">30007</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span><br>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30008</span> <span class="hljs-comment"># 如果是NodePort类型，设置一个可用端口</span><br></code></pre></td></tr></table></figure><ul><li>搭建集群</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 创建集群</span><br>kind create cluster --name hello-cluster --config kind-config.yaml<br><span class="hljs-comment"># 加载镜像</span><br>kind load docker-image go-hello-world-image:v0.0.1 --name hello-cluster<br><span class="hljs-comment"># 创建 deployment</span><br>kubectl apply -f hello-deploy.yaml<br><span class="hljs-comment"># 创建 service</span><br>kubectl apply -f hello-service.yaml<br></code></pre></td></tr></table></figure><ul><li>Docker信息概览</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ docker ps<br>CONTAINER ID   IMAGE                         COMMAND                  CREATED       STATUS       PORTS                                         NAMES<br>a20802131005   kindest/node:v1.29.0          <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   4 hours ago   Up 4 hours                                                 hello-cluster-worker<br>710f6226c8ab   kindest/node:v1.29.0          <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   4 hours ago   Up 4 hours   127.0.0.1:35839-&gt;6443/tcp                     hello-cluster-control-plane<br>c21dc643b049   kindest/node:v1.29.0          <span class="hljs-string">&quot;/usr/local/bin/entr…&quot;</span>   4 hours ago   Up 4 hours                                                 hello-cluster-worker2<br>bc232db8553e   go-hello-world-image:v0.0.1   <span class="hljs-string">&quot;./main&quot;</span>                 4 hours ago   Up 4 hours   0.0.0.0:10086-&gt;8080/tcp, :::10086-&gt;8080/tcp   hello-local<br></code></pre></td></tr></table></figure><ul><li>nodes信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl get nodes -o wide<br>NAME                          STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME<br>hello-cluster-control-plane   Ready    control-plane   3h41m   v1.29.0   172.18.0.2    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-91-generic   containerd://1.7.1<br>hello-cluster-worker          Ready    &lt;none&gt;          3h41m   v1.29.0   172.18.0.4    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-91-generic   containerd://1.7.1<br>hello-cluster-worker2         Ready    &lt;none&gt;          3h41m   v1.29.0   172.18.0.3    &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.15.0-91-generic   containerd://1.7.1<br></code></pre></td></tr></table></figure><ul><li>pods信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl get pods -o wide<br>NAME                                         READY   STATUS    RESTARTS   AGE     IP           NODE                    NOMINATED NODE   READINESS GATES<br>go-hello-world-deployment-58569cd74c-85hw5   1/1     Running   0          3h33m   10.244.1.5   hello-cluster-worker    &lt;none&gt;           &lt;none&gt;<br>go-hello-world-deployment-58569cd74c-bd8ht   1/1     Running   0          3h33m   10.244.2.3   hello-cluster-worker2   &lt;none&gt;           &lt;none&gt;<br>go-hello-world-deployment-58569cd74c-qgfm7   1/1     Running   0          3h33m   10.244.1.4   hello-cluster-worker    &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><ul><li>service信息</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl get service -o wide<br>NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE     SELECTOR<br>hello-service   NodePort    10.96.2.242   &lt;none&gt;        8080:30008/TCP   3h13m   app=go-hello-world<br>kubernetes      ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP          3h42m   &lt;none&gt;<br></code></pre></td></tr></table></figure><h3 id="2-4-结果与问题"><a href="#2-4-结果与问题" class="headerlink" title="2.4 结果与问题"></a>2.4 结果与问题</h3><p>使用Kind创建了集群。自定义的服务监听 <code>port = 8080</code>，service监听 <code>sport = 30007</code>，在workNode上开放 <code>NodePort=30008</code>。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h5 id="1-进入到pod内部"><a href="#1-进入到pod内部" class="headerlink" title="1 进入到pod内部"></a>1 进入到pod内部</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">yzq@ubuntu:~/Documents/k8s-proj/hello$ kubectl <span class="hljs-built_in">exec</span> -it go-hello-world-deployment-58569cd74c-85hw5 sh<br>kubectl <span class="hljs-built_in">exec</span> [POD] [COMMAND] is DEPRECATED and will be removed <span class="hljs-keyword">in</span> a future version. Use kubectl <span class="hljs-built_in">exec</span> [POD] -- [COMMAND] instead.<br>/app <span class="hljs-comment"># curl localhost:8080/test</span><br>hello world<br>/app <span class="hljs-comment"># curl 10.244.1.4:8080/test</span><br>hello world<br>/app <span class="hljs-comment"># curl 10.96.2.242:30007/test</span><br>hello world<br>/app <span class="hljs-comment"># curl hello-service:30007/test</span><br>hello world<br></code></pre></td></tr></table></figure><ul><li>可以用 <code>localhost:port</code>访问本pod服务</li><li>可以用 <code>pod ip:port</code>访问对应pod的服务</li><li>创建Service后，可以用 <code>clusterIP:sport</code>可以访问服务</li><li>创建Service后，<strong>无法</strong>通过 <code>hello-service:sport</code>访问服务 <a href="#bug1">[1]</a></li></ul><h5 id="2-进入到workNode内部"><a href="#2-进入到workNode内部" class="headerlink" title="2 进入到workNode内部"></a>2 进入到workNode内部</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker <span class="hljs-built_in">exec</span> -it hello-cluster-worker sh<br></code></pre></td></tr></table></figure><ul><li>使用 <code>localhost:NodePort</code>可以访问本workNode上的服务；</li><li>使用 <code>WorkNodeIp:NodePort</code>和 <code>其他WorkNodeIp:NodePort</code><strong>无法</strong>访问服务 <a href="#bug2">[2]</a></li></ul><h4 id="问题与解决"><a href="#问题与解决" class="headerlink" title="问题与解决"></a>问题与解决</h4><p><a id="bug1">[1]</a> 检查kube-dns，无明显错误，重启后解决。</p><p><a id="bug2">[2]</a> 排查过程如下：</p><ul><li><p>检查 <code>ufw</code>防火墙：未开启防火墙</p></li><li><p>检查CNI日志，workNode拥有子网段，可以分配给pod<br>Node hello-cluster-control-plane has CIDR [<code>10.244.0.0/24</code>]<br>Node hello-cluster-worker has CIDR [<code>10.244.1.0/24</code>]<br>Node hello-cluster-worker2 has CIDR [<code>10.244.2.0/24</code>]</p></li><li><p>检查 <code>ping</code>：可以ping通IP</p></li><li><p>通过 <code>curl -v</code> 查看详细信息，发现走了代理 <code>192.168.115.1</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Windows的IP为192.168.115.1；在这个windows里面，我运行了一个ubuntu虚拟机，IP为192.168.115.128；</span><br><span class="hljs-comment"># windows上的7890端口设置了代理服务器，用于访问资源；</span><br><span class="hljs-comment"># 为了让ubuntu虚拟机同样能够访问资源，我配置ubuntu的代理服务器为192.168.115.1:7890；   </span><br><br>curl -v 172.18.0.3:30008<br><br>* Uses proxy <span class="hljs-built_in">env</span> variable no_proxy == <span class="hljs-string">&#x27;172.18.0.0/16,fc00:f853:ccd:e793::/64,localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24,::1,10.96.0.0/16,10.244.0.0/16,hello-cluster-control-plane,hello-cluster-worker,hello-cluster-worker2,.svc,.svc.cluster,.svc.cluster.local&#x27;</span><br>* Uses proxy <span class="hljs-built_in">env</span> variable http_proxy == <span class="hljs-string">&#x27;http://192.168.115.1:7890/&#x27;</span><br>*   Trying 192.168.115.1:7890...<br>* Connected to 192.168.115.1 (192.168.115.1) port 7890 (<span class="hljs-comment">#0)</span><br>&gt; GET http://172.18.0.3:30008/ HTTP/1.1<br>&gt; Host: 172.18.0.3:30008<br>&gt; User-Agent: curl/7.74.0<br>&gt; Accept: */*<br>&gt; Proxy-Connection: Keep-Alive<br>&gt; <br>* Mark bundle as not supporting multiuse<br>&lt; HTTP/1.1 502 Bad Gateway<br>&lt; Connection: keep-alive<br>&lt; Keep-Alive: <span class="hljs-built_in">timeout</span>=4<br>&lt; Proxy-Connection: keep-alive<br>&lt; Content-Length: 0<br>&lt; <br></code></pre></td></tr></table></figure></li><li><p><code>curl --noproxy</code>：强制使用curl不走代理，访问成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># curl -v --noproxy &#x27;*&#x27; 172.18.0.3:30008/test</span><br>hello world<br></code></pre></td></tr></table></figure></li><li><p>检查workNode上的环境变量：确实存在no_proxy环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># env | grep no_proxy</span><br>no_proxy=172.18.0.0/16,fc00:f853:ccd:e793::/64,localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24,::1,10.96.0.0/16,10.244.0.0/16,hello-cluster-control-plane,hello-cluster-worker,hello-cluster-worker2,.svc,.svc.cluster,.svc.cluster.local<span class="hljs-string">&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p>使用其他工具 <code>wget</code>访问接口：也走了代理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># wget -O - http://172.18.0.3:30008/test</span><br></code></pre></td></tr></table></figure></li><li><p>现在问题已经在于：在kind创建的这个workNode中，path包含了proxy和no_proxy，但no_proxy没有起作用，在curl和wget都出现这个现象。</p></li><li><p>回到vmware ubuntu界面，把整个虚拟机的代理关闭。</p></li><li><p>重新创建集群，使用<code>workNode:NodePort</code><strong>可以</strong>访问。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>kubelabs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>kind</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
